{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-06T23:45:39.794199Z",
     "start_time": "2025-10-06T23:45:39.344297Z"
    }
   },
   "source": [
    "# === Holding_status, date/revenue filters, EV bridge compute/flag/filter ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ===================== 1) Add holding_status =====================\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "if \"exit_date\" not in df.columns:\n",
    "    raise KeyError(\"Column 'exit_date' not found in working.csv. Run the earlier ADD_COLUMNS step from 'deal' first.\")\n",
    "norm = df[\"exit_date\"].copy()\n",
    "if norm.dtype == object:\n",
    "    norm = norm.replace({\"\": pd.NA, \"NaT\": pd.NA, \"nat\": pd.NA, \"None\": pd.NA})\n",
    "is_exited = norm.notna()\n",
    "df[\"holding_status\"] = is_exited.map({True: \"exited\", False: \"unexited\"})\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(\"Added column: holding_status\")\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "exited_count = (check[\"holding_status\"] == \"exited\").sum()\n",
    "unexited_count = (check[\"holding_status\"] == \"unexited\").sum()\n",
    "total_after = len(check)\n",
    "assert exited_count + unexited_count == total_after, \"Status coverage failed: counts don't add up to total rows.\"\n",
    "status_per_deal = check.groupby(\"deal_id\")[\"holding_status\"].nunique(dropna=False)\n",
    "mixed = status_per_deal[status_per_deal > 1]\n",
    "assert mixed.empty, f\"{len(mixed)} deal_id(s) have mixed exited/unexited rows.\"\n",
    "deal_status = check.groupby(\"deal_id\")[\"holding_status\"].first()\n",
    "deal_exited = (deal_status == \"exited\").sum()\n",
    "deal_unexited = (deal_status == \"unexited\").sum()\n",
    "total_deals = deal_status.size\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ===================== 2) Filter unreasonable dates + assert revenue>0 =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "ref = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "lower = pd.Timestamp(1980, 1, 1).normalize()\n",
    "q_end = pd.Timestamp.today().to_period(\"Q-DEC\").end_time.normalize()\n",
    "date_ok = ref.notna() & (ref >= lower) & (ref <= q_end)\n",
    "\n",
    "rev_num = pd.to_numeric(df[\"revenue\"], errors=\"coerce\")\n",
    "rev_ok = rev_num > 0\n",
    "\n",
    "keep_mask = date_ok & rev_ok\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "dropped = before_rows - len(after)\n",
    "print(\n",
    "    f\"Dropped {dropped} rows due to missing/out-of-range reference_date \"\n",
    "    f\"or revenue <= 0 / non-numeric. Kept range [{lower.date()} .. {q_end.date()}].\"\n",
    ")\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "ref2 = pd.to_datetime(check[\"reference_date\"], errors=\"coerce\")\n",
    "assert ref2.notna().all(), \"Found rows with null reference_date.\"\n",
    "assert ((ref2 >= lower) & (ref2 <= q_end)).all(), \"Found dates outside bounds.\"\n",
    "rev2 = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "assert (rev2 > 0).all(), \"Found rows with revenue <= 0 or non-numeric.\"\n",
    "print(f\"FILTER check passed. Remaining rows: {len(check)}\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ===================== 3) Filter for rows missing ebitda or <2 of [EV/ND/Eq] =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "rev_num   = num(df[\"revenue\"])\n",
    "ebitda_num= num(df[\"ebitda\"])\n",
    "ev_num    = num(df[\"enterprise_value\"])\n",
    "nd_num    = num(df[\"net_debt\"])\n",
    "eq_num    = num(df[\"equity\"])\n",
    "\n",
    "rev_ok    = rev_num.notna()\n",
    "ebitda_ok = ebitda_num.notna()\n",
    "trio_non_null = ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)\n",
    "trio_ok   = trio_non_null >= 2\n",
    "keep_mask = rev_ok & ebitda_ok & trio_ok\n",
    "\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "dropped = before_rows - len(after)\n",
    "print(f\"Filtering out {dropped} rows (kept {len(after)} of {before_rows}).\")\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "rev_num   = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "ebitda_num= pd.to_numeric(check[\"ebitda\"], errors=\"coerce\")\n",
    "ev_num    = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd_num    = pd.to_numeric(check[\"net_debt\"], errors=\"coerce\")\n",
    "eq_num    = pd.to_numeric(check[\"equity\"], errors=\"coerce\")\n",
    "assert rev_num.notna().all(), \"Found rows with empty revenue after filtering.\"\n",
    "assert ebitda_num.notna().all(), \"Found rows with empty ebitda after filtering.\"\n",
    "assert ((ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)) >= 2).all(), \\\n",
    "       \"Found rows with fewer than two of [enterprise_value, net_debt, equity] present.\"\n",
    "print(\"FILTER check passed. Shape:\", check.shape)\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ===================== 4) Compute missing EV/ND/Eq (bridge) =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "ev0 = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd0 = pd.to_numeric(df[\"net_debt\"], errors=\"coerce\")\n",
    "eq0 = pd.to_numeric(df[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "present_cnt = ev0.notna().astype(int) + nd0.notna().astype(int) + eq0.notna().astype(int)\n",
    "calc_mask = (present_cnt == 2)\n",
    "df[\"EV_bridge_calc\"] = np.where(calc_mask, \"Yes\", \"No\")\n",
    "\n",
    "need_ev = calc_mask & ev0.isna()\n",
    "need_nd = calc_mask & nd0.isna()\n",
    "need_eq = calc_mask & eq0.isna()\n",
    "df.loc[need_ev, \"enterprise_value\"] = (eq0 + nd0)[need_ev]\n",
    "df.loc[need_nd, \"net_debt\"]         = (ev0 - eq0)[need_nd]\n",
    "df.loc[need_eq, \"equity\"]           = (ev0 - nd0)[need_eq]\n",
    "\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Computed missing EV/NetDebt/Equity for {calc_mask.sum()} rows. Added 'EV_bridge_calc'.\")\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "ev = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(check[\"net_debt\"], errors=\"coerce\")\n",
    "eq = pd.to_numeric(check[\"equity\"], errors=\"coerce\")\n",
    "present_cnt_after = ev.notna().astype(int) + nd.notna().astype(int) + eq.notna().astype(int)\n",
    "yes_mask = (check[\"EV_bridge_calc\"] == \"Yes\")\n",
    "assert (present_cnt_after[yes_mask] == 3).all(), \"Some 'Yes' rows still missing a value.\"\n",
    "assert (present_cnt_after >= 2).all(), \"Found rows with fewer than two of the trio present.\"\n",
    "assert len(check) == before_rows, \"Row count changed.\"\n",
    "print(f\"COMPUTE check passed. Bridges computed: {yes_mask.sum()}, total rows: {len(check)}.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ===================== 5) Flag EV bridge error vs actual EV (abs tol=1001) =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "ev = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(df[\"net_debt\"], errors=\"coerce\")\n",
    "eq = pd.to_numeric(df[\"equity\"], errors=\"coerce\")\n",
    "all3 = ev.notna() & nd.notna() & eq.notna()\n",
    "residual = ev - (eq + nd)\n",
    "tol = 1001.0\n",
    "ok = all3 & (residual.abs() <= tol)\n",
    "df[\"EV_bridge_error\"] = np.where(ok, \"Ok\", \"Error\")\n",
    "df[\"EV_bridge_residual\"] = residual\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "ok_count = int((df[\"EV_bridge_error\"] == \"Ok\").sum())\n",
    "err_count = int((df[\"EV_bridge_error\"] == \"Error\").sum())\n",
    "print(f\"Bridge check vs actual EV (abs tol={tol:g}): Ok={ok_count}, Error={err_count}, Total={len(df)}\")\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "vc = check[\"EV_bridge_error\"].value_counts()\n",
    "ok = int(vc.get(\"Ok\", 0)); err = int(vc.get(\"Error\", 0))\n",
    "assert ok + err == len(check), \"Flags do not cover all rows.\"\n",
    "print(f\"Check passed. Ok={ok}, Error={err}, Total={len(check)}\")\n",
    "\n",
    "# ===================== 6) Remove faulty bridges =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "before = len(df)\n",
    "keep_mask = df[\"EV_bridge_error\"] != \"Error\"\n",
    "after = df.loc[keep_mask].reset_index(drop=True)\n",
    "dropped = before - len(after)\n",
    "print(f\"Dropped {dropped} rows with EV_bridge_error == 'Error'. Kept {len(after)} of {before}.\")\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert (check[\"EV_bridge_error\"] != \"Error\").all(), \"Found remaining rows with EV_bridge_error == 'Error'.\"\n",
    "print(\"FILTER check passed. Rows:\", len(check))\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added column: holding_status\n",
      "unique_deals_exited: 561 unique_deals_unexited: 604 unique_deals_total: 1165\n",
      "Dropped 431 rows due to missing/out-of-range reference_date or revenue <= 0 / non-numeric. Kept range [1980-01-01 .. 2025-12-31].\n",
      "FILTER check passed. Remaining rows: 4527\n",
      "unique_deals_exited: 536 unique_deals_unexited: 456 unique_deals_total: 992\n",
      "Filtering out 2360 rows (kept 2167 of 4527).\n",
      "FILTER check passed. Shape: (2167, 28)\n",
      "unique_deals_exited: 460 unique_deals_unexited: 372 unique_deals_total: 832\n",
      "Computed missing EV/NetDebt/Equity for 1019 rows. Added 'EV_bridge_calc'.\n",
      "COMPUTE check passed. Bridges computed: 1019, total rows: 2167.\n",
      "unique_deals_exited: 460 unique_deals_unexited: 372 unique_deals_total: 832\n",
      "Bridge check vs actual EV (abs tol=1001): Ok=1681, Error=486, Total=2167\n",
      "Check passed. Ok=1681, Error=486, Total=2167\n",
      "Dropped 486 rows with EV_bridge_error == 'Error'. Kept 1681 of 2167.\n",
      "FILTER check passed. Rows: 1681\n",
      "unique_deals_exited: 365 unique_deals_unexited: 292 unique_deals_total: 657\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T23:45:39.941356Z",
     "start_time": "2025-10-06T23:45:39.801135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Date-to-financial matching for exited and unexited deals ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper(s) ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "def select_closest_within_window(frame: pd.DataFrame, target_col: str, ref_col: str = \"_ref_dt\"):\n",
    "    \"\"\"Return a Series indexed by deal_id with the winning 'id' (closest ref within ±3 months).\"\"\"\n",
    "    tgt = frame.groupby(\"deal_id\")[target_col].transform(\"first\")\n",
    "    start = tgt - pd.DateOffset(months=3)\n",
    "    end   = tgt + pd.DateOffset(months=3)\n",
    "\n",
    "    in_window = frame[ref_col].between(start, end, inclusive=\"both\")\n",
    "    tmp = frame.loc[in_window, [\"deal_id\", \"id\", ref_col, target_col]].copy()\n",
    "    if tmp.empty:\n",
    "        return pd.Series(dtype=object)\n",
    "\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[ref_col] - tmp[target_col]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[ref_col] >= tmp[target_col]).astype(int)\n",
    "\n",
    "    tmp_sorted = tmp.sort_values(\n",
    "        [\"deal_id\", \"_abs_diff_days\", \"_is_after_or_eq\", ref_col],\n",
    "        ascending=[True, True, False, True],\n",
    "    )\n",
    "    winners = tmp_sorted.groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    "    return winners  # index: deal_id, values: id\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ===================== 1) Exited deals: keep entry+exit matched rows within ±3 months =====================\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "df[\"_exit_dt\"]  = pd.to_datetime(df[\"exit_date\"],     errors=\"coerce\")\n",
    "\n",
    "# Masks\n",
    "exited    = df[\"holding_status\"] == \"exited\"\n",
    "unexited  = df[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# Only operate on exited deals\n",
    "df_ex = df.loc[exited].copy()\n",
    "\n",
    "# Entry winner (requires entry_date)\n",
    "entry_winners = select_closest_within_window(df_ex, target_col=\"_entry_dt\")\n",
    "\n",
    "# Exit winner (requires exit_date)\n",
    "exit_winners = select_closest_within_window(df_ex, target_col=\"_exit_dt\")\n",
    "\n",
    "# Deals must have both winners to survive\n",
    "entry_ok_deals = set(entry_winners.index)\n",
    "exit_ok_deals  = set(exit_winners.index)\n",
    "survivor_deals = entry_ok_deals & exit_ok_deals\n",
    "\n",
    "# Drop deals where entry winner == exit winner (entry=exit)\n",
    "coincident_deals = {d for d in survivor_deals if entry_winners[d] == exit_winners[d]}\n",
    "if coincident_deals:\n",
    "    print(f\"Removing {len(coincident_deals)} exited deal(s) where entry and exit map to the same id.\")\n",
    "survivor_deals = survivor_deals - coincident_deals\n",
    "\n",
    "# Keep set for exited deals: union of entry+exit winners\n",
    "keep_ids_exited = set(entry_winners.loc[list(survivor_deals)].tolist()) | set(\n",
    "    exit_winners.loc[list(survivor_deals)].tolist()\n",
    ")\n",
    "\n",
    "# Final keep mask:\n",
    "# - keep all rows for unexited deals (untouched)\n",
    "# - for exited deals: keep only winner ids; drop entire deal if not in survivor_deals\n",
    "keep_mask = unexited | (exited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_exited))\n",
    "\n",
    "before_rows = len(df)\n",
    "before_deals_ex = df.loc[exited, \"deal_id\"].nunique()\n",
    "\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\",\"_exit_dt\"] if c in out.columns])\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# Reporting\n",
    "after_rows = len(out)\n",
    "after_deals_ex = out.loc[out[\"holding_status\"]==\"exited\", \"deal_id\"].nunique()\n",
    "dropped_exited_deals = before_deals_ex - after_deals_ex\n",
    "print(\n",
    "    f\"Exited deals kept: {after_deals_ex} (dropped {dropped_exited_deals} with no entry/exit match in ±3 months). \"\n",
    "    f\"Rows now: {after_rows} (from {before_rows}).\"\n",
    ")\n",
    "\n",
    "# --- Validation for exited ---\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# 1) Unexited deals untouched in cardinality of rows per deal (relative order not asserted here)\n",
    "\n",
    "rows_per_deal = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_deal == 2).all(), \"Found exited deals with != 2 kept rows.\"\n",
    "\n",
    "# 3) Verify kept rows are within ±3 months of the respective target dates\n",
    "ck = check.loc[ex_mask].copy()\n",
    "ref = pd.to_datetime(ck[\"reference_date\"], errors=\"coerce\")\n",
    "ent = pd.to_datetime(ck[\"entry_date\"], errors=\"coerce\")\n",
    "exi = pd.to_datetime(ck[\"exit_date\"],  errors=\"coerce\")\n",
    "\n",
    "# Tag each row as 'entry_candidate' or 'exit_candidate' by closeness\n",
    "abs_diff_entry = (ref - ent).abs()\n",
    "abs_diff_exit  = (ref - exi).abs()\n",
    "is_entry_like = abs_diff_entry <= abs_diff_exit\n",
    "\n",
    "from pandas import DateOffset\n",
    "ok_window = (\n",
    "    (is_entry_like & ref.between(ent - DateOffset(months=3), ent + DateOffset(months=3), inclusive=\"both\")) |\n",
    "    (~is_entry_like & ref.between(exi - DateOffset(months=3), exi + DateOffset(months=3), inclusive=\"both\"))\n",
    ")\n",
    "assert ok_window.all(), \"Kept exited rows outside ±3 months window.\"\n",
    "\n",
    "# 4) Summarize counts\n",
    "two_rows = int((rows_per_deal == 2).sum())\n",
    "one_row  = int((rows_per_deal == 1).sum())\n",
    "print(f\"Check passed. Exited deals with 2 rows: {two_rows}; with 1 row (entry=exit candidate): {one_row}.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n",
    "\n",
    "# ===================== 2) Unexited deals: keep entry match (±3 months) + latest ref_date ≤ today =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "\n",
    "today = pd.Timestamp.today().normalize()\n",
    "\n",
    "is_unexited = df[\"holding_status\"] == \"unexited\"\n",
    "is_exited   = df[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Work only on unexited deals\n",
    "df_un = df.loc[is_unexited].copy()\n",
    "\n",
    "# Entry winner within ±3 months (required)\n",
    "entry_winners_un = select_closest_within_window(df_un, target_col=\"_entry_dt\")\n",
    "entry_ok_deals = set(entry_winners_un.index)\n",
    "\n",
    "# Latest ref_date ≤ today (required)\n",
    "ref_le_today = df_un[df_un[\"_ref_dt\"] <= today].copy()\n",
    "latest_ids = (\n",
    "    ref_le_today.sort_values([\"deal_id\", \"_ref_dt\"], ascending=[True, False])\n",
    "                .groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    ")\n",
    "latest_ok_deals = set(latest_ids.index)\n",
    "\n",
    "# Survivors must have both entry match and a latest≤today\n",
    "survivor_deals = entry_ok_deals & latest_ok_deals\n",
    "\n",
    "# Drop deals where entry winner id == latest id\n",
    "coincident = {d for d in survivor_deals if entry_winners_un[d] == latest_ids[d]}\n",
    "survivor_deals -= coincident\n",
    "\n",
    "# Keep exactly the two ids (entry+latest) for survivors; leave exited deals untouched\n",
    "keep_ids_un = set(entry_winners_un.loc[list(survivor_deals)].tolist()) | set(latest_ids.loc[list(survivor_deals)].tolist())\n",
    "\n",
    "keep_mask = is_exited | (is_unexited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_un))\n",
    "\n",
    "before_rows = len(df)\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers and save\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\"] if c in out.columns])\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "print(f\"Unexited: survivors={len(survivor_deals)}, dropped_coincident={len(coincident)}, rows_now={len(out)} (from {before_rows}).\")\n",
    "\n",
    "# --- Validation for unexited & global constraints ---\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Unexited: exactly 2 rows per deal_id\n",
    "rows_per_un = check.loc[un_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_un == 2).all(), \"Unexited deals must have exactly 2 rows.\"\n",
    "\n",
    "# Validate the two rows are entry-match and latest≤today\n",
    "ck_un = check.loc[un_mask].copy()\n",
    "ck_un[\"_ref_dt\"] = pd.to_datetime(ck_un[\"reference_date\"], errors=\"coerce\")\n",
    "ck_un[\"_entry_dt\"] = pd.to_datetime(ck_un[\"entry_date\"], errors=\"coerce\")\n",
    "\n",
    "def entry_winner_verify(frame):\n",
    "    from pandas import DateOffset\n",
    "    tgt = frame.groupby(\"deal_id\")[\"_entry_dt\"].transform(\"first\")\n",
    "    start = tgt - DateOffset(months=3)\n",
    "    end   = tgt + DateOffset(months=3)\n",
    "    in_window = frame[\"_ref_dt\"].between(start, end, inclusive=\"both\")\n",
    "    tmp = frame.loc[in_window, [\"deal_id\",\"id\",\"_ref_dt\",\"_entry_dt\"]].copy()\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[\"_ref_dt\"] - tmp[\"_entry_dt\"]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[\"_ref_dt\"] >= tmp[\"_entry_dt\"]).astype(int)\n",
    "    tmp = tmp.sort_values([\"deal_id\",\"_abs_diff_days\",\"_is_after_or_eq\",\"_ref_dt\"],\n",
    "                          ascending=[True, True, False, True])\n",
    "    return tmp.groupby(\"deal_id\")[\"id\"].first()\n",
    "\n",
    "entry_verify = entry_winner_verify(ck_un)\n",
    "latest_verify = (ck_un[ck_un[\"_ref_dt\"] <= pd.Timestamp.today().normalize()]\n",
    "                 .sort_values([\"deal_id\",\"_ref_dt\"], ascending=[True, False])\n",
    "                 .groupby(\"deal_id\")[\"id\"].first())\n",
    "\n",
    "for d, grp in ck_un.groupby(\"deal_id\"):\n",
    "    ids = set(grp[\"id\"])\n",
    "    assert d in entry_verify.index and d in latest_verify.index, f\"Deal {d}: missing entry or latest id.\"\n",
    "    assert entry_verify[d] in ids and latest_verify[d] in ids, f\"Deal {d}: kept rows are not entry+latest.\"\n",
    "\n",
    "# Exited: unchanged cardinality constraint (still ≤ 2)\n",
    "rows_per_ex = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "\n",
    "# Deal-level row-count report for unexited\n",
    "two_rows_un  = int((rows_per_un == 2).sum())\n",
    "one_row_un   = int((rows_per_un == 1).sum())\n",
    "gt2_rows_un  = int((rows_per_un > 2).sum())\n",
    "total_un     = int(rows_per_un.size)\n",
    "\n",
    "print(f\"Unexited row-counts per deal_id — 2 rows: {two_rows_un}, 1 row: {one_row_un}, >2 rows: {gt2_rows_un}, total: {total_un}\")\n",
    "\n",
    "# Keep your hard guarantees\n",
    "assert one_row_un == 0, \"Unexited deals with exactly 1 row found.\"\n",
    "assert gt2_rows_un == 0, \"Unexited deals with >2 rows found.\"\n",
    "assert (rows_per_un == 2).all(), \"Unexited deals must have exactly 2 rows.\"\n",
    "assert (rows_per_ex <= 2).all(), \"Exited deals show >2 rows after unexited processing.\"\n",
    "\n",
    "# Deal-level row-count report for ALL deals (exited + unexited)\n",
    "rows_per_all = check.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "two_rows_all = int((rows_per_all == 2).sum())\n",
    "one_row_all  = int((rows_per_all == 1).sum())\n",
    "gt2_rows_all = int((rows_per_all > 2).sum())\n",
    "total_all    = int(rows_per_all.size)\n",
    "\n",
    "# ---------- Step 3b: Enforce one currency per deal_id (drop mixed/unknown) ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "if \"reporting_currency_financials\" not in df.columns:\n",
    "    raise KeyError(\"Missing 'reporting_currency_financials'—ensure Step 3 added it.\")\n",
    "\n",
    "cur = df[\"reporting_currency_financials\"].astype(str).str.strip()\n",
    "# Treat \"\", \"nan\", \"NaN\" as null\n",
    "cur = cur.mask(cur.str.lower().isin({\"\", \"nan\"}))\n",
    "df[\"_currency\"] = cur\n",
    "\n",
    "# Rule 1: require a defined currency for every row\n",
    "rows_with_null_cur = df[\"_currency\"].isna().sum()\n",
    "\n",
    "# Rule 2: require exactly one distinct currency per deal_id\n",
    "per_deal_nuniq = df.groupby(\"deal_id\")[\"_currency\"].nunique(dropna=False)\n",
    "bad_deals = per_deal_nuniq[(per_deal_nuniq != 1)].index\n",
    "\n",
    "keep_mask = df[\"deal_id\"].isin(bad_deals) == False\n",
    "keep_mask &= df[\"_currency\"].notna()\n",
    "\n",
    "before_deals = df[\"deal_id\"].nunique()\n",
    "before_rows  = len(df)\n",
    "\n",
    "out = df.loc[keep_mask].drop(columns=[\"_currency\"]).reset_index(drop=True)\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "after_deals = out[\"deal_id\"].nunique()\n",
    "after_rows  = len(out)\n",
    "\n",
    "print(f\"Currency consistency: dropped {before_deals - after_deals} deal_id(s) with mixed/unknown currencies; \"\n",
    "      f\"rows: {after_rows} (from {before_rows}).\")\n",
    "\n",
    "# Hard assertions\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "assert check[\"reporting_currency_financials\"].notna().all(), \"Null currency remains.\"\n",
    "nu = check.groupby(\"deal_id\")[\"reporting_currency_financials\"].nunique()\n",
    "assert (nu == 1).all(), \"Found deal(s) with multiple currencies.\"\n",
    "print(\"CURRENCY check passed.\")\n",
    "\n",
    "\n",
    "print(f\"All deals — 2 rows: {two_rows_all}, 1 row: {one_row_all}, >2 rows: {gt2_rows_all}, total: {total_all}\")\n",
    "\n",
    "# Hard guarantees across the whole dataset\n",
    "assert one_row_all == 0, \"Found deals with exactly 1 row.\"\n",
    "assert gt2_rows_all == 0, \"Found deals with >2 rows.\"\n",
    "assert (rows_per_all == 2).all(), \"All deals must have exactly 2 rows.\"\n",
    "\n",
    "print(\"Unexited selection check passed.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "id": "394a8f291076b99f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 2 exited deal(s) where entry and exit map to the same id.\n",
      "Exited deals kept: 255 (dropped 110 with no entry/exit match in ±3 months). Rows now: 1333 (from 1681).\n",
      "Check passed. Exited deals with 2 rows: 255; with 1 row (entry=exit candidate): 0.\n",
      "unique_deals_exited: 255 unique_deals_unexited: 292 unique_deals_total: 547\n",
      "unique_deals: 547\n",
      "Unexited: survivors=180, dropped_coincident=33, rows_now=870 (from 1333).\n",
      "Unexited row-counts per deal_id — 2 rows: 180, 1 row: 0, >2 rows: 0, total: 180\n",
      "Currency consistency: dropped 1 deal_id(s) with mixed/unknown currencies; rows: 868 (from 870).\n",
      "CURRENCY check passed.\n",
      "All deals — 2 rows: 435, 1 row: 0, >2 rows: 0, total: 435\n",
      "Unexited selection check passed.\n",
      "unique_deals_exited: 255 unique_deals_unexited: 179 unique_deals_total: 434\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T23:45:40.020882Z",
     "start_time": "2025-10-06T23:45:39.949858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Calculate metrics for analysis + then drop unreasonable metrics ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "\n",
    "# ===================== 1) Add calculated columns (row-wise) =====================\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "#Exclude a faulty deal:\n",
    "df = df.loc[df[\"deal_id\"] != \"83283299-100b-44c6-a997-ec634d90768d\"].copy()\n",
    "df = df.loc[df[\"deal_id\"] != \"2376a9b2-d983-48fc-b62a-52fb78bf1409\"].copy()\n",
    "df = df.loc[df[\"deal_id\"] != \"98cb6742-e749-4344-85ab-5d5713d30e59\"].copy()\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "ev = num(df[\"enterprise_value\"])\n",
    "eb = num(df[\"ebitda\"])\n",
    "nd = num(df[\"net_debt\"])\n",
    "eq = num(df[\"equity\"])\n",
    "rv = num(df[\"revenue\"])\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    ebitda_margin = (eb / rv).where(rv != 0)\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    xebitda = (ev / eb).where(eb != 0)\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    de_ratio = (nd / eq).where(eq != 0)\n",
    "\n",
    "# --- NEW: dividends & capital_injections at DEAL level from EXIT row (latest ref_date) ---\n",
    "df[\"_ref_dt\"] = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "# rank within deal: 1 = entry (earliest), 2 = exit (latest)\n",
    "df[\"_rank\"] = df.groupby(\"deal_id\")[\"_ref_dt\"].rank(method=\"first\", ascending=True)\n",
    "\n",
    "# enterprise_value at exit row (rank 2), per deal\n",
    "exit_eq = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "             .assign(exit_eq=eq[df[\"_rank\"] == 2].values)\n",
    "             .groupby(\"deal_id\")[\"exit_eq\"].first())\n",
    "\n",
    "# broadcast to both rows of each deal\n",
    "dividends = df[\"deal_id\"].map((0.1481481481 * exit_eq).to_dict()) #0.1481481481\n",
    "capital_injections = df[\"deal_id\"].map((-0.1111111111 * exit_eq).to_dict()) #-0.1111111111\n",
    "# --- END NEW ---\n",
    "\n",
    "interest_rate = pd.Series(0.09, index=df.index)\n",
    "\n",
    "today = pd.Timestamp.today().normalize()\n",
    "ent = pd.to_datetime(df.get(\"entry_date\"), errors=\"coerce\")\n",
    "exi = pd.to_datetime(df.get(\"exit_date\"),  errors=\"coerce\")\n",
    "is_exited = (df[\"holding_status\"] == \"exited\")\n",
    "\n",
    "days = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "days.loc[is_exited] = (exi - ent).dt.days.loc[is_exited]\n",
    "days.loc[~is_exited] = (today - ent).dt.days.loc[~is_exited]\n",
    "days = days.where(days >= 0)\n",
    "holding_period = days / 365.25\n",
    "\n",
    "cost_of_debt = (1 + interest_rate) ** holding_period - 1\n",
    "\n",
    "df[\"ebitda_margin\"]      = ebitda_margin\n",
    "df[\"xebitda\"]            = xebitda\n",
    "df[\"de_ratio\"]           = de_ratio\n",
    "df[\"dividends\"]          = dividends\n",
    "df[\"capital_injections\"] = capital_injections\n",
    "df[\"interest_rate\"]      = interest_rate\n",
    "df[\"holding_period\"]     = holding_period\n",
    "df[\"cost_of_debt\"]       = cost_of_debt\n",
    "\n",
    "# clean helper cols\n",
    "df = df.drop(columns=[\"_ref_dt\", \"_rank\"])\n",
    "\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "g = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "new_cols = [\"ebitda_margin\",\"xebitda\",\"de_ratio\",\"dividends\",\"capital_injections\",\"interest_rate\",\"holding_period\",\"cost_of_debt\"]\n",
    "nn = {c: int(g[c].notna().sum()) for c in new_cols}\n",
    "print(\"Added columns:\", \", \".join(new_cols))\n",
    "print(\"Non-null counts:\", nn)\n",
    "\n",
    "# ensure availability of by_status for the print (matches prior cells’ semantics)\n",
    "by_status = (g.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ===================== 2) Drop deals with unreasonable financial metrics =====================\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "before_deals = df[\"deal_id\"].nunique()\n",
    "\n",
    "em  = pd.to_numeric(df[\"ebitda_margin\"], errors=\"coerce\")\n",
    "xe  = pd.to_numeric(df[\"xebitda\"],        errors=\"coerce\")\n",
    "der = pd.to_numeric(df[\"de_ratio\"],       errors=\"coerce\")\n",
    "\n",
    "v_em  = em > 1\n",
    "v_xe  = xe.isna() | (xe < -1000) | (xe > 1000)\n",
    "v_der = der.isna() | (der < -1)   | (der > 20)\n",
    "\n",
    "bad_em_deals  = set(df.loc[v_em,  \"deal_id\"].dropna().unique())\n",
    "bad_xe_deals  = set(df.loc[v_xe,  \"deal_id\"].dropna().unique())\n",
    "bad_der_deals = set(df.loc[v_der, \"deal_id\"].dropna().unique())\n",
    "bad_deals = bad_em_deals | bad_xe_deals | bad_der_deals\n",
    "\n",
    "keep_mask = ~df[\"deal_id\"].isin(bad_deals)\n",
    "out = df.loc[keep_mask].reset_index(drop=True)\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "after_rows  = len(out)\n",
    "after_deals = out[\"deal_id\"].nunique()\n",
    "print(\n",
    "    f\"Dropped {before_deals - after_deals} deal_id(s). \"\n",
    "    f\"[ebitda_margin>1: {len(bad_em_deals)}, xebitda null/|>|1000: {len(bad_xe_deals)}, \"\n",
    "    f\"de_ratio null/<-1/>20: {len(bad_der_deals)}]  Rows: {after_rows} (from {before_rows}).\"\n",
    ")\n",
    "\n",
    "# ---- checks ----\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "em2  = pd.to_numeric(check[\"ebitda_margin\"], errors=\"coerce\")\n",
    "xe2  = pd.to_numeric(check[\"xebitda\"],        errors=\"coerce\")\n",
    "der2 = pd.to_numeric(check[\"de_ratio\"],       errors=\"coerce\")\n",
    "assert not (em2 > 1).any(), \"Remaining rows with ebitda_margin > 1.\"\n",
    "assert not (xe2.isna() | (xe2 < -1000) | (xe2 > 1000)).any(), \"Remaining rows with invalid xebitda.\"\n",
    "assert not (der2.isna() | (der2 < -1) | (der2 > 20)).any(), \"Remaining rows with invalid de_ratio.\"\n",
    "rows_per_deal = check.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_deal == 2).all(), \"Each remaining deal must have exactly 2 rows.\"\n",
    "\n",
    "g = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "id": "dbaee67da1b8de4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added columns: ebitda_margin, xebitda, de_ratio, dividends, capital_injections, interest_rate, holding_period, cost_of_debt\n",
      "Non-null counts: {'ebitda_margin': 862, 'xebitda': 861, 'de_ratio': 849, 'dividends': 862, 'capital_injections': 862, 'interest_rate': 862, 'holding_period': 862, 'cost_of_debt': 862}\n",
      "unique_deals_exited: 253 unique_deals_unexited: 178 unique_deals_total: 431\n",
      "Dropped 22 deal_id(s). [ebitda_margin>1: 1, xebitda null/|>|1000: 4, de_ratio null/<-1/>20: 17]  Rows: 818 (from 862).\n",
      "unique_deals_exited: 235 unique_deals_unexited: 174 unique_deals_total: 409\n"
     ]
    }
   ],
   "execution_count": 94
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
