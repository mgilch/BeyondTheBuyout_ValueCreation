{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-04T18:17:03.946268Z",
     "start_time": "2025-10-04T18:17:03.816278Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#----- Start the data transformation by adding \"holding_status\" -----#\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    # Helpful diagnostics\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "before_rows = len(df)\n",
    "\n",
    "# Normalize exit_date blanks to NA (handles \"\", \"NaT\" strings)\n",
    "if \"exit_date\" not in df.columns:\n",
    "    raise KeyError(\"Column 'exit_date' not found in working.csv. Run the earlier ADD_COLUMNS step from 'deal' first.\")\n",
    "norm = df[\"exit_date\"].copy()\n",
    "if norm.dtype == object:\n",
    "    norm = norm.replace({\"\": pd.NA, \"NaT\": pd.NA, \"nat\": pd.NA, \"None\": pd.NA})\n",
    "\n",
    "# Determine holding_status: any non-null exit_date => exited; else unexited\n",
    "is_exited = norm.notna()\n",
    "df[\"holding_status\"] = is_exited.map({True: \"exited\", False: \"unexited\"})\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(\"Added column: holding_status\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "\n",
    "# Row-level coverage\n",
    "exited_count = (check[\"holding_status\"] == \"exited\").sum()\n",
    "unexited_count = (check[\"holding_status\"] == \"unexited\").sum()\n",
    "total_after = len(check)\n",
    "assert exited_count + unexited_count == total_after, \"Status coverage failed: counts don't add up to total rows.\"\n",
    "\n",
    "# Deal-level consistency: each deal_id should have a single status\n",
    "status_per_deal = check.groupby(\"deal_id\")[\"holding_status\"].nunique(dropna=False)\n",
    "mixed = status_per_deal[status_per_deal > 1]\n",
    "assert mixed.empty, f\"{len(mixed)} deal_id(s) have mixed exited/unexited rows.\"\n",
    "\n",
    "print(f\"Check passed. exited={exited_count}, unexited={unexited_count}, total_rows={total_after}\")\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added column: holding_status\n",
      "Check passed. exited=2511, unexited=2445, total_rows=4956\n",
      "unique_deals: 1164\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:17:04.058422Z",
     "start_time": "2025-10-04T18:17:03.952057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "#----- Filter for unreasonable dates -----#\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# Parse dates (tolerant). Expect ISO strings like \"YYYY-MM-DD\" from earlier steps.\n",
    "ref = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "lower = pd.Timestamp(1980, 1, 1).normalize()\n",
    "q_end = pd.Timestamp.today().to_period(\"Q-DEC\").end_time.normalize()\n",
    "date_ok = ref.isna() | ((ref >= lower) & (ref <= q_end))\n",
    "\n",
    "# --- Revenue non-zero (treat non-numeric/NA as \"keep\")\n",
    "rev_num = pd.to_numeric(df[\"revenue\"], errors=\"coerce\")\n",
    "rev_ok = rev_num.fillna(np.inf) != 0\n",
    "\n",
    "# Keep if reference_date is NA OR within [lower, q_end]; drop otherwise.\n",
    "keep_mask = date_ok & rev_ok\n",
    "\n",
    "# Preserve original order\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "\n",
    "dropped = before_rows - len(after)\n",
    "print(f\"Dropped {dropped} rows outside [{lower.date()} .. {q_end.date()}] or with revenue == 0.\")\n",
    "\n",
    "# Save\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "ref2 = pd.to_datetime(check[\"reference_date\"], errors=\"coerce\")\n",
    "\n",
    "# 1) Dates within bounds (for non-null)\n",
    "assert ((ref2.dropna() >= lower) & (ref2.dropna() <= q_end)).all(), \"Found dates outside bounds.\"\n",
    "\n",
    "# 2) No revenue == 0\n",
    "rev2 = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "assert not (rev2 == 0).any(), \"Found rows with revenue == 0.\"\n",
    "\n",
    "print(f\"FILTER check passed. Remaining rows: {len(check)}\")\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "635e1b3e20b82d56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 31 rows outside [1980-01-01 .. 2025-12-31] or with revenue == 0.\n",
      "FILTER check passed. Remaining rows: 4925\n",
      "unique_deals: 1163\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:17:04.125672Z",
     "start_time": "2025-10-04T18:17:04.066791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Filter out rows that have no revenue or no EBITDA -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# Temporary numeric views for presence checks (does not change df)\n",
    "def num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "rev_num   = num(df[\"revenue\"])\n",
    "ebitda_num= num(df[\"ebitda\"])\n",
    "ev_num    = num(df[\"enterprise_value\"])\n",
    "nd_num    = num(df[\"net_debt\"])\n",
    "eq_num    = num(df[\"equity\"])\n",
    "\n",
    "# Presence logic\n",
    "rev_ok    = rev_num.notna()\n",
    "ebitda_ok = ebitda_num.notna()\n",
    "trio_non_null = ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)\n",
    "trio_ok   = trio_non_null >= 2\n",
    "\n",
    "keep_mask = rev_ok & ebitda_ok & trio_ok\n",
    "\n",
    "# Preserve original order exactly\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "\n",
    "dropped = before_rows - len(after)\n",
    "print(f\"Filtering out {dropped} rows (kept {len(after)} of {before_rows}).\")\n",
    "\n",
    "# Save\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "# Recompute numeric presence on the saved data for verification\n",
    "rev_num   = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "ebitda_num= pd.to_numeric(check[\"ebitda\"], errors=\"coerce\")\n",
    "ev_num    = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd_num    = pd.to_numeric(check[\"net_debt\"], errors=\"coerce\")\n",
    "eq_num    = pd.to_numeric(check[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "assert rev_num.notna().all(), \"Found rows with empty revenue after filtering.\"\n",
    "assert ebitda_num.notna().all(), \"Found rows with empty ebitda after filtering.\"\n",
    "assert ((ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)) >= 2).all(), \\\n",
    "       \"Found rows with fewer than two of [enterprise_value, net_debt, equity] present.\"\n",
    "\n",
    "print(\"FILTER check passed. Shape:\", check.shape)\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "fbe9eecaf8b30ee7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 2760 rows (kept 2165 of 4925).\n",
      "FILTER check passed. Shape: (2165, 28)\n",
      "unique_deals: 831\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:17:04.183418Z",
     "start_time": "2025-10-04T18:17:04.129628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Calculate missing EV-EqV bridge items -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# Numeric views for calculations (do not mutate these)\n",
    "ev0 = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd0 = pd.to_numeric(df[\"net_debt\"], errors=\"coerce\")\n",
    "eq0 = pd.to_numeric(df[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "# Count how many of the trio are present (non-null)\n",
    "present_cnt = ev0.notna().astype(int) + nd0.notna().astype(int) + eq0.notna().astype(int)\n",
    "calc_mask = (present_cnt == 2)\n",
    "\n",
    "# Initialize flag: Yes if we will compute a missing third, else No\n",
    "df[\"EV_bridge_calc\"] = np.where(calc_mask, \"Yes\", \"No\")\n",
    "\n",
    "# Compute missing field using the other two (using original, unmodified numeric series)\n",
    "need_ev = calc_mask & ev0.isna()\n",
    "need_nd = calc_mask & nd0.isna()\n",
    "need_eq = calc_mask & eq0.isna()\n",
    "\n",
    "df.loc[need_ev, \"enterprise_value\"] = (eq0 + nd0)[need_ev]\n",
    "df.loc[need_nd, \"net_debt\"]         = (ev0 - eq0)[need_nd]\n",
    "df.loc[need_eq, \"equity\"]           = (ev0 - nd0)[need_eq]\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Computed missing EV/NetDebt/Equity for {calc_mask.sum()} rows. Added 'EV_bridge_calc'.\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Re-evaluate presence after computation\n",
    "ev = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(check[\"net_debt\"], errors=\"coerce\")\n",
    "eq = pd.to_numeric(check[\"equity\"], errors=\"coerce\")\n",
    "present_cnt_after = ev.notna().astype(int) + nd.notna().astype(int) + eq.notna().astype(int)\n",
    "\n",
    "# 1) Rows that were marked \"Yes\" now have all three present\n",
    "yes_mask = (check[\"EV_bridge_calc\"] == \"Yes\")\n",
    "assert (present_cnt_after[yes_mask] == 3).all(), \"Some 'Yes' rows still missing a value.\"\n",
    "\n",
    "# 2) All rows still meet the earlier rule (>= 2 present)\n",
    "assert (present_cnt_after >= 2).all(), \"Found rows with fewer than two of the trio present.\"\n",
    "\n",
    "# 3) Row count preserved\n",
    "assert len(check) == before_rows, \"Row count changed.\"\n",
    "\n",
    "print(f\"COMPUTE check passed. Bridges computed: {yes_mask.sum()}, total rows: {len(check)}.\")\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "f4befa8497d8effc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed missing EV/NetDebt/Equity for 1019 rows. Added 'EV_bridge_calc'.\n",
      "COMPUTE check passed. Bridges computed: 1019, total rows: 2165.\n",
      "unique_deals: 831\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:17:04.244805Z",
     "start_time": "2025-10-04T18:17:04.188323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Flag faulty EV-EqV bridges -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Numeric views\n",
    "ev = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(df[\"net_debt\"], errors=\"coerce\")\n",
    "eq = pd.to_numeric(df[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "# Require all three present\n",
    "all3 = ev.notna() & nd.notna() & eq.notna()\n",
    "# Differences for the three identities\n",
    "d_ev = ev - (eq + nd)\n",
    "d_nd = nd - (ev - eq)\n",
    "d_eq = eq - (ev - nd)\n",
    "\n",
    "# Tolerance: 3% of |EV| with a small absolute floor\n",
    "abs_floor = 1e-6\n",
    "tol = np.maximum(0.03 * ev.abs(), abs_floor)\n",
    "ok_ev = all3 & (d_ev.abs() <= tol)\n",
    "ok_nd = all3 & (d_nd.abs() <= tol)\n",
    "ok_eq = all3 & (d_eq.abs() <= tol)\n",
    "\n",
    "ok_bridge = ok_ev & ok_nd & ok_eq\n",
    "\n",
    "# Anything not all3 or exceeding tolerance -> Error\n",
    "df[\"EV_bridge_error\"] = np.where(ok_bridge, \"Ok\", \"Error\")\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "ok_count = (df[\"EV_bridge_error\"] == \"Ok\").sum()\n",
    "err_count = (df[\"EV_bridge_error\"] == \"Error\").sum()\n",
    "print(f\"Bridge check (3% of |EV| tolerance): Ok={ok_count}, Error={err_count}, Total={len(df)}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "vc = check[\"EV_bridge_error\"].value_counts()\n",
    "ok = int(vc.get(\"Ok\", 0))\n",
    "err = int(vc.get(\"Error\", 0))\n",
    "assert ok + err == len(check), \"Flags do not cover all rows.\"\n",
    "print(f\"Check passed. Ok={ok}, Error={err}, Total={len(check)}\")\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "31c60498be2675ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bridge check (3% of |EV| tolerance): Ok=1856, Error=309, Total=2165\n",
      "Check passed. Ok=1856, Error=309, Total=2165\n",
      "unique_deals: 831\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:17:04.297849Z",
     "start_time": "2025-10-04T18:17:04.253946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#----- Remove faulty EV-EqV bridges -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "before = len(df)\n",
    "\n",
    "keep_mask = df[\"EV_bridge_error\"] != \"Error\"\n",
    "after = df.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "dropped = before - len(after)\n",
    "print(f\"Dropped {dropped} rows with EV_bridge_error == 'Error'. Kept {len(after)} of {before}.\")\n",
    "\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert (check[\"EV_bridge_error\"] != \"Error\").all(), \"Found remaining rows with EV_bridge_error == 'Error'.\"\n",
    "print(\"FILTER check passed. Rows:\", len(check))\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "9747c88dca9082eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 309 rows with EV_bridge_error == 'Error'. Kept 1856 of 2165.\n",
      "FILTER check passed. Rows: 1856\n",
      "unique_deals: 751\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:17:04.359225Z",
     "start_time": "2025-10-04T18:17:04.301867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Entry/ exit date <---> financial data matching -----#\n",
    "\n",
    "#-----\n",
    "\"\"\"Every unique deal_id with holding_status == \"exited\" should have only two rows (i.e. id). And these rows should be the ones with the reference date matched as closely as possible to the entry and exit date, if a reference_date exists within a +-3 month window. All deal_id with holding_status == \"unexited\" should not be touched by the exit operation.\"\"\"\n",
    "#-----\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "df[\"_exit_dt\"]  = pd.to_datetime(df[\"exit_date\"],     errors=\"coerce\")\n",
    "\n",
    "# Masks\n",
    "exited    = df[\"holding_status\"] == \"exited\"\n",
    "unexited  = df[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# --- Helper: select closest row to a target date within ±3 calendar months (inclusive)\n",
    "def select_closest_within_window(frame: pd.DataFrame, target_col: str, ref_col: str = \"_ref_dt\"):\n",
    "    \"\"\"Return a Series indexed by deal_id with the winning 'id' (closest ref within ±3 months).\"\"\"\n",
    "    # Target per deal\n",
    "    tgt = frame.groupby(\"deal_id\")[target_col].transform(\"first\")\n",
    "    start = tgt - pd.DateOffset(months=3)\n",
    "    end   = tgt + pd.DateOffset(months=3)\n",
    "\n",
    "    in_window = frame[ref_col].between(start, end, inclusive=\"both\")\n",
    "\n",
    "    # Tie-breakers: 1) min abs diff  2) prefer on/after target  3) earliest ref\n",
    "    tmp = frame.loc[in_window, [\"deal_id\", \"id\", ref_col, target_col]].copy()\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[ref_col] - tmp[target_col]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[ref_col] >= tmp[target_col]).astype(int)\n",
    "\n",
    "    tmp_sorted = tmp.sort_values(\n",
    "        [\"deal_id\", \"_abs_diff_days\", \"_is_after_or_eq\", ref_col],\n",
    "        ascending=[True, True, False, True],\n",
    "    )\n",
    "    # Winner per deal_id\n",
    "    winners = tmp_sorted.groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    "    return winners  # index: deal_id, values: id\n",
    "\n",
    "# Only operate on exited deals\n",
    "df_ex = df.loc[exited].copy()\n",
    "\n",
    "# Entry winner (requires entry_date)\n",
    "entry_winners = select_closest_within_window(df_ex, target_col=\"_entry_dt\")\n",
    "\n",
    "# Exit winner (requires exit_date)\n",
    "exit_winners = select_closest_within_window(df_ex, target_col=\"_exit_dt\")\n",
    "\n",
    "# Deals must have both winners to survive\n",
    "entry_ok_deals = set(entry_winners.index)\n",
    "exit_ok_deals  = set(exit_winners.index)\n",
    "survivor_deals = entry_ok_deals & exit_ok_deals\n",
    "\n",
    "# --- NEW: drop deals where entry winner == exit winner (entry=exit)\n",
    "coincident_deals = {d for d in survivor_deals if entry_winners[d] == exit_winners[d]}\n",
    "if coincident_deals:\n",
    "    print(f\"Removing {len(coincident_deals)} exited deal(s) where entry and exit map to the same id.\")\n",
    "survivor_deals = survivor_deals - coincident_deals\n",
    "# --- END NEW\n",
    "\n",
    "# Build set of ids to keep for exited deals: union of entry+exit winners per surviving deal\n",
    "keep_ids_exited = set(entry_winners.loc[list(survivor_deals)].tolist()) | set(\n",
    "    exit_winners.loc[list(survivor_deals)].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "# Final keep mask:\n",
    "# - keep all rows for unexited deals (untouched)\n",
    "# - for exited deals: keep only winner ids; drop entire deal if not in survivor_deals\n",
    "keep_mask = unexited | (exited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_exited))\n",
    "\n",
    "before_rows = len(df)\n",
    "before_deals_ex = df.loc[exited, \"deal_id\"].nunique()\n",
    "\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\",\"_exit_dt\"] if c in out.columns])\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# Reporting\n",
    "after_rows = len(out)\n",
    "after_deals_ex = out.loc[out[\"holding_status\"]==\"exited\", \"deal_id\"].nunique()\n",
    "dropped_exited_deals = before_deals_ex - after_deals_ex\n",
    "print(\n",
    "    f\"Exited deals kept: {after_deals_ex} (dropped {dropped_exited_deals} with no entry/exit match in ±3 months). \"\n",
    "    f\"Rows now: {after_rows} (from {before_rows}).\"\n",
    ")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# 1) Unexited deals untouched in cardinality of rows per deal (relative order not asserted here)\n",
    "\n",
    "rows_per_deal = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_deal == 2).all(), \"Found exited deals with != 2 kept rows.\"\n",
    "\n",
    "\n",
    "# 3) Verify kept rows are within ±3 months of the respective target dates\n",
    "ck = check.loc[ex_mask].copy()\n",
    "ref = pd.to_datetime(ck[\"reference_date\"], errors=\"coerce\")\n",
    "ent = pd.to_datetime(ck[\"entry_date\"], errors=\"coerce\")\n",
    "exi = pd.to_datetime(ck[\"exit_date\"],  errors=\"coerce\")\n",
    "\n",
    "# Tag each row as 'entry_candidate' or 'exit_candidate' by closeness\n",
    "abs_diff_entry = (ref - ent).abs()\n",
    "abs_diff_exit  = (ref - exi).abs()\n",
    "is_entry_like = abs_diff_entry <= abs_diff_exit\n",
    "\n",
    "# Both must be within ±3 months relative to whichever they represent.\n",
    "from pandas import DateOffset\n",
    "ok_window = (\n",
    "    (is_entry_like & ref.between(ent - DateOffset(months=3), ent + DateOffset(months=3), inclusive=\"both\")) |\n",
    "    (~is_entry_like & ref.between(exi - DateOffset(months=3), exi + DateOffset(months=3), inclusive=\"both\"))\n",
    ")\n",
    "assert ok_window.all(), \"Kept exited rows outside ±3 months window.\"\n",
    "\n",
    "# 4) Summarize counts\n",
    "two_rows = int((rows_per_deal == 2).sum())\n",
    "one_row  = int((rows_per_deal == 1).sum())\n",
    "print(f\"Check passed. Exited deals with 2 rows: {two_rows}; with 1 row (entry=exit candidate): {one_row}.\")\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "51e0e03ab5591ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 2 exited deal(s) where entry and exit map to the same id.\n",
      "Exited deals kept: 276 (dropped 135 with no entry/exit match in ±3 months). Rows now: 1471 (from 1856).\n",
      "Check passed. Exited deals with 2 rows: 276; with 1 row (entry=exit candidate): 0.\n",
      "unique_deals: 616\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:17:04.412743Z",
     "start_time": "2025-10-04T18:17:04.364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "\n",
    "today = pd.Timestamp.today().normalize()\n",
    "\n",
    "is_unexited = df[\"holding_status\"] == \"unexited\"\n",
    "is_exited   = df[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Helper: closest row to target within ±3 calendar months (inclusive)\n",
    "def select_closest_within_window(frame: pd.DataFrame, target_col: str, ref_col: str = \"_ref_dt\"):\n",
    "    \"\"\"Return Series indexed by deal_id with the winning 'id'.\"\"\"\n",
    "    tgt = frame.groupby(\"deal_id\")[target_col].transform(\"first\")\n",
    "    start = tgt - pd.DateOffset(months=3)\n",
    "    end   = tgt + pd.DateOffset(months=3)\n",
    "    in_window = frame[ref_col].between(start, end, inclusive=\"both\")\n",
    "\n",
    "    tmp = frame.loc[in_window, [\"deal_id\", \"id\", ref_col, target_col]].copy()\n",
    "    if tmp.empty:\n",
    "        return pd.Series(dtype=object)\n",
    "\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[ref_col] - tmp[target_col]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[ref_col] >= tmp[target_col]).astype(int)\n",
    "\n",
    "    tmp_sorted = tmp.sort_values(\n",
    "        [\"deal_id\", \"_abs_diff_days\", \"_is_after_or_eq\", ref_col],\n",
    "        ascending=[True, True, False, True],\n",
    "    )\n",
    "    return tmp_sorted.groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    "\n",
    "# Work only on unexited deals\n",
    "df_un = df.loc[is_unexited].copy()\n",
    "\n",
    "# Entry winner within ±3 months (required)\n",
    "entry_winners_un = select_closest_within_window(df_un, target_col=\"_entry_dt\")\n",
    "entry_ok_deals = set(entry_winners_un.index)\n",
    "\n",
    "# Latest ref_date ≤ today (required)\n",
    "ref_le_today = df_un[df_un[\"_ref_dt\"] <= today].copy()\n",
    "latest_ids = (\n",
    "    ref_le_today.sort_values([\"deal_id\", \"_ref_dt\"], ascending=[True, False])\n",
    "                .groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    ")\n",
    "latest_ok_deals = set(latest_ids.index)\n",
    "\n",
    "# Survivors must have both entry match and a latest≤today\n",
    "survivor_deals = entry_ok_deals & latest_ok_deals\n",
    "\n",
    "# Drop deals where entry winner id == latest id\n",
    "coincident = {d for d in survivor_deals if entry_winners_un[d] == latest_ids[d]}\n",
    "survivor_deals -= coincident\n",
    "\n",
    "# Keep exactly the two ids (entry+latest) for survivors; leave exited deals untouched\n",
    "keep_ids_un = set(entry_winners_un.loc[list(survivor_deals)].tolist()) | set(latest_ids.loc[list(survivor_deals)].tolist())\n",
    "\n",
    "keep_mask = is_exited | (is_unexited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_un))\n",
    "\n",
    "before_rows = len(df)\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers and save\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\"] if c in out.columns])\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "print(f\"Unexited: survivors={len(survivor_deals)}, dropped_coincident={len(coincident)}, rows_now={len(out)} (from {before_rows}).\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Unexited: exactly 2 rows per deal_id\n",
    "rows_per_un = check.loc[un_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_un == 2).all(), \"Unexited deals must have exactly 2 rows.\"\n",
    "\n",
    "# Validate the two rows are entry-match and latest≤today\n",
    "ck_un = check.loc[un_mask].copy()\n",
    "ck_un[\"_ref_dt\"] = pd.to_datetime(ck_un[\"reference_date\"], errors=\"coerce\")\n",
    "ck_un[\"_entry_dt\"] = pd.to_datetime(ck_un[\"entry_date\"], errors=\"coerce\")\n",
    "\n",
    "# Recompute entry winners for verification\n",
    "def entry_winner_verify(frame):\n",
    "    from pandas import DateOffset\n",
    "    tgt = frame.groupby(\"deal_id\")[\"_entry_dt\"].transform(\"first\")\n",
    "    start = tgt - DateOffset(months=3)\n",
    "    end   = tgt + DateOffset(months=3)\n",
    "    in_window = frame[\"_ref_dt\"].between(start, end, inclusive=\"both\")\n",
    "    tmp = frame.loc[in_window, [\"deal_id\",\"id\",\"_ref_dt\",\"_entry_dt\"]].copy()\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[\"_ref_dt\"] - tmp[\"_entry_dt\"]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[\"_ref_dt\"] >= tmp[\"_entry_dt\"]).astype(int)\n",
    "    tmp = tmp.sort_values([\"deal_id\",\"_abs_diff_days\",\"_is_after_or_eq\",\"_ref_dt\"],\n",
    "                          ascending=[True, True, False, True])\n",
    "    return tmp.groupby(\"deal_id\")[\"id\"].first()\n",
    "\n",
    "entry_verify = entry_winner_verify(ck_un)\n",
    "latest_verify = (ck_un[ck_un[\"_ref_dt\"] <= pd.Timestamp.today().normalize()]\n",
    "                 .sort_values([\"deal_id\",\"_ref_dt\"], ascending=[True, False])\n",
    "                 .groupby(\"deal_id\")[\"id\"].first())\n",
    "\n",
    "for d, grp in ck_un.groupby(\"deal_id\"):\n",
    "    ids = set(grp[\"id\"])\n",
    "    assert d in entry_verify.index and d in latest_verify.index, f\"Deal {d}: missing entry or latest id.\"\n",
    "    assert entry_verify[d] in ids and latest_verify[d] in ids, f\"Deal {d}: kept rows are not entry+latest.\"\n",
    "\n",
    "# Exited: unchanged cardinality constraint (still ≤ 2)\n",
    "rows_per_ex = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_ex <= 2).all(), \"Exited deals show >2 rows after unexited processing.\"\n",
    "\n",
    "print(\"Unexited selection check passed.\")\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "5938b96915ce38b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexited: survivors=202, dropped_coincident=43, rows_now=956 (from 1471).\n",
      "Unexited selection check passed.\n",
      "unique_deals: 478\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:17:04.492840Z",
     "start_time": "2025-10-04T18:17:04.417860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# --- Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "df[\"_exit_dt\"]  = pd.to_datetime(df[\"exit_date\"],     errors=\"coerce\")\n",
    "today = pd.Timestamp.today().normalize()\n",
    "\n",
    "# --- Numeric views\n",
    "ev = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(df[\"net_debt\"],         errors=\"coerce\")\n",
    "eq = pd.to_numeric(df[\"equity\"],           errors=\"coerce\")\n",
    "rev= pd.to_numeric(df[\"revenue\"],          errors=\"coerce\")\n",
    "eb = pd.to_numeric(df[\"ebitda\"],           errors=\"coerce\")\n",
    "\n",
    "# --- Bookkeeping\n",
    "issues = {}\n",
    "\n",
    "def flag(name, bad_index):\n",
    "    bad_ids = df.loc[bad_index, \"deal_id\"].unique().tolist()\n",
    "    issues[name] = bad_ids\n",
    "\n",
    "# 1) Exactly two rows per deal_id\n",
    "rows_per_deal = df.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "bad = rows_per_deal.index[rows_per_deal != 2]\n",
    "flag(\"not_exactly_two_rows\", df[\"deal_id\"].isin(bad))\n",
    "\n",
    "# 2) Required fields present and finite; revenue != 0\n",
    "req_na = ev.isna() | nd.isna() | eq.isna() | rev.isna() | eb.isna()\n",
    "flag(\"missing_required_fields\", req_na)\n",
    "flag(\"revenue_zero\", (rev == 0))\n",
    "\n",
    "# 3) Date-window checks\n",
    "is_exited   = df[\"holding_status\"] == \"exited\"\n",
    "is_unexited = df[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# Windows (inclusive ±3 calendar months)\n",
    "from pandas import DateOffset\n",
    "entry_start = df[\"_entry_dt\"] - DateOffset(months=3)\n",
    "entry_end   = df[\"_entry_dt\"] + DateOffset(months=3)\n",
    "exit_start  = df[\"_exit_dt\"]  - DateOffset(months=3)\n",
    "exit_end    = df[\"_exit_dt\"]  + DateOffset(months=3)\n",
    "\n",
    "in_entry_win = df[\"_ref_dt\"].between(entry_start, entry_end, inclusive=\"both\")\n",
    "in_exit_win  = df[\"_ref_dt\"].between(exit_start,  exit_end,  inclusive=\"both\")\n",
    "le_today     = df[\"_ref_dt\"] <= today\n",
    "\n",
    "# 3a) Exited: exactly one entry-window row AND exactly one exit-window row per deal\n",
    "ex = df[is_exited].copy()\n",
    "ex_grp = ex.groupby(\"deal_id\", as_index=False)\n",
    "ex_count_entry = ex_grp[\"id\"].apply(lambda s: in_entry_win.loc[s.index].sum()).set_index(\"deal_id\")[\"id\"]\n",
    "ex_count_exit  = ex_grp[\"id\"].apply(lambda s: in_exit_win.loc[s.index].sum()).set_index(\"deal_id\")[\"id\"]\n",
    "bad_ex_entry = ex_count_entry.index[ex_count_entry != 1]\n",
    "bad_ex_exit  = ex_count_exit.index[ex_count_exit != 1]\n",
    "flag(\"exited_entry_window_count_!=1\", df[\"deal_id\"].isin(bad_ex_entry) & is_exited)\n",
    "flag(\"exited_exit_window_count_!=1\",  df[\"deal_id\"].isin(bad_ex_exit)  & is_exited)\n",
    "\n",
    "# 3b) Unexited: exactly one entry-window row; exactly one latest<=today row; they must be different\n",
    "un = df[is_unexited].copy()\n",
    "un_grp = un.groupby(\"deal_id\", as_index=False)\n",
    "\n",
    "un_count_entry = un_grp[\"id\"].apply(lambda s: in_entry_win.loc[s.index].sum()).set_index(\"deal_id\")[\"id\"]\n",
    "bad_un_entry_count = un_count_entry.index[un_count_entry != 1]\n",
    "flag(\"unexited_entry_window_count_!=1\", df[\"deal_id\"].isin(bad_un_entry_count) & is_unexited)\n",
    "\n",
    "# latest<=today id per unexited deal\n",
    "un_le_today = un[le_today.loc[un.index]].copy()\n",
    "latest_id = (un_le_today.sort_values([\"deal_id\",\"_ref_dt\"], ascending=[True, False])\n",
    "                       .groupby(\"deal_id\")[\"id\"].first())\n",
    "\n",
    "# deals lacking any ref_date <= today\n",
    "un_deals = un[\"deal_id\"].unique()\n",
    "missing_latest = [d for d in un_deals if d not in latest_id.index]\n",
    "flag(\"unexited_missing_latest_le_today\", df[\"deal_id\"].isin(missing_latest) & is_unexited)\n",
    "\n",
    "# entry-winner id per unexited deal (pick closest within window as earlier)\n",
    "def entry_winner_ids(frame):\n",
    "    tmp = frame.copy()\n",
    "    tmp = tmp[in_entry_win.loc[tmp.index]]\n",
    "    if tmp.empty:\n",
    "        return pd.Series(dtype=object)\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[\"_ref_dt\"] - tmp[\"_entry_dt\"]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[\"_ref_dt\"] >= tmp[\"_entry_dt\"]).astype(int)\n",
    "    tmp = tmp.sort_values([\"deal_id\",\"_abs_diff_days\",\"_is_after_or_eq\",\"_ref_dt\"],\n",
    "                          ascending=[True, True, False, True])\n",
    "    return tmp.groupby(\"deal_id\")[\"id\"].first()\n",
    "\n",
    "entry_id_un = entry_winner_ids(un)\n",
    "\n",
    "# Check distinctness and membership\n",
    "bad_un_distinct = []\n",
    "bad_un_membership = []\n",
    "for d, sub in un.groupby(\"deal_id\"):\n",
    "    if d in bad_un_entry_count or d in missing_latest:\n",
    "        continue  # already flagged above\n",
    "    ids = set(sub[\"id\"])\n",
    "    e_id = entry_id_un.get(d, None)\n",
    "    l_id = latest_id.get(d, None)\n",
    "    if e_id is None or l_id is None:\n",
    "        continue\n",
    "    if e_id == l_id:\n",
    "        bad_un_distinct.append(d)\n",
    "    if e_id not in ids or l_id not in ids:\n",
    "        bad_un_membership.append(d)\n",
    "\n",
    "flag(\"unexited_entry_equals_latest\", df[\"deal_id\"].isin(bad_un_distinct) & is_unexited)\n",
    "flag(\"unexited_missing_entry_or_latest_row\", df[\"deal_id\"].isin(bad_un_membership) & is_unexited)\n",
    "\n",
    "# 4) EV–EqV–ND bridge with 3%*|EV| tolerance (independent identities)\n",
    "abs_floor = 1e-9\n",
    "tol = np.maximum(0.03 * ev.abs(), abs_floor)\n",
    "bad_ev = (ev - (eq + nd)).abs() > tol\n",
    "bad_nd = (nd - (ev - eq)).abs() > tol\n",
    "bad_eq = (eq - (ev - nd)).abs() > tol\n",
    "flag(\"ev_bridge_ev_vs_eq_plus_nd\", bad_ev)\n",
    "flag(\"ev_bridge_nd_vs_ev_minus_eq\", bad_nd)\n",
    "flag(\"ev_bridge_eq_vs_ev_minus_nd\", bad_eq)\n",
    "\n",
    "# 5) Basic key integrity\n",
    "flag(\"id_not_unique\", df[\"id\"].duplicated(keep=False))\n",
    "flag(\"deal_id_null\", df[\"deal_id\"].isna())\n",
    "\n",
    "# --- Summary\n",
    "total_deals = df[\"deal_id\"].nunique()\n",
    "failed = {k: len(v) for k, v in issues.items() if v}\n",
    "print(f\"Deals in sample: {total_deals}\")\n",
    "if failed:\n",
    "    for k, n in failed.items():\n",
    "        print(f\"- {k}: {n} deal(s)\")\n",
    "    # Optional: assert no failures\n",
    "    # assert False, \"QA failed; see counts above.\"\n",
    "else:\n",
    "    print(\"QA passed: all checks satisfied.\")\n"
   ],
   "id": "faa1c952d2a5378c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deals in sample: 478\n",
      "- unexited_entry_window_count_!=1: 7 deal(s)\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
