{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-07T01:15:08.254716Z",
     "start_time": "2025-10-07T01:15:07.933219Z"
    }
   },
   "source": [
    "# === Holding_status, date/revenue filters, EV bridge compute/flag/filter ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ===================== 1) Add holding_status (+ optional exited-only filter) =====================\n",
    "HOLDING_FILTER_MODE = \"both\"   # options: \"both\" (default) | \"exited_only\"\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "df[\"_ord\"] = np.arange(len(df))  # preserve original row order\n",
    "\n",
    "if \"exit_date\" not in df.columns:\n",
    "    raise KeyError(\"Column 'exit_date' not found in working.csv. Run the earlier ADD_COLUMNS step from 'deal' first.\")\n",
    "\n",
    "# holding status\n",
    "norm = df[\"exit_date\"].copy()\n",
    "if norm.dtype == object:\n",
    "    norm = norm.replace({\"\": pd.NA, \"NaT\": pd.NA, \"nat\": pd.NA, \"None\": pd.NA})\n",
    "is_exited = norm.notna()\n",
    "df[\"holding_status\"] = is_exited.map({True: \"exited\", False: \"unexited\"})\n",
    "\n",
    "# enforce deal-level consistency before any filtering\n",
    "status_per_deal = df.groupby(\"deal_id\")[\"holding_status\"].nunique(dropna=False)\n",
    "mixed = status_per_deal[status_per_deal > 1]\n",
    "assert mixed.empty, f\"{len(mixed)} deal_id(s) have mixed exited/unexited rows.\"\n",
    "\n",
    "# optional: exited-only filter (deal-wide)\n",
    "if HOLDING_FILTER_MODE == \"exited_only\":\n",
    "    deal_first_status = df.groupby(\"deal_id\")[\"holding_status\"].first()\n",
    "    keep_deals = set(deal_first_status[deal_first_status == \"exited\"].index)\n",
    "    before_rows, before_deals = len(df), df[\"deal_id\"].nunique()\n",
    "    df = df[df[\"deal_id\"].isin(keep_deals)].copy()\n",
    "    df = df.sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "    print(f\"Mode=exited_only: kept {len(keep_deals)} exited deal(s); \"\n",
    "          f\"dropped {before_deals - len(keep_deals)} unexited. Rows now {len(df)} (from {before_rows}).\")\n",
    "else:\n",
    "    df = df.sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "    print(\"Mode=both: no deal-level filtering applied.\")\n",
    "\n",
    "# persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# post-write checks\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "exited_count = (check[\"holding_status\"] == \"exited\").sum()\n",
    "unexited_count = (check[\"holding_status\"] == \"unexited\").sum()\n",
    "total_after = len(check)\n",
    "assert exited_count + unexited_count == total_after, \"Status coverage failed.\"\n",
    "\n",
    "status_per_deal = check.groupby(\"deal_id\")[\"holding_status\"].nunique(dropna=False)\n",
    "assert (status_per_deal <= 1).all(), \"Found deal(s) with mixed statuses after filtering.\"\n",
    "\n",
    "if HOLDING_FILTER_MODE == \"exited_only\":\n",
    "    deal_status = check.groupby(\"deal_id\")[\"holding_status\"].first()\n",
    "    assert (deal_status == \"exited\").all(), \"Non-exited deal(s) remain in exited_only mode.\"\n",
    "\n",
    "# report unique deals by status\n",
    "by_status = (check.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "                   .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    check[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "\n",
    "# ===================== 2) Filter unreasonable dates =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "ref = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "lower = pd.Timestamp(1980, 1, 1).normalize()\n",
    "q_end = pd.Timestamp.today().to_period(\"Q-DEC\").end_time.normalize()\n",
    "date_ok = ref.notna() & (ref >= lower) & (ref <= q_end)\n",
    "\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[date_ok].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "dropped = before_rows - len(after)\n",
    "print(\n",
    "    f\"Dropped {dropped} rows due to missing/out-of-range reference_date. \"\n",
    "    f\"Kept range [{lower.date()} .. {q_end.date()}].\"\n",
    ")\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "ref2 = pd.to_datetime(check[\"reference_date\"], errors=\"coerce\")\n",
    "assert ref2.notna().all(), \"Found rows with null reference_date.\"\n",
    "assert ((ref2 >= lower) & (ref2 <= q_end)).all(), \"Found dates outside bounds.\"\n",
    "print(f\"FILTER (dates) check passed. Remaining rows: {len(check)}\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ===================== 3) Filter for revenue>0, EBITDA (toggle), and <2 of [EV/ND/Eq] =============\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# ---- toggle: set to True to require strictly positive EBITDA; False to only require non-missing EBITDA\n",
    "REQUIRE_POSITIVE_EBITDA = True\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "rev_num    = num(df[\"revenue\"])\n",
    "ebitda_num = num(df[\"ebitda\"])\n",
    "ev_num     = num(df[\"enterprise_value\"])\n",
    "nd_num     = num(df[\"net_debt\"])\n",
    "eq_num     = num(df[\"equity\"])\n",
    "\n",
    "rev_ok    = rev_num > 0\n",
    "if REQUIRE_POSITIVE_EBITDA:\n",
    "    ebitda_ok = ebitda_num > 0\n",
    "else:\n",
    "    ebitda_ok = ebitda_num.notna()\n",
    "\n",
    "trio_non_null = ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)\n",
    "trio_ok       = trio_non_null >= 2\n",
    "\n",
    "keep_mask = rev_ok & ebitda_ok & trio_ok\n",
    "\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "dropped = before_rows - len(after)\n",
    "mode_str = \"EBITDA>0\" if REQUIRE_POSITIVE_EBITDA else \"non-missing EBITDA\"\n",
    "print(f\"Filtering out {dropped} rows (kept {len(after)} of {before_rows}) using revenue>0, {mode_str}, and ≥2 of [EV, ND, Eq].\")\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "rev_num    = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "ebitda_num = pd.to_numeric(check[\"ebitda\"], errors=\"coerce\")\n",
    "ev_num     = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd_num     = pd.to_numeric(check[\"net_debt\"], errors=\"coerce\")\n",
    "eq_num     = pd.to_numeric(check[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "# Assertions aligned with the new rules\n",
    "assert (rev_num > 0).all(), \"Found rows with revenue <= 0 or non-numeric.\"\n",
    "if REQUIRE_POSITIVE_EBITDA:\n",
    "    assert (ebitda_num > 0).all(), \"Found rows with EBITDA <= 0 or non-numeric.\"\n",
    "else:\n",
    "    assert ebitda_num.notna().all(), \"Found rows with empty EBITDA after filtering.\"\n",
    "assert ((ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)) >= 2).all(), \\\n",
    "       \"Found rows with fewer than two of [enterprise_value, net_debt, equity] present.\"\n",
    "print(\"FILTER (revenue/EBITDA/trio) check passed. Shape:\", check.shape)\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "\n",
    "# ==== 4) EV / ND / Equity bridge: compute missing, flag, and filter (consolidated) ====\n",
    "\n",
    "ENFORCE_POSITIVE_EV_AND_EQ = True\n",
    "# False = Just remove faulty bridges; True = Also drop deals with any EV<=0 or Equity<=0\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# -- load & preserve order\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "before_rows = len(df); before_deals = df[\"deal_id\"].nunique()\n",
    "\n",
    "# -- numeric views\n",
    "ev0 = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd0 = pd.to_numeric(df[\"net_debt\"],         errors=\"coerce\")\n",
    "eq0 = pd.to_numeric(df[\"equity\"],           errors=\"coerce\")\n",
    "\n",
    "# -- (1) compute missing member when exactly two present\n",
    "present_cnt = ev0.notna().astype(int) + nd0.notna().astype(int) + eq0.notna().astype(int)\n",
    "calc_mask = (present_cnt == 2)\n",
    "df[\"EV_bridge_calc\"] = np.where(calc_mask, \"Yes\", \"No\")\n",
    "\n",
    "need_ev = calc_mask & ev0.isna()\n",
    "need_nd = calc_mask & nd0.isna()\n",
    "need_eq = calc_mask & eq0.isna()\n",
    "if need_ev.any(): df.loc[need_ev, \"enterprise_value\"] = (eq0 + nd0)[need_ev]\n",
    "if need_nd.any(): df.loc[need_nd, \"net_debt\"]         = (ev0 - eq0)[need_nd]\n",
    "if need_eq.any(): df.loc[need_eq, \"equity\"]           = (ev0 - nd0)[need_eq]\n",
    "\n",
    "# -- (2) flag residual vs. EV with tolerance\n",
    "ev = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(df[\"net_debt\"],         errors=\"coerce\")\n",
    "eq = pd.to_numeric(df[\"equity\"],           errors=\"coerce\")\n",
    "\n",
    "all3 = ev.notna() & nd.notna() & eq.notna()\n",
    "residual = ev - (eq + nd)\n",
    "tol = 1001.0\n",
    "ok = all3 & (residual.abs() <= tol)\n",
    "\n",
    "df[\"EV_bridge_error\"] = np.where(ok, \"Ok\", \"Error\")\n",
    "df[\"EV_bridge_residual\"] = residual\n",
    "\n",
    "# -- (3) mandatory filter: remove faulty bridges (row-level)\n",
    "before_faulty_rows = len(df)\n",
    "df_ok = df.loc[df[\"EV_bridge_error\"] != \"Error\"].copy()\n",
    "dropped_faulty = before_faulty_rows - len(df_ok)\n",
    "\n",
    "# -- (4) optional deal-wide positivity filter on EV and Equity\n",
    "if ENFORCE_POSITIVE_EV_AND_EQ:\n",
    "    ev_pos = pd.to_numeric(df_ok[\"enterprise_value\"], errors=\"coerce\") > 0\n",
    "    eq_pos = pd.to_numeric(df_ok[\"equity\"],           errors=\"coerce\") > 0\n",
    "    ok_row = ev_pos & eq_pos\n",
    "\n",
    "    # keep only deals where ALL rows pass EV>0 and Eq>0\n",
    "    ok_deal = ok_row.groupby(df_ok[\"deal_id\"]).all()\n",
    "    keep_deals = set(ok_deal[ok_deal].index)\n",
    "\n",
    "    before_pos_deals = df_ok[\"deal_id\"].nunique()\n",
    "    df_ok = df_ok[df_ok[\"deal_id\"].isin(keep_deals)].copy()\n",
    "    dropped_pos_deals = before_pos_deals - len(keep_deals)\n",
    "else:\n",
    "    dropped_pos_deals = 0\n",
    "\n",
    "# -- persist in original order\n",
    "df_ok = df_ok.sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "df_ok.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# -- checks\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "ev_c = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd_c = pd.to_numeric(check[\"net_debt\"],         errors=\"coerce\")\n",
    "eq_c = pd.to_numeric(check[\"equity\"],           errors=\"coerce\")\n",
    "\n",
    "# recompute presence\n",
    "present_cnt_after = ev_c.notna().astype(int) + nd_c.notna().astype(int) + eq_c.notna().astype(int)\n",
    "\n",
    "# rows that had calc \"Yes\" must have all three now\n",
    "yes_mask = (check.get(\"EV_bridge_calc\", pd.Series(index=check.index, data=\"No\")) == \"Yes\")\n",
    "assert (present_cnt_after[yes_mask] == 3).all(), \"Some 'Yes' rows still missing EV/ND/Eq.\"\n",
    "assert (present_cnt_after >= 2).all(), \"Found rows with fewer than two of [EV, ND, Eq].\"\n",
    "assert (check[\"EV_bridge_error\"] != \"Error\").all(), \"Faulty bridges remain after filter.\"\n",
    "\n",
    "if ENFORCE_POSITIVE_EV_AND_EQ:\n",
    "    # every remaining row must satisfy EV>0 and Eq>0, by construction\n",
    "    assert (ev_c > 0).all() and (eq_c > 0).all(), \"Non-positive EV/Eq survived positivity filter.\"\n",
    "\n",
    "# -- reporting\n",
    "print(f\"Computed missing EV/ND/Eq where exactly two present. calc_flag rows: {int(calc_mask.sum())}\")\n",
    "print(f\"Dropped {dropped_faulty} rows with EV_bridge_error == 'Error'.\")\n",
    "if ENFORCE_POSITIVE_EV_AND_EQ:\n",
    "    print(f\"Dropped {dropped_pos_deals} deal(s) due to EV<=0 or Equity<=0 in any row.\")\n",
    "print(f\"Rows now: {len(check)} (from {before_rows}); deals now: {check['deal_id'].nunique()} (from {before_deals}).\")\n",
    "\n",
    "# status counts\n",
    "by_status = (check.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "                   .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    check[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=both: no deal-level filtering applied.\n",
      "unique_deals_exited: 561 unique_deals_unexited: 604 unique_deals_total: 1165\n",
      "Dropped 13 rows due to missing/out-of-range reference_date. Kept range [1980-01-01 .. 2025-12-31].\n",
      "FILTER (dates) check passed. Remaining rows: 4945\n",
      "unique_deals_exited: 561 unique_deals_unexited: 604 unique_deals_total: 1165\n",
      "Filtering out 3031 rows (kept 1914 of 4945) using revenue>0, EBITDA>0, and ≥2 of [EV, ND, Eq].\n",
      "FILTER (revenue/EBITDA/trio) check passed. Shape: (1914, 28)\n",
      "unique_deals_exited: 439 unique_deals_unexited: 357 unique_deals_total: 796\n",
      "Computed missing EV/ND/Eq where exactly two present. calc_flag rows: 814\n",
      "Dropped 471 rows with EV_bridge_error == 'Error'.\n",
      "Dropped 9 deal(s) due to EV<=0 or Equity<=0 in any row.\n",
      "Rows now: 1420 (from 1914); deals now: 610 (from 796).\n",
      "unique_deals_exited: 337 unique_deals_unexited: 273 unique_deals_total: 610\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T01:15:08.375424Z",
     "start_time": "2025-10-07T01:15:08.258984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Date-to-financial matching for exited and unexited deals ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper(s) ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "def select_closest_within_window(frame: pd.DataFrame, target_col: str, ref_col: str = \"_ref_dt\"):\n",
    "    \"\"\"Return a Series indexed by deal_id with the winning 'id' (closest ref within ±3 months).\"\"\"\n",
    "    tgt = frame.groupby(\"deal_id\")[target_col].transform(\"first\")\n",
    "    start = tgt - pd.DateOffset(months=3)\n",
    "    end   = tgt + pd.DateOffset(months=3)\n",
    "\n",
    "    in_window = frame[ref_col].between(start, end, inclusive=\"both\")\n",
    "    tmp = frame.loc[in_window, [\"deal_id\", \"id\", ref_col, target_col]].copy()\n",
    "    if tmp.empty:\n",
    "        return pd.Series(dtype=object)\n",
    "\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[ref_col] - tmp[target_col]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[ref_col] >= tmp[target_col]).astype(int)\n",
    "\n",
    "    tmp_sorted = tmp.sort_values(\n",
    "        [\"deal_id\", \"_abs_diff_days\", \"_is_after_or_eq\", ref_col],\n",
    "        ascending=[True, True, False, True],\n",
    "    )\n",
    "    winners = tmp_sorted.groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    "    return winners  # index: deal_id, values: id\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ===================== 1) Exited deals: keep entry+exit matched rows within ±3 months =====================\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "df[\"_exit_dt\"]  = pd.to_datetime(df[\"exit_date\"],     errors=\"coerce\")\n",
    "\n",
    "# Masks\n",
    "exited    = df[\"holding_status\"] == \"exited\"\n",
    "unexited  = df[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# Only operate on exited deals\n",
    "df_ex = df.loc[exited].copy()\n",
    "\n",
    "# Entry winner (requires entry_date)\n",
    "entry_winners = select_closest_within_window(df_ex, target_col=\"_entry_dt\")\n",
    "\n",
    "# Exit winner (requires exit_date)\n",
    "exit_winners = select_closest_within_window(df_ex, target_col=\"_exit_dt\")\n",
    "\n",
    "# Deals must have both winners to survive\n",
    "entry_ok_deals = set(entry_winners.index)\n",
    "exit_ok_deals  = set(exit_winners.index)\n",
    "survivor_deals = entry_ok_deals & exit_ok_deals\n",
    "\n",
    "# Drop deals where entry winner == exit winner (entry=exit)\n",
    "coincident_deals = {d for d in survivor_deals if entry_winners[d] == exit_winners[d]}\n",
    "if coincident_deals:\n",
    "    print(f\"Removing {len(coincident_deals)} exited deal(s) where entry and exit map to the same id.\")\n",
    "survivor_deals = survivor_deals - coincident_deals\n",
    "\n",
    "# Keep set for exited deals: union of entry+exit winners\n",
    "keep_ids_exited = set(entry_winners.loc[list(survivor_deals)].tolist()) | set(\n",
    "    exit_winners.loc[list(survivor_deals)].tolist()\n",
    ")\n",
    "\n",
    "# Final keep mask:\n",
    "# - keep all rows for unexited deals (untouched)\n",
    "# - for exited deals: keep only winner ids; drop entire deal if not in survivor_deals\n",
    "keep_mask = unexited | (exited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_exited))\n",
    "\n",
    "before_rows = len(df)\n",
    "before_deals_ex = df.loc[exited, \"deal_id\"].nunique()\n",
    "\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\",\"_exit_dt\"] if c in out.columns])\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# Reporting\n",
    "after_rows = len(out)\n",
    "after_deals_ex = out.loc[out[\"holding_status\"]==\"exited\", \"deal_id\"].nunique()\n",
    "dropped_exited_deals = before_deals_ex - after_deals_ex\n",
    "print(\n",
    "    f\"Exited deals kept: {after_deals_ex} (dropped {dropped_exited_deals} with no entry/exit match in ±3 months). \"\n",
    "    f\"Rows now: {after_rows} (from {before_rows}).\"\n",
    ")\n",
    "\n",
    "# --- Validation for exited ---\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# 1) Unexited deals untouched in cardinality of rows per deal (relative order not asserted here)\n",
    "\n",
    "rows_per_deal = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_deal == 2).all(), \"Found exited deals with != 2 kept rows.\"\n",
    "\n",
    "# 3) Verify kept rows are within ±3 months of the respective target dates\n",
    "ck = check.loc[ex_mask].copy()\n",
    "ref = pd.to_datetime(ck[\"reference_date\"], errors=\"coerce\")\n",
    "ent = pd.to_datetime(ck[\"entry_date\"], errors=\"coerce\")\n",
    "exi = pd.to_datetime(ck[\"exit_date\"],  errors=\"coerce\")\n",
    "\n",
    "# Tag each row as 'entry_candidate' or 'exit_candidate' by closeness\n",
    "abs_diff_entry = (ref - ent).abs()\n",
    "abs_diff_exit  = (ref - exi).abs()\n",
    "is_entry_like = abs_diff_entry <= abs_diff_exit\n",
    "\n",
    "from pandas import DateOffset\n",
    "ok_window = (\n",
    "    (is_entry_like & ref.between(ent - DateOffset(months=3), ent + DateOffset(months=3), inclusive=\"both\")) |\n",
    "    (~is_entry_like & ref.between(exi - DateOffset(months=3), exi + DateOffset(months=3), inclusive=\"both\"))\n",
    ")\n",
    "assert ok_window.all(), \"Kept exited rows outside ±3 months window.\"\n",
    "\n",
    "# 4) Summarize counts\n",
    "two_rows = int((rows_per_deal == 2).sum())\n",
    "one_row  = int((rows_per_deal == 1).sum())\n",
    "print(f\"Check passed. Exited deals with 2 rows: {two_rows}; with 1 row (entry=exit candidate): {one_row}.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n",
    "\n",
    "# ===================== 2) Unexited deals: keep entry match (±3 months) + latest ref_date ≤ today =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "\n",
    "today = pd.Timestamp.today().normalize()\n",
    "\n",
    "is_unexited = df[\"holding_status\"] == \"unexited\"\n",
    "is_exited   = df[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Work only on unexited deals\n",
    "df_un = df.loc[is_unexited].copy()\n",
    "\n",
    "# Entry winner within ±3 months (required)\n",
    "entry_winners_un = select_closest_within_window(df_un, target_col=\"_entry_dt\")\n",
    "entry_ok_deals = set(entry_winners_un.index)\n",
    "\n",
    "# Latest ref_date ≤ today (required)\n",
    "ref_le_today = df_un[df_un[\"_ref_dt\"] <= today].copy()\n",
    "latest_ids = (\n",
    "    ref_le_today.sort_values([\"deal_id\", \"_ref_dt\"], ascending=[True, False])\n",
    "                .groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    ")\n",
    "latest_ok_deals = set(latest_ids.index)\n",
    "\n",
    "# Survivors must have both entry match and a latest≤today\n",
    "survivor_deals = entry_ok_deals & latest_ok_deals\n",
    "\n",
    "# Drop deals where entry winner id == latest id\n",
    "coincident = {d for d in survivor_deals if entry_winners_un[d] == latest_ids[d]}\n",
    "survivor_deals -= coincident\n",
    "\n",
    "# Keep exactly the two ids (entry+latest) for survivors; leave exited deals untouched\n",
    "keep_ids_un = set(entry_winners_un.loc[list(survivor_deals)].tolist()) | set(latest_ids.loc[list(survivor_deals)].tolist())\n",
    "\n",
    "keep_mask = is_exited | (is_unexited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_un))\n",
    "\n",
    "before_rows = len(df)\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers and save\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\"] if c in out.columns])\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "print(f\"Unexited: survivors={len(survivor_deals)}, dropped_coincident={len(coincident)}, rows_now={len(out)} (from {before_rows}).\")\n",
    "\n",
    "# --- Validation for unexited & global constraints ---\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Unexited: exactly 2 rows per deal_id\n",
    "rows_per_un = check.loc[un_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_un == 2).all(), \"Unexited deals must have exactly 2 rows.\"\n",
    "\n",
    "# Validate the two rows are entry-match and latest≤today\n",
    "ck_un = check.loc[un_mask].copy()\n",
    "ck_un[\"_ref_dt\"] = pd.to_datetime(ck_un[\"reference_date\"], errors=\"coerce\")\n",
    "ck_un[\"_entry_dt\"] = pd.to_datetime(ck_un[\"entry_date\"], errors=\"coerce\")\n",
    "\n",
    "def entry_winner_verify(frame):\n",
    "    from pandas import DateOffset\n",
    "    tgt = frame.groupby(\"deal_id\")[\"_entry_dt\"].transform(\"first\")\n",
    "    start = tgt - DateOffset(months=3)\n",
    "    end   = tgt + DateOffset(months=3)\n",
    "    in_window = frame[\"_ref_dt\"].between(start, end, inclusive=\"both\")\n",
    "    tmp = frame.loc[in_window, [\"deal_id\",\"id\",\"_ref_dt\",\"_entry_dt\"]].copy()\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[\"_ref_dt\"] - tmp[\"_entry_dt\"]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[\"_ref_dt\"] >= tmp[\"_entry_dt\"]).astype(int)\n",
    "    tmp = tmp.sort_values([\"deal_id\",\"_abs_diff_days\",\"_is_after_or_eq\",\"_ref_dt\"],\n",
    "                          ascending=[True, True, False, True])\n",
    "    return tmp.groupby(\"deal_id\")[\"id\"].first()\n",
    "\n",
    "entry_verify = entry_winner_verify(ck_un)\n",
    "latest_verify = (ck_un[ck_un[\"_ref_dt\"] <= pd.Timestamp.today().normalize()]\n",
    "                 .sort_values([\"deal_id\",\"_ref_dt\"], ascending=[True, False])\n",
    "                 .groupby(\"deal_id\")[\"id\"].first())\n",
    "\n",
    "for d, grp in ck_un.groupby(\"deal_id\"):\n",
    "    ids = set(grp[\"id\"])\n",
    "    assert d in entry_verify.index and d in latest_verify.index, f\"Deal {d}: missing entry or latest id.\"\n",
    "    assert entry_verify[d] in ids and latest_verify[d] in ids, f\"Deal {d}: kept rows are not entry+latest.\"\n",
    "\n",
    "# Exited: unchanged cardinality constraint (still ≤ 2)\n",
    "rows_per_ex = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "\n",
    "# Deal-level row-count report for unexited\n",
    "two_rows_un  = int((rows_per_un == 2).sum())\n",
    "one_row_un   = int((rows_per_un == 1).sum())\n",
    "gt2_rows_un  = int((rows_per_un > 2).sum())\n",
    "total_un     = int(rows_per_un.size)\n",
    "\n",
    "print(f\"Unexited row-counts per deal_id — 2 rows: {two_rows_un}, 1 row: {one_row_un}, >2 rows: {gt2_rows_un}, total: {total_un}\")\n",
    "\n",
    "# Keep your hard guarantees\n",
    "assert one_row_un == 0, \"Unexited deals with exactly 1 row found.\"\n",
    "assert gt2_rows_un == 0, \"Unexited deals with >2 rows found.\"\n",
    "assert (rows_per_un == 2).all(), \"Unexited deals must have exactly 2 rows.\"\n",
    "assert (rows_per_ex <= 2).all(), \"Exited deals show >2 rows after unexited processing.\"\n",
    "\n",
    "# Deal-level row-count report for ALL deals (exited + unexited)\n",
    "rows_per_all = check.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "two_rows_all = int((rows_per_all == 2).sum())\n",
    "one_row_all  = int((rows_per_all == 1).sum())\n",
    "gt2_rows_all = int((rows_per_all > 2).sum())\n",
    "total_all    = int(rows_per_all.size)\n",
    "\n",
    "# ---------- Step 3b: Enforce one currency per deal_id (drop mixed/unknown) ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "if \"reporting_currency_financials\" not in df.columns:\n",
    "    raise KeyError(\"Missing 'reporting_currency_financials'—ensure Step 3 added it.\")\n",
    "\n",
    "cur = df[\"reporting_currency_financials\"].astype(str).str.strip()\n",
    "# Treat \"\", \"nan\", \"NaN\" as null\n",
    "cur = cur.mask(cur.str.lower().isin({\"\", \"nan\"}))\n",
    "df[\"_currency\"] = cur\n",
    "\n",
    "# Rule 1: require a defined currency for every row\n",
    "rows_with_null_cur = df[\"_currency\"].isna().sum()\n",
    "\n",
    "# Rule 2: require exactly one distinct currency per deal_id\n",
    "per_deal_nuniq = df.groupby(\"deal_id\")[\"_currency\"].nunique(dropna=False)\n",
    "bad_deals = per_deal_nuniq[(per_deal_nuniq != 1)].index\n",
    "\n",
    "keep_mask = df[\"deal_id\"].isin(bad_deals) == False\n",
    "keep_mask &= df[\"_currency\"].notna()\n",
    "\n",
    "before_deals = df[\"deal_id\"].nunique()\n",
    "before_rows  = len(df)\n",
    "\n",
    "out = df.loc[keep_mask].drop(columns=[\"_currency\"]).reset_index(drop=True)\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "after_deals = out[\"deal_id\"].nunique()\n",
    "after_rows  = len(out)\n",
    "\n",
    "print(f\"Currency consistency: dropped {before_deals - after_deals} deal_id(s) with mixed/unknown currencies; \"\n",
    "      f\"rows: {after_rows} (from {before_rows}).\")\n",
    "\n",
    "# Hard assertions\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "assert check[\"reporting_currency_financials\"].notna().all(), \"Null currency remains.\"\n",
    "nu = check.groupby(\"deal_id\")[\"reporting_currency_financials\"].nunique()\n",
    "assert (nu == 1).all(), \"Found deal(s) with multiple currencies.\"\n",
    "print(\"CURRENCY check passed.\")\n",
    "\n",
    "\n",
    "print(f\"All deals — 2 rows: {two_rows_all}, 1 row: {one_row_all}, >2 rows: {gt2_rows_all}, total: {total_all}\")\n",
    "\n",
    "# Hard guarantees across the whole dataset\n",
    "assert one_row_all == 0, \"Found deals with exactly 1 row.\"\n",
    "assert gt2_rows_all == 0, \"Found deals with >2 rows.\"\n",
    "assert (rows_per_all == 2).all(), \"All deals must have exactly 2 rows.\"\n",
    "\n",
    "print(\"Unexited selection check passed.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "id": "394a8f291076b99f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 2 exited deal(s) where entry and exit map to the same id.\n",
      "Exited deals kept: 215 (dropped 122 with no entry/exit match in ±3 months). Rows now: 1137 (from 1420).\n",
      "Check passed. Exited deals with 2 rows: 215; with 1 row (entry=exit candidate): 0.\n",
      "unique_deals_exited: 215 unique_deals_unexited: 273 unique_deals_total: 488\n",
      "unique_deals: 488\n",
      "Unexited: survivors=149, dropped_coincident=35, rows_now=728 (from 1137).\n",
      "Unexited row-counts per deal_id — 2 rows: 149, 1 row: 0, >2 rows: 0, total: 149\n",
      "Currency consistency: dropped 1 deal_id(s) with mixed/unknown currencies; rows: 726 (from 728).\n",
      "CURRENCY check passed.\n",
      "All deals — 2 rows: 364, 1 row: 0, >2 rows: 0, total: 364\n",
      "Unexited selection check passed.\n",
      "unique_deals_exited: 215 unique_deals_unexited: 148 unique_deals_total: 363\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T01:15:08.445904Z",
     "start_time": "2025-10-07T01:15:08.384251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Calculate metrics for analysis + then drop unreasonable metrics ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "\n",
    "# ===================== 1) Add calculated columns (row-wise) =====================\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "#Exclude a faulty deal:\n",
    "df = df.loc[df[\"deal_id\"] != \"83283299-100b-44c6-a997-ec634d90768d\"].copy()\n",
    "df = df.loc[df[\"deal_id\"] != \"2376a9b2-d983-48fc-b62a-52fb78bf1409\"].copy()\n",
    "df = df.loc[df[\"deal_id\"] != \"98cb6742-e749-4344-85ab-5d5713d30e59\"].copy()\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "ev = num(df[\"enterprise_value\"])\n",
    "eb = num(df[\"ebitda\"])\n",
    "nd = num(df[\"net_debt\"])\n",
    "eq = num(df[\"equity\"])\n",
    "rv = num(df[\"revenue\"])\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    ebitda_margin = (eb / rv).where(rv != 0)\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    xebitda = (ev / eb).where(eb != 0)\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    de_ratio = (nd / eq).where(eq != 0)\n",
    "\n",
    "# --- NEW: dividends & capital_injections at DEAL level from EXIT row (latest ref_date) ---\n",
    "df[\"_ref_dt\"] = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "# rank within deal: 1 = entry (earliest), 2 = exit (latest)\n",
    "df[\"_rank\"] = df.groupby(\"deal_id\")[\"_ref_dt\"].rank(method=\"first\", ascending=True)\n",
    "\n",
    "# enterprise_value at exit row (rank 2), per deal\n",
    "exit_eq = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "             .assign(exit_eq=eq[df[\"_rank\"] == 2].values)\n",
    "             .groupby(\"deal_id\")[\"exit_eq\"].first())\n",
    "\n",
    "# broadcast to both rows of each deal\n",
    "dividends = df[\"deal_id\"].map((0.71481481481 * exit_eq).to_dict()) #0.1481481481\n",
    "capital_injections = df[\"deal_id\"].map((-0.1111111111 * exit_eq).to_dict()) #-0.1111111111\n",
    "# --- END NEW ---\n",
    "\n",
    "interest_rate = pd.Series(0.09, index=df.index)\n",
    "\n",
    "today = pd.Timestamp.today().normalize()\n",
    "ent = pd.to_datetime(df.get(\"entry_date\"), errors=\"coerce\")\n",
    "exi = pd.to_datetime(df.get(\"exit_date\"),  errors=\"coerce\")\n",
    "is_exited = (df[\"holding_status\"] == \"exited\")\n",
    "\n",
    "days = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "days.loc[is_exited] = (exi - ent).dt.days.loc[is_exited]\n",
    "days.loc[~is_exited] = (today - ent).dt.days.loc[~is_exited]\n",
    "days = days.where(days >= 0)\n",
    "holding_period = days / 365.25\n",
    "\n",
    "cost_of_debt = (1 + interest_rate) ** holding_period - 1\n",
    "\n",
    "df[\"ebitda_margin\"]      = ebitda_margin\n",
    "df[\"xebitda\"]            = xebitda\n",
    "df[\"de_ratio\"]           = de_ratio\n",
    "df[\"dividends\"]          = dividends\n",
    "df[\"capital_injections\"] = capital_injections\n",
    "df[\"interest_rate\"]      = interest_rate\n",
    "df[\"holding_period\"]     = holding_period\n",
    "df[\"cost_of_debt\"]       = cost_of_debt\n",
    "\n",
    "# clean helper cols\n",
    "df = df.drop(columns=[\"_ref_dt\", \"_rank\"])\n",
    "\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "g = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "new_cols = [\"ebitda_margin\",\"xebitda\",\"de_ratio\",\"dividends\",\"capital_injections\",\"interest_rate\",\"holding_period\",\"cost_of_debt\"]\n",
    "nn = {c: int(g[c].notna().sum()) for c in new_cols}\n",
    "print(\"Added columns:\", \", \".join(new_cols))\n",
    "print(\"Non-null counts:\", nn)\n",
    "\n",
    "# ensure availability of by_status for the print (matches prior cells’ semantics)\n",
    "by_status = (g.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ===================== 2) Drop deals with unreasonable financial metrics =====================\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "before_deals = df[\"deal_id\"].nunique()\n",
    "\n",
    "em  = pd.to_numeric(df[\"ebitda_margin\"], errors=\"coerce\")\n",
    "xe  = pd.to_numeric(df[\"xebitda\"],        errors=\"coerce\")\n",
    "der = pd.to_numeric(df[\"de_ratio\"],       errors=\"coerce\")\n",
    "\n",
    "v_em  = em > 1\n",
    "v_xe  = xe.isna() | (xe < -1000) | (xe > 1000)\n",
    "v_der = der.isna() | (der < -1)   | (der > 20)\n",
    "\n",
    "bad_em_deals  = set(df.loc[v_em,  \"deal_id\"].dropna().unique())\n",
    "bad_xe_deals  = set(df.loc[v_xe,  \"deal_id\"].dropna().unique())\n",
    "bad_der_deals = set(df.loc[v_der, \"deal_id\"].dropna().unique())\n",
    "bad_deals = bad_em_deals | bad_xe_deals | bad_der_deals\n",
    "\n",
    "keep_mask = ~df[\"deal_id\"].isin(bad_deals)\n",
    "out = df.loc[keep_mask].reset_index(drop=True)\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "after_rows  = len(out)\n",
    "after_deals = out[\"deal_id\"].nunique()\n",
    "print(\n",
    "    f\"Dropped {before_deals - after_deals} deal_id(s). \"\n",
    "    f\"[ebitda_margin>1: {len(bad_em_deals)}, xebitda null/|>|1000: {len(bad_xe_deals)}, \"\n",
    "    f\"de_ratio null/<-1/>20: {len(bad_der_deals)}]  Rows: {after_rows} (from {before_rows}).\"\n",
    ")\n",
    "\n",
    "# ---- checks ----\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "em2  = pd.to_numeric(check[\"ebitda_margin\"], errors=\"coerce\")\n",
    "xe2  = pd.to_numeric(check[\"xebitda\"],        errors=\"coerce\")\n",
    "der2 = pd.to_numeric(check[\"de_ratio\"],       errors=\"coerce\")\n",
    "assert not (em2 > 1).any(), \"Remaining rows with ebitda_margin > 1.\"\n",
    "assert not (xe2.isna() | (xe2 < -1000) | (xe2 > 1000)).any(), \"Remaining rows with invalid xebitda.\"\n",
    "assert not (der2.isna() | (der2 < -1) | (der2 > 20)).any(), \"Remaining rows with invalid de_ratio.\"\n",
    "rows_per_deal = check.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_deal == 2).all(), \"Each remaining deal must have exactly 2 rows.\"\n",
    "\n",
    "g = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "id": "dbaee67da1b8de4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added columns: ebitda_margin, xebitda, de_ratio, dividends, capital_injections, interest_rate, holding_period, cost_of_debt\n",
      "Non-null counts: {'ebitda_margin': 720, 'xebitda': 720, 'de_ratio': 720, 'dividends': 720, 'capital_injections': 720, 'interest_rate': 720, 'holding_period': 720, 'cost_of_debt': 720}\n",
      "unique_deals_exited: 213 unique_deals_unexited: 147 unique_deals_total: 360\n",
      "Dropped 4 deal_id(s). [ebitda_margin>1: 1, xebitda null/|>|1000: 2, de_ratio null/<-1/>20: 1]  Rows: 712 (from 720).\n",
      "unique_deals_exited: 211 unique_deals_unexited: 145 unique_deals_total: 356\n"
     ]
    }
   ],
   "execution_count": 127
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
