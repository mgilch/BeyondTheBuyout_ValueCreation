{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#----- Start the data transformation by adding \"holding_status\" -----#\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    # Helpful diagnostics\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "before_rows = len(df)\n",
    "\n",
    "# Normalize exit_date blanks to NA (handles \"\", \"NaT\" strings)\n",
    "if \"exit_date\" not in df.columns:\n",
    "    raise KeyError(\"Column 'exit_date' not found in working.csv. Run the earlier ADD_COLUMNS step from 'deal' first.\")\n",
    "norm = df[\"exit_date\"].copy()\n",
    "if norm.dtype == object:\n",
    "    norm = norm.replace({\"\": pd.NA, \"NaT\": pd.NA, \"nat\": pd.NA, \"None\": pd.NA})\n",
    "\n",
    "# Determine holding_status: any non-null exit_date => exited; else unexited\n",
    "is_exited = norm.notna()\n",
    "df[\"holding_status\"] = is_exited.map({True: \"exited\", False: \"unexited\"})\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(\"Added column: holding_status\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "\n",
    "# Row-level coverage\n",
    "exited_count = (check[\"holding_status\"] == \"exited\").sum()\n",
    "unexited_count = (check[\"holding_status\"] == \"unexited\").sum()\n",
    "total_after = len(check)\n",
    "assert exited_count + unexited_count == total_after, \"Status coverage failed: counts don't add up to total rows.\"\n",
    "\n",
    "# Deal-level consistency: each deal_id should have a single status\n",
    "status_per_deal = check.groupby(\"deal_id\")[\"holding_status\"].nunique(dropna=False)\n",
    "mixed = status_per_deal[status_per_deal > 1]\n",
    "assert mixed.empty, f\"{len(mixed)} deal_id(s) have mixed exited/unexited rows.\"\n",
    "\n",
    "# Deal-level split\n",
    "deal_status = check.groupby(\"deal_id\")[\"holding_status\"].first()\n",
    "deal_exited = (deal_status == \"exited\").sum()\n",
    "deal_unexited = (deal_status == \"unexited\").sum()\n",
    "total_deals = deal_status.size\n",
    "\n",
    "print(f\"Check passed. exited={exited_count}, unexited={unexited_count}, total_rows={total_after}\")\n",
    "\n",
    "print(f\"Deal-level: exited_deals={deal_exited}, unexited_deals={deal_unexited}, total_unique_deal_ids={total_deals}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "#----- Filter for unreasonable dates -----#\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# Parse dates (strict for presence; tolerant for format)\n",
    "ref = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "lower = pd.Timestamp(1980, 1, 1).normalize()\n",
    "q_end = pd.Timestamp.today().to_period(\"Q-DEC\").end_time.normalize()\n",
    "\n",
    "# reference_date must be present and within bounds\n",
    "date_ok = ref.notna() & (ref >= lower) & (ref <= q_end)\n",
    "\n",
    "# Revenue must be strictly > 0 (non-numeric/NA will be coerced to NaN and dropped)\n",
    "rev_num = pd.to_numeric(df[\"revenue\"], errors=\"coerce\")\n",
    "rev_ok = rev_num > 0\n",
    "\n",
    "# Keep only rows meeting both constraints\n",
    "keep_mask = date_ok & rev_ok\n",
    "\n",
    "# Preserve original order\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "\n",
    "dropped = before_rows - len(after)\n",
    "print(\n",
    "    f\"Dropped {dropped} rows due to missing/out-of-range reference_date \"\n",
    "    f\"or revenue <= 0 / non-numeric. Kept range [{lower.date()} .. {q_end.date()}].\"\n",
    ")\n",
    "\n",
    "# Save\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "ref2 = pd.to_datetime(check[\"reference_date\"], errors=\"coerce\")\n",
    "# 1) No null reference_date\n",
    "assert ref2.notna().all(), \"Found rows with null reference_date.\"\n",
    "# 2) Dates within bounds\n",
    "assert ((ref2 >= lower) & (ref2 <= q_end)).all(), \"Found dates outside bounds.\"\n",
    "# 3) Revenue strictly > 0\n",
    "rev2 = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "assert (rev2 > 0).all(), \"Found rows with revenue <= 0 or non-numeric.\"\n",
    "print(f\"FILTER check passed. Remaining rows: {len(check)}\")\n",
    "\n",
    "print(f\"Deal-level: exited_deals={deal_exited}, unexited_deals={deal_unexited}, total_unique_deal_ids={total_deals}\")\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "635e1b3e20b82d56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Filter out rows that have no revenue or no EBITDA and check EV-EqV Bridge for calculatability -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# Temporary numeric views for presence checks (does not change df)\n",
    "def num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "rev_num   = num(df[\"revenue\"])\n",
    "ebitda_num= num(df[\"ebitda\"])\n",
    "ev_num    = num(df[\"enterprise_value\"])\n",
    "nd_num    = num(df[\"net_debt\"])\n",
    "eq_num    = num(df[\"equity\"])\n",
    "\n",
    "# Presence logic\n",
    "rev_ok    = rev_num.notna()\n",
    "ebitda_ok = ebitda_num.notna()\n",
    "trio_non_null = ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)\n",
    "trio_ok   = trio_non_null >= 2\n",
    "\n",
    "keep_mask = rev_ok & ebitda_ok & trio_ok\n",
    "\n",
    "# Preserve original order exactly\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "\n",
    "dropped = before_rows - len(after)\n",
    "print(f\"Filtering out {dropped} rows (kept {len(after)} of {before_rows}).\")\n",
    "\n",
    "# Save\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "# Recompute numeric presence on the saved data for verification\n",
    "rev_num   = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "ebitda_num= pd.to_numeric(check[\"ebitda\"], errors=\"coerce\")\n",
    "ev_num    = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd_num    = pd.to_numeric(check[\"net_debt\"], errors=\"coerce\")\n",
    "eq_num    = pd.to_numeric(check[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "assert rev_num.notna().all(), \"Found rows with empty revenue after filtering.\"\n",
    "assert ebitda_num.notna().all(), \"Found rows with empty ebitda after filtering.\"\n",
    "assert ((ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)) >= 2).all(), \\\n",
    "       \"Found rows with fewer than two of [enterprise_value, net_debt, equity] present.\"\n",
    "\n",
    "print(\"FILTER check passed. Shape:\", check.shape)\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "fbe9eecaf8b30ee7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Calculate missing EV-EqV bridge items -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# Numeric views for calculations (do not mutate these)\n",
    "ev0 = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd0 = pd.to_numeric(df[\"net_debt\"], errors=\"coerce\")\n",
    "eq0 = pd.to_numeric(df[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "# Count how many of the trio are present (non-null)\n",
    "present_cnt = ev0.notna().astype(int) + nd0.notna().astype(int) + eq0.notna().astype(int)\n",
    "calc_mask = (present_cnt == 2)\n",
    "\n",
    "# Initialize flag: Yes if we will compute a missing third, else No\n",
    "df[\"EV_bridge_calc\"] = np.where(calc_mask, \"Yes\", \"No\")\n",
    "\n",
    "# Compute missing field using the other two (using original, unmodified numeric series)\n",
    "need_ev = calc_mask & ev0.isna()\n",
    "need_nd = calc_mask & nd0.isna()\n",
    "need_eq = calc_mask & eq0.isna()\n",
    "\n",
    "df.loc[need_ev, \"enterprise_value\"] = (eq0 + nd0)[need_ev]\n",
    "df.loc[need_nd, \"net_debt\"]         = (ev0 - eq0)[need_nd]\n",
    "df.loc[need_eq, \"equity\"]           = (ev0 - nd0)[need_eq]\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Computed missing EV/NetDebt/Equity for {calc_mask.sum()} rows. Added 'EV_bridge_calc'.\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Re-evaluate presence after computation\n",
    "ev = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(check[\"net_debt\"], errors=\"coerce\")\n",
    "eq = pd.to_numeric(check[\"equity\"], errors=\"coerce\")\n",
    "present_cnt_after = ev.notna().astype(int) + nd.notna().astype(int) + eq.notna().astype(int)\n",
    "\n",
    "# 1) Rows that were marked \"Yes\" now have all three present\n",
    "yes_mask = (check[\"EV_bridge_calc\"] == \"Yes\")\n",
    "assert (present_cnt_after[yes_mask] == 3).all(), \"Some 'Yes' rows still missing a value.\"\n",
    "\n",
    "# 2) All rows still meet the earlier rule (>= 2 present)\n",
    "assert (present_cnt_after >= 2).all(), \"Found rows with fewer than two of the trio present.\"\n",
    "\n",
    "# 3) Row count preserved\n",
    "assert len(check) == before_rows, \"Row count changed.\"\n",
    "\n",
    "print(f\"COMPUTE check passed. Bridges computed: {yes_mask.sum()}, total rows: {len(check)}.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "f4befa8497d8effc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Flag EV-EqV bridge vs actual values (abs tol = 1001) -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Numeric views\n",
    "ev = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(df[\"net_debt\"], errors=\"coerce\")\n",
    "eq = pd.to_numeric(df[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "# Require all three present\n",
    "all3 = ev.notna() & nd.notna() & eq.notna()\n",
    "\n",
    "# Compare computed to recorded (actual) EV\n",
    "residual = ev - (eq + nd)\n",
    "tol = 1001.0  # fixed absolute tolerance\n",
    "\n",
    "ok = all3 & (residual.abs() <= tol)\n",
    "df[\"EV_bridge_error\"] = np.where(ok, \"Ok\", \"Error\")\n",
    "df[\"EV_bridge_residual\"] = residual  # optional: keep for diagnostics\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "ok_count = int((df[\"EV_bridge_error\"] == \"Ok\").sum())\n",
    "err_count = int((df[\"EV_bridge_error\"] == \"Error\").sum())\n",
    "print(f\"Bridge check vs actual EV (abs tol={tol:g}): Ok={ok_count}, Error={err_count}, Total={len(df)}\")\n",
    "\n",
    "# Verify coverage after reload\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "vc = check[\"EV_bridge_error\"].value_counts()\n",
    "ok = int(vc.get(\"Ok\", 0)); err = int(vc.get(\"Error\", 0))\n",
    "assert ok + err == len(check), \"Flags do not cover all rows.\"\n",
    "print(f\"Check passed. Ok={ok}, Error={err}, Total={len(check)}\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"),\n",
    "                                   dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "31c60498be2675ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#----- Remove faulty EV-EqV bridges -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "before = len(df)\n",
    "\n",
    "keep_mask = df[\"EV_bridge_error\"] != \"Error\"\n",
    "after = df.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "dropped = before - len(after)\n",
    "print(f\"Dropped {dropped} rows with EV_bridge_error == 'Error'. Kept {len(after)} of {before}.\")\n",
    "\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert (check[\"EV_bridge_error\"] != \"Error\").all(), \"Found remaining rows with EV_bridge_error == 'Error'.\"\n",
    "print(\"FILTER check passed. Rows:\", len(check))\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "9747c88dca9082eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- For exited deals: Entry/ exit date <---> financial data matching -----#\n",
    "\n",
    "#-----\n",
    "\"\"\"Every unique deal_id with holding_status == \"exited\" should have only two rows (i.e. id). And these rows should be the ones with the reference date matched as closely as possible to the entry and exit date, if a reference_date exists within a +-3 month window. All deal_id with holding_status == \"unexited\" should not be touched by the exit operation.\"\"\"\n",
    "#-----\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "df[\"_exit_dt\"]  = pd.to_datetime(df[\"exit_date\"],     errors=\"coerce\")\n",
    "\n",
    "# Masks\n",
    "exited    = df[\"holding_status\"] == \"exited\"\n",
    "unexited  = df[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# --- Helper: select closest row to a target date within ±3 calendar months (inclusive)\n",
    "def select_closest_within_window(frame: pd.DataFrame, target_col: str, ref_col: str = \"_ref_dt\"):\n",
    "    \"\"\"Return a Series indexed by deal_id with the winning 'id' (closest ref within ±3 months).\"\"\"\n",
    "    # Target per deal\n",
    "    tgt = frame.groupby(\"deal_id\")[target_col].transform(\"first\")\n",
    "    start = tgt - pd.DateOffset(months=3)\n",
    "    end   = tgt + pd.DateOffset(months=3)\n",
    "\n",
    "    in_window = frame[ref_col].between(start, end, inclusive=\"both\")\n",
    "\n",
    "    # Tie-breakers: 1) min abs diff  2) prefer on/after target  3) earliest ref\n",
    "    tmp = frame.loc[in_window, [\"deal_id\", \"id\", ref_col, target_col]].copy()\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[ref_col] - tmp[target_col]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[ref_col] >= tmp[target_col]).astype(int)\n",
    "\n",
    "    tmp_sorted = tmp.sort_values(\n",
    "        [\"deal_id\", \"_abs_diff_days\", \"_is_after_or_eq\", ref_col],\n",
    "        ascending=[True, True, False, True],\n",
    "    )\n",
    "    # Winner per deal_id\n",
    "    winners = tmp_sorted.groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    "    return winners  # index: deal_id, values: id\n",
    "\n",
    "# Only operate on exited deals\n",
    "df_ex = df.loc[exited].copy()\n",
    "\n",
    "# Entry winner (requires entry_date)\n",
    "entry_winners = select_closest_within_window(df_ex, target_col=\"_entry_dt\")\n",
    "\n",
    "# Exit winner (requires exit_date)\n",
    "exit_winners = select_closest_within_window(df_ex, target_col=\"_exit_dt\")\n",
    "\n",
    "# Deals must have both winners to survive\n",
    "entry_ok_deals = set(entry_winners.index)\n",
    "exit_ok_deals  = set(exit_winners.index)\n",
    "survivor_deals = entry_ok_deals & exit_ok_deals\n",
    "\n",
    "# --- NEW: drop deals where entry winner == exit winner (entry=exit)\n",
    "coincident_deals = {d for d in survivor_deals if entry_winners[d] == exit_winners[d]}\n",
    "if coincident_deals:\n",
    "    print(f\"Removing {len(coincident_deals)} exited deal(s) where entry and exit map to the same id.\")\n",
    "survivor_deals = survivor_deals - coincident_deals\n",
    "# --- END NEW\n",
    "\n",
    "# Build set of ids to keep for exited deals: union of entry+exit winners per surviving deal\n",
    "keep_ids_exited = set(entry_winners.loc[list(survivor_deals)].tolist()) | set(\n",
    "    exit_winners.loc[list(survivor_deals)].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "# Final keep mask:\n",
    "# - keep all rows for unexited deals (untouched)\n",
    "# - for exited deals: keep only winner ids; drop entire deal if not in survivor_deals\n",
    "keep_mask = unexited | (exited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_exited))\n",
    "\n",
    "before_rows = len(df)\n",
    "before_deals_ex = df.loc[exited, \"deal_id\"].nunique()\n",
    "\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\",\"_exit_dt\"] if c in out.columns])\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# Reporting\n",
    "after_rows = len(out)\n",
    "after_deals_ex = out.loc[out[\"holding_status\"]==\"exited\", \"deal_id\"].nunique()\n",
    "dropped_exited_deals = before_deals_ex - after_deals_ex\n",
    "print(\n",
    "    f\"Exited deals kept: {after_deals_ex} (dropped {dropped_exited_deals} with no entry/exit match in ±3 months). \"\n",
    "    f\"Rows now: {after_rows} (from {before_rows}).\"\n",
    ")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# 1) Unexited deals untouched in cardinality of rows per deal (relative order not asserted here)\n",
    "\n",
    "rows_per_deal = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_deal == 2).all(), \"Found exited deals with != 2 kept rows.\"\n",
    "\n",
    "\n",
    "# 3) Verify kept rows are within ±3 months of the respective target dates\n",
    "ck = check.loc[ex_mask].copy()\n",
    "ref = pd.to_datetime(ck[\"reference_date\"], errors=\"coerce\")\n",
    "ent = pd.to_datetime(ck[\"entry_date\"], errors=\"coerce\")\n",
    "exi = pd.to_datetime(ck[\"exit_date\"],  errors=\"coerce\")\n",
    "\n",
    "# Tag each row as 'entry_candidate' or 'exit_candidate' by closeness\n",
    "abs_diff_entry = (ref - ent).abs()\n",
    "abs_diff_exit  = (ref - exi).abs()\n",
    "is_entry_like = abs_diff_entry <= abs_diff_exit\n",
    "\n",
    "# Both must be within ±3 months relative to whichever they represent.\n",
    "from pandas import DateOffset\n",
    "ok_window = (\n",
    "    (is_entry_like & ref.between(ent - DateOffset(months=3), ent + DateOffset(months=3), inclusive=\"both\")) |\n",
    "    (~is_entry_like & ref.between(exi - DateOffset(months=3), exi + DateOffset(months=3), inclusive=\"both\"))\n",
    ")\n",
    "assert ok_window.all(), \"Kept exited rows outside ±3 months window.\"\n",
    "\n",
    "# 4) Summarize counts\n",
    "two_rows = int((rows_per_deal == 2).sum())\n",
    "one_row  = int((rows_per_deal == 1).sum())\n",
    "print(f\"Check passed. Exited deals with 2 rows: {two_rows}; with 1 row (entry=exit candidate): {one_row}.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "51e0e03ab5591ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- For unexited deals: Entry/ exit date <---> financial data matching -----#\n",
    "#----- Take last available reference date financial data for unexited deals -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "\n",
    "today = pd.Timestamp.today().normalize()\n",
    "\n",
    "is_unexited = df[\"holding_status\"] == \"unexited\"\n",
    "is_exited   = df[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Helper: closest row to target within ±3 calendar months (inclusive)\n",
    "def select_closest_within_window(frame: pd.DataFrame, target_col: str, ref_col: str = \"_ref_dt\"):\n",
    "    \"\"\"Return Series indexed by deal_id with the winning 'id'.\"\"\"\n",
    "    tgt = frame.groupby(\"deal_id\")[target_col].transform(\"first\")\n",
    "    start = tgt - pd.DateOffset(months=3)\n",
    "    end   = tgt + pd.DateOffset(months=3)\n",
    "    in_window = frame[ref_col].between(start, end, inclusive=\"both\")\n",
    "\n",
    "    tmp = frame.loc[in_window, [\"deal_id\", \"id\", ref_col, target_col]].copy()\n",
    "    if tmp.empty:\n",
    "        return pd.Series(dtype=object)\n",
    "\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[ref_col] - tmp[target_col]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[ref_col] >= tmp[target_col]).astype(int)\n",
    "\n",
    "    tmp_sorted = tmp.sort_values(\n",
    "        [\"deal_id\", \"_abs_diff_days\", \"_is_after_or_eq\", ref_col],\n",
    "        ascending=[True, True, False, True],\n",
    "    )\n",
    "    return tmp_sorted.groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    "\n",
    "# Work only on unexited deals\n",
    "df_un = df.loc[is_unexited].copy()\n",
    "\n",
    "# Entry winner within ±3 months (required)\n",
    "entry_winners_un = select_closest_within_window(df_un, target_col=\"_entry_dt\")\n",
    "entry_ok_deals = set(entry_winners_un.index)\n",
    "\n",
    "# Latest ref_date ≤ today (required)\n",
    "ref_le_today = df_un[df_un[\"_ref_dt\"] <= today].copy()\n",
    "latest_ids = (\n",
    "    ref_le_today.sort_values([\"deal_id\", \"_ref_dt\"], ascending=[True, False])\n",
    "                .groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    ")\n",
    "latest_ok_deals = set(latest_ids.index)\n",
    "\n",
    "# Survivors must have both entry match and a latest≤today\n",
    "survivor_deals = entry_ok_deals & latest_ok_deals\n",
    "\n",
    "# Drop deals where entry winner id == latest id\n",
    "coincident = {d for d in survivor_deals if entry_winners_un[d] == latest_ids[d]}\n",
    "survivor_deals -= coincident\n",
    "\n",
    "# Keep exactly the two ids (entry+latest) for survivors; leave exited deals untouched\n",
    "keep_ids_un = set(entry_winners_un.loc[list(survivor_deals)].tolist()) | set(latest_ids.loc[list(survivor_deals)].tolist())\n",
    "\n",
    "keep_mask = is_exited | (is_unexited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_un))\n",
    "\n",
    "before_rows = len(df)\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers and save\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\"] if c in out.columns])\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "print(f\"Unexited: survivors={len(survivor_deals)}, dropped_coincident={len(coincident)}, rows_now={len(out)} (from {before_rows}).\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Unexited: exactly 2 rows per deal_id\n",
    "rows_per_un = check.loc[un_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_un == 2).all(), \"Unexited deals must have exactly 2 rows.\"\n",
    "\n",
    "# Validate the two rows are entry-match and latest≤today\n",
    "ck_un = check.loc[un_mask].copy()\n",
    "ck_un[\"_ref_dt\"] = pd.to_datetime(ck_un[\"reference_date\"], errors=\"coerce\")\n",
    "ck_un[\"_entry_dt\"] = pd.to_datetime(ck_un[\"entry_date\"], errors=\"coerce\")\n",
    "\n",
    "# Recompute entry winners for verification\n",
    "def entry_winner_verify(frame):\n",
    "    from pandas import DateOffset\n",
    "    tgt = frame.groupby(\"deal_id\")[\"_entry_dt\"].transform(\"first\")\n",
    "    start = tgt - DateOffset(months=3)\n",
    "    end   = tgt + DateOffset(months=3)\n",
    "    in_window = frame[\"_ref_dt\"].between(start, end, inclusive=\"both\")\n",
    "    tmp = frame.loc[in_window, [\"deal_id\",\"id\",\"_ref_dt\",\"_entry_dt\"]].copy()\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[\"_ref_dt\"] - tmp[\"_entry_dt\"]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[\"_ref_dt\"] >= tmp[\"_entry_dt\"]).astype(int)\n",
    "    tmp = tmp.sort_values([\"deal_id\",\"_abs_diff_days\",\"_is_after_or_eq\",\"_ref_dt\"],\n",
    "                          ascending=[True, True, False, True])\n",
    "    return tmp.groupby(\"deal_id\")[\"id\"].first()\n",
    "\n",
    "entry_verify = entry_winner_verify(ck_un)\n",
    "latest_verify = (ck_un[ck_un[\"_ref_dt\"] <= pd.Timestamp.today().normalize()]\n",
    "                 .sort_values([\"deal_id\",\"_ref_dt\"], ascending=[True, False])\n",
    "                 .groupby(\"deal_id\")[\"id\"].first())\n",
    "\n",
    "for d, grp in ck_un.groupby(\"deal_id\"):\n",
    "    ids = set(grp[\"id\"])\n",
    "    assert d in entry_verify.index and d in latest_verify.index, f\"Deal {d}: missing entry or latest id.\"\n",
    "    assert entry_verify[d] in ids and latest_verify[d] in ids, f\"Deal {d}: kept rows are not entry+latest.\"\n",
    "\n",
    "# Exited: unchanged cardinality constraint (still ≤ 2)\n",
    "rows_per_ex = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "\n",
    "# Deal-level row-count report for unexited\n",
    "two_rows_un  = int((rows_per_un == 2).sum())\n",
    "one_row_un   = int((rows_per_un == 1).sum())\n",
    "gt2_rows_un  = int((rows_per_un > 2).sum())\n",
    "total_un     = int(rows_per_un.size)\n",
    "\n",
    "print(f\"Unexited row-counts per deal_id — 2 rows: {two_rows_un}, 1 row: {one_row_un}, >2 rows: {gt2_rows_un}, total: {total_un}\")\n",
    "\n",
    "# Keep your hard guarantees\n",
    "assert one_row_un == 0, \"Unexited deals with exactly 1 row found.\"\n",
    "assert gt2_rows_un == 0, \"Unexited deals with >2 rows found.\"\n",
    "assert (rows_per_un == 2).all(), \"Unexited deals must have exactly 2 rows.\"\n",
    "assert (rows_per_ex <= 2).all(), \"Exited deals show >2 rows after unexited processing.\"\n",
    "\n",
    "# Deal-level row-count report for ALL deals (exited + unexited)\n",
    "rows_per_all = check.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "two_rows_all = int((rows_per_all == 2).sum())\n",
    "one_row_all  = int((rows_per_all == 1).sum())\n",
    "gt2_rows_all = int((rows_per_all > 2).sum())\n",
    "total_all    = int(rows_per_all.size)\n",
    "\n",
    "print(f\"All deals — 2 rows: {two_rows_all}, 1 row: {one_row_all}, >2 rows: {gt2_rows_all}, total: {total_all}\")\n",
    "\n",
    "# Hard guarantees across the whole dataset\n",
    "assert one_row_all == 0, \"Found deals with exactly 1 row.\"\n",
    "assert gt2_rows_all == 0, \"Found deals with >2 rows.\"\n",
    "assert (rows_per_all == 2).all(), \"All deals must have exactly 2 rows.\"\n",
    "\n",
    "print(\"Unexited selection check passed.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "id": "5938b96915ce38b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Add derived columns (row-wise) -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Numeric views (do not mutate originals)\n",
    "def num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "ev = num(df[\"enterprise_value\"])\n",
    "eb = num(df[\"ebitda\"])\n",
    "nd = num(df[\"net_debt\"])\n",
    "eq = num(df[\"equity\"])\n",
    "rv = num(df[\"revenue\"])\n",
    "\n",
    "# 1) ebitda_margin = ebitda / revenue\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    ebitda_margin = (eb / rv).where(rv != 0)\n",
    "\n",
    "# 2) xebitda = enterprise_value / ebitda\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    xebitda = (ev / eb).where(eb != 0)\n",
    "\n",
    "# 3) de_ratio = net_debt / equity\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    de_ratio = (nd / eq).where(eq != 0)\n",
    "\n",
    "# 4) dividends = EV * 0.1\n",
    "dividends = ev * 0.10\n",
    "\n",
    "# 5) capital_injections = EV * 0.1\n",
    "capital_injections = ev * 0.10\n",
    "\n",
    "# 6) interest_rate = 0.1 (constant)\n",
    "interest_rate = pd.Series(0.10, index=df.index)\n",
    "\n",
    "# 7) holding_period (years)\n",
    "today = pd.Timestamp.today().normalize()\n",
    "ent = pd.to_datetime(df.get(\"entry_date\"), errors=\"coerce\")\n",
    "exi = pd.to_datetime(df.get(\"exit_date\"),  errors=\"coerce\")\n",
    "is_exited = (df[\"holding_status\"] == \"exited\")\n",
    "\n",
    "days = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "days.loc[is_exited] = (exi - ent).dt.days.loc[is_exited]\n",
    "days.loc[~is_exited] = (today - ent).dt.days.loc[~is_exited]\n",
    "days = days.where(days >= 0)  # negative or missing -> NA\n",
    "holding_period = days / 365.25  # float years\n",
    "\n",
    "# Assign new columns\n",
    "df[\"ebitda_margin\"]      = ebitda_margin\n",
    "df[\"xebitda\"]            = xebitda\n",
    "df[\"de_ratio\"]           = de_ratio\n",
    "df[\"dividends\"]          = dividends\n",
    "df[\"capital_injections\"] = capital_injections\n",
    "df[\"interest_rate\"]      = interest_rate\n",
    "df[\"holding_period\"]     = holding_period\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# Minimal verification (disk-based)\n",
    "g = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "new_cols = [\"ebitda_margin\",\"xebitda\",\"de_ratio\",\"dividends\",\"capital_injections\",\"interest_rate\",\"holding_period\"]\n",
    "nn = {c: int(g[c].notna().sum()) for c in new_cols}\n",
    "print(\"Added columns:\", \", \".join(new_cols))\n",
    "print(\"Non-null counts:\", nn)\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "id": "1e4bc010b8845a6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Drop deals that have unreasonable financial metrics -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "before_deals = df[\"deal_id\"].nunique()\n",
    "\n",
    "# Numeric views\n",
    "em  = pd.to_numeric(df[\"ebitda_margin\"], errors=\"coerce\")\n",
    "xe  = pd.to_numeric(df[\"xebitda\"],        errors=\"coerce\")\n",
    "der = pd.to_numeric(df[\"de_ratio\"],       errors=\"coerce\")\n",
    "\n",
    "# Violations (row-wise)\n",
    "v_em  = em > 1\n",
    "v_xe  = xe.isna() | (xe < -1000) | (xe > 1000)\n",
    "v_der = der.isna() | (der < -1)   | (der > 20)\n",
    "\n",
    "# Aggregate to deal-level: drop if any row violates\n",
    "bad_em_deals  = set(df.loc[v_em,  \"deal_id\"].dropna().unique())\n",
    "bad_xe_deals  = set(df.loc[v_xe,  \"deal_id\"].dropna().unique())\n",
    "bad_der_deals = set(df.loc[v_der, \"deal_id\"].dropna().unique())\n",
    "bad_deals = bad_em_deals | bad_xe_deals | bad_der_deals\n",
    "\n",
    "# Drop entire deals\n",
    "keep_mask = ~df[\"deal_id\"].isin(bad_deals)\n",
    "out = df.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "# Persist\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# Reporting\n",
    "after_rows  = len(out)\n",
    "after_deals = out[\"deal_id\"].nunique()\n",
    "print(\n",
    "    f\"Dropped {before_deals - after_deals} deal_id(s). \"\n",
    "    f\"[ebitda_margin>1: {len(bad_em_deals)}, xebitda null/|>|1000: {len(bad_xe_deals)}, \"\n",
    "    f\"de_ratio null/<-1/>20: {len(bad_der_deals)}]  Rows: {after_rows} (from {before_rows}).\"\n",
    ")\n",
    "\n",
    "# Checks (reload from disk)\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "em2  = pd.to_numeric(check[\"ebitda_margin\"], errors=\"coerce\")\n",
    "xe2  = pd.to_numeric(check[\"xebitda\"],        errors=\"coerce\")\n",
    "der2 = pd.to_numeric(check[\"de_ratio\"],       errors=\"coerce\")\n",
    "\n",
    "assert not (em2 > 1).any(),                      \"Remaining rows with ebitda_margin > 1.\"\n",
    "assert not (xe2.isna() | (xe2 < -1000) | (xe2 > 1000)).any(), \"Remaining rows with invalid xebitda.\"\n",
    "assert not (der2.isna() | (der2 < -1) | (der2 > 20)).any(),   \"Remaining rows with invalid de_ratio.\"\n",
    "\n",
    "rows_per_deal = check.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_deal == 2).all(), \"Each remaining deal must have exactly 2 rows.\"\n",
    "\n",
    "# Disk-only status split (mirrors your unique_deals print)\n",
    "g = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "id": "3c2cd0ac8649afee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#----- Just a QA script that checks various things -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# --- Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "df[\"_exit_dt\"]  = pd.to_datetime(df[\"exit_date\"],     errors=\"coerce\")\n",
    "today = pd.Timestamp.today().normalize()\n",
    "\n",
    "# --- Numeric views\n",
    "ev = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(df[\"net_debt\"],         errors=\"coerce\")\n",
    "eq = pd.to_numeric(df[\"equity\"],           errors=\"coerce\")\n",
    "rev= pd.to_numeric(df[\"revenue\"],          errors=\"coerce\")\n",
    "eb = pd.to_numeric(df[\"ebitda\"],           errors=\"coerce\")\n",
    "\n",
    "# --- Bookkeeping\n",
    "issues = {}\n",
    "\n",
    "def flag(name, bad_index):\n",
    "    bad_ids = df.loc[bad_index, \"deal_id\"].unique().tolist()\n",
    "    issues[name] = bad_ids\n",
    "\n",
    "# 1) Exactly two rows per deal_id\n",
    "rows_per_deal = df.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "bad = rows_per_deal.index[rows_per_deal != 2]\n",
    "flag(\"not_exactly_two_rows\", df[\"deal_id\"].isin(bad))\n",
    "\n",
    "# 2) Required fields present and finite; revenue != 0\n",
    "req_na = ev.isna() | nd.isna() | eq.isna() | rev.isna() | eb.isna()\n",
    "flag(\"missing_required_fields\", req_na)\n",
    "flag(\"revenue_zero\", (rev == 0))\n",
    "\n",
    "# 3) Date-window checks\n",
    "is_exited   = df[\"holding_status\"] == \"exited\"\n",
    "is_unexited = df[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# Windows (inclusive ±3 calendar months)\n",
    "from pandas import DateOffset\n",
    "entry_start = df[\"_entry_dt\"] - DateOffset(months=3)\n",
    "entry_end   = df[\"_entry_dt\"] + DateOffset(months=3)\n",
    "exit_start  = df[\"_exit_dt\"]  - DateOffset(months=3)\n",
    "exit_end    = df[\"_exit_dt\"]  + DateOffset(months=3)\n",
    "\n",
    "in_entry_win = df[\"_ref_dt\"].between(entry_start, entry_end, inclusive=\"both\")\n",
    "in_exit_win  = df[\"_ref_dt\"].between(exit_start,  exit_end,  inclusive=\"both\")\n",
    "le_today     = df[\"_ref_dt\"] <= today\n",
    "\n",
    "# 3a) Exited: exactly one entry-window row AND exactly one exit-window row per deal\n",
    "ex = df[is_exited].copy()\n",
    "ex_grp = ex.groupby(\"deal_id\", as_index=False)\n",
    "ex_count_entry = ex_grp[\"id\"].apply(lambda s: in_entry_win.loc[s.index].sum()).set_index(\"deal_id\")[\"id\"]\n",
    "ex_count_exit  = ex_grp[\"id\"].apply(lambda s: in_exit_win.loc[s.index].sum()).set_index(\"deal_id\")[\"id\"]\n",
    "bad_ex_entry = ex_count_entry.index[ex_count_entry != 1]\n",
    "bad_ex_exit  = ex_count_exit.index[ex_count_exit != 1]\n",
    "flag(\"exited_entry_window_count_!=1\", df[\"deal_id\"].isin(bad_ex_entry) & is_exited)\n",
    "flag(\"exited_exit_window_count_!=1\",  df[\"deal_id\"].isin(bad_ex_exit)  & is_exited)\n",
    "\n",
    "# 3b) Unexited: exactly one entry-window row; exactly one latest<=today row; they must be different\n",
    "un = df[is_unexited].copy()\n",
    "un_grp = un.groupby(\"deal_id\", as_index=False)\n",
    "\n",
    "un_count_entry = un_grp[\"id\"].apply(lambda s: in_entry_win.loc[s.index].sum()).set_index(\"deal_id\")[\"id\"]\n",
    "bad_un_entry_count = un_count_entry.index[un_count_entry != 1]\n",
    "flag(\"unexited_entry_window_count_!=1\", df[\"deal_id\"].isin(bad_un_entry_count) & is_unexited)\n",
    "\n",
    "# latest<=today id per unexited deal\n",
    "un_le_today = un[le_today.loc[un.index]].copy()\n",
    "latest_id = (un_le_today.sort_values([\"deal_id\",\"_ref_dt\"], ascending=[True, False])\n",
    "                       .groupby(\"deal_id\")[\"id\"].first())\n",
    "\n",
    "# deals lacking any ref_date <= today\n",
    "un_deals = un[\"deal_id\"].unique()\n",
    "missing_latest = [d for d in un_deals if d not in latest_id.index]\n",
    "flag(\"unexited_missing_latest_le_today\", df[\"deal_id\"].isin(missing_latest) & is_unexited)\n",
    "\n",
    "# entry-winner id per unexited deal (pick closest within window as earlier)\n",
    "def entry_winner_ids(frame):\n",
    "    tmp = frame.copy()\n",
    "    tmp = tmp[in_entry_win.loc[tmp.index]]\n",
    "    if tmp.empty:\n",
    "        return pd.Series(dtype=object)\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[\"_ref_dt\"] - tmp[\"_entry_dt\"]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[\"_ref_dt\"] >= tmp[\"_entry_dt\"]).astype(int)\n",
    "    tmp = tmp.sort_values([\"deal_id\",\"_abs_diff_days\",\"_is_after_or_eq\",\"_ref_dt\"],\n",
    "                          ascending=[True, True, False, True])\n",
    "    return tmp.groupby(\"deal_id\")[\"id\"].first()\n",
    "\n",
    "entry_id_un = entry_winner_ids(un)\n",
    "\n",
    "# Check distinctness and membership\n",
    "bad_un_distinct = []\n",
    "bad_un_membership = []\n",
    "for d, sub in un.groupby(\"deal_id\"):\n",
    "    if d in bad_un_entry_count or d in missing_latest:\n",
    "        continue  # already flagged above\n",
    "    ids = set(sub[\"id\"])\n",
    "    e_id = entry_id_un.get(d, None)\n",
    "    l_id = latest_id.get(d, None)\n",
    "    if e_id is None or l_id is None:\n",
    "        continue\n",
    "    if e_id == l_id:\n",
    "        bad_un_distinct.append(d)\n",
    "    if e_id not in ids or l_id not in ids:\n",
    "        bad_un_membership.append(d)\n",
    "\n",
    "flag(\"unexited_entry_equals_latest\", df[\"deal_id\"].isin(bad_un_distinct) & is_unexited)\n",
    "flag(\"unexited_missing_entry_or_latest_row\", df[\"deal_id\"].isin(bad_un_membership) & is_unexited)\n",
    "\n",
    "# 4) EV–EqV–ND bridge with 3%*|EV| tolerance (independent identities)\n",
    "abs_floor = 1e-9\n",
    "tol = np.maximum(0.03 * ev.abs(), abs_floor)\n",
    "bad_ev = (ev - (eq + nd)).abs() > tol\n",
    "bad_nd = (nd - (ev - eq)).abs() > tol\n",
    "bad_eq = (eq - (ev - nd)).abs() > tol\n",
    "flag(\"ev_bridge_ev_vs_eq_plus_nd\", bad_ev)\n",
    "flag(\"ev_bridge_nd_vs_ev_minus_eq\", bad_nd)\n",
    "flag(\"ev_bridge_eq_vs_ev_minus_nd\", bad_eq)\n",
    "\n",
    "# 5) Basic key integrity\n",
    "flag(\"id_not_unique\", df[\"id\"].duplicated(keep=False))\n",
    "flag(\"deal_id_null\", df[\"deal_id\"].isna())\n",
    "\n",
    "# --- Summary\n",
    "total_deals = df[\"deal_id\"].nunique()\n",
    "failed = {k: len(v) for k, v in issues.items() if v}\n",
    "print(f\"Deals in sample: {total_deals}\")\n",
    "if failed:\n",
    "    for k, n in failed.items():\n",
    "        print(f\"- {k}: {n} deal(s)\")\n",
    "    # Optional: assert no failures\n",
    "    # assert False, \"QA failed; see counts above.\"\n",
    "else:\n",
    "    print(\"QA passed: all checks satisfied.\")\n"
   ],
   "id": "a40b85b64646dc05",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
