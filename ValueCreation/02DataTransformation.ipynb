{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T20:17:38.269481Z",
     "start_time": "2025-10-15T20:17:37.902284Z"
    }
   },
   "source": [
    "# === Holding_status, date/revenue filters, EV bridge compute/flag/filter ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ===================== 0) Optional: per-deal minimum row guard (default OFF) =====================\n",
    "ENFORCE_MIN_ROWS_PER_DEAL = False   # optional safeguard; default OFF → no effect on output\n",
    "MIN_ROWS_PER_DEAL = 2\n",
    "\n",
    "def maybe_enforce_min_rows(frame: pd.DataFrame, label: str) -> pd.DataFrame:\n",
    "    if not ENFORCE_MIN_ROWS_PER_DEAL:\n",
    "        return frame\n",
    "    counts = frame.groupby(\"deal_id\")[\"id\"].count()\n",
    "    keep_deals = counts[counts >= MIN_ROWS_PER_DEAL].index\n",
    "    before = frame[\"deal_id\"].nunique()\n",
    "    out = frame[frame[\"deal_id\"].isin(keep_deals)].copy()\n",
    "    print(f\"{label}: Min-row guard kept {len(keep_deals)} deals (≥{MIN_ROWS_PER_DEAL} rows); \"\n",
    "          f\"dropped {before - len(keep_deals)}.\")\n",
    "    return out\n",
    "\n",
    "# ===================== 1) Add holding_status (+ optional exited-only filter) =====================\n",
    "HOLDING_FILTER_MODE = \"both\"   # options: \"both\" (default) | \"exited_only\"\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "df[\"_ord\"] = np.arange(len(df))  # preserve original row order\n",
    "\n",
    "if \"exit_date\" not in df.columns:\n",
    "    raise KeyError(\"Column 'exit_date' not found in working.csv. Run the earlier ADD_COLUMNS step from 'deal' first.\")\n",
    "\n",
    "# holding status (SAFER: parse to datetime and test .notna())\n",
    "norm_exit = pd.to_datetime(df[\"exit_date\"], errors=\"coerce\")\n",
    "is_exited = norm_exit.notna()\n",
    "df[\"holding_status\"] = is_exited.map({True: \"exited\", False: \"unexited\"})\n",
    "\n",
    "# enforce deal-level consistency before any filtering\n",
    "status_per_deal = df.groupby(\"deal_id\")[\"holding_status\"].nunique(dropna=False)\n",
    "mixed = status_per_deal[status_per_deal > 1]\n",
    "assert mixed.empty, f\"{len(mixed)} deal_id(s) have mixed exited/unexited rows.\"\n",
    "\n",
    "# optional: exited-only filter (deal-wide)\n",
    "if HOLDING_FILTER_MODE == \"exited_only\":\n",
    "    deal_first_status = df.groupby(\"deal_id\")[\"holding_status\"].first()\n",
    "    keep_deals = set(deal_first_status[deal_first_status == \"exited\"].index)\n",
    "    before_rows, before_deals = len(df), df[\"deal_id\"].nunique()\n",
    "    df = df[df[\"deal_id\"].isin(keep_deals)].copy()\n",
    "    df = df.sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "    print(f\"Mode=exited_only: kept {len(keep_deals)} exited deal(s); \"\n",
    "          f\"dropped {before_deals - len(keep_deals)} unexited. Rows now {len(df)} (from {before_rows}).\")\n",
    "else:\n",
    "    df = df.sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "    print(\"Mode=both: no deal-level filtering applied.\")\n",
    "\n",
    "# persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# post-write checks\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "exited_count = (check[\"holding_status\"] == \"exited\").sum()\n",
    "unexited_count = (check[\"holding_status\"] == \"unexited\").sum()\n",
    "total_after = len(check)\n",
    "assert exited_count + unexited_count == total_after, \"Status coverage failed.\"\n",
    "\n",
    "status_per_deal = check.groupby(\"deal_id\")[\"holding_status\"].nunique(dropna=False)\n",
    "assert (status_per_deal <= 1).all(), \"Found deal(s) with mixed statuses after filtering.\"\n",
    "\n",
    "if HOLDING_FILTER_MODE == \"exited_only\":\n",
    "    deal_status = check.groupby(\"deal_id\")[\"holding_status\"].first()\n",
    "    assert (deal_status == \"exited\").all(), \"Non-exited deal(s) remain in exited_only mode.\"\n",
    "\n",
    "# report unique deals by status\n",
    "by_status = (check.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "                   .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    check[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ===================== 2) Filter unreasonable dates =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "ref = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "lower = pd.Timestamp(1976, 1, 1).normalize()\n",
    "q_end = pd.Timestamp.today().to_period(\"Q-DEC\").end_time.normalize()\n",
    "date_ok = ref.notna() & (ref >= lower) & (ref <= q_end)\n",
    "\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[date_ok].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "after = maybe_enforce_min_rows(after, label=\"FILTER(dates)\")  # optional; no-op by default\n",
    "dropped = before_rows - len(after)\n",
    "print(\n",
    "    f\"Dropped {dropped} rows due to missing/out-of-range reference_date. \"\n",
    "    f\"Kept range [{lower.date()} .. {q_end.date()}].\"\n",
    ")\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "ref2 = pd.to_datetime(check[\"reference_date\"], errors=\"coerce\")\n",
    "assert ref2.notna().all(), \"Found rows with null reference_date.\"\n",
    "assert ((ref2 >= lower) & (ref2 <= q_end)).all(), \"Found dates outside bounds.\"\n",
    "print(f\"FILTER (dates) check passed. Remaining rows: {len(check)}\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ===================== 3) Filter for revenue>0, EBITDA (toggle), and <2 of [EV/ND/Eq] =============\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# ---- Toggle: Set to True to require strictly positive EBITDA; False to only require non-missing EBITDA\n",
    "REQUIRE_POSITIVE_EBITDA = True\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "rev_num    = num(df[\"revenue\"])\n",
    "ebitda_num = num(df[\"ebitda\"])\n",
    "ev_num     = num(df[\"enterprise_value\"])\n",
    "nd_num     = num(df[\"net_debt\"])\n",
    "eq_num     = num(df[\"equity\"])\n",
    "\n",
    "rev_ok    = rev_num > 0\n",
    "if REQUIRE_POSITIVE_EBITDA:\n",
    "    ebitda_ok = ebitda_num > 0\n",
    "else:\n",
    "    ebitda_ok = ebitda_num.notna()\n",
    "\n",
    "trio_non_null = ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)\n",
    "trio_ok       = trio_non_null >= 2\n",
    "\n",
    "keep_mask = rev_ok & ebitda_ok & trio_ok\n",
    "\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "after = maybe_enforce_min_rows(after, label=\"FILTER(revenue/EBITDA/trio)\")  # optional; no-op by default\n",
    "dropped = before_rows - len(after)\n",
    "mode_str = \"EBITDA>0\" if REQUIRE_POSITIVE_EBITDA else \"non-missing EBITDA\"\n",
    "print(f\"Filtering out {dropped} rows (kept {len(after)} of {before_rows}) using revenue>0, {mode_str}, and ≥2 of [EV, ND, Eq].\")\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "rev_num    = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "ebitda_num = pd.to_numeric(check[\"ebitda\"], errors=\"coerce\")\n",
    "ev_num     = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd_num     = pd.to_numeric(check[\"net_debt\"], errors=\"coerce\")\n",
    "eq_num     = pd.to_numeric(check[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "# Assertions aligned with the new rules\n",
    "assert (rev_num > 0).all(), \"Found rows with revenue <= 0 or non-numeric.\"\n",
    "if REQUIRE_POSITIVE_EBITDA:\n",
    "    assert (ebitda_num > 0).all(), \"Found rows with EBITDA <= 0 or non-numeric.\"\n",
    "else:\n",
    "    assert ebitda_num.notna().all(), \"Found rows with empty EBITDA after filtering.\"\n",
    "assert ((ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)) >= 2).all(), \\\n",
    "       \"Found rows with fewer than two of [enterprise_value, net_debt, equity] present.\"\n",
    "print(\"FILTER (revenue/EBITDA/trio) check passed. Shape:\", check.shape)\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "# ==== 4) EV / ND / Equity bridge: compute missing, flag, and filter (consolidated) ====\n",
    "\n",
    "ENFORCE_POSITIVE_EV_AND_EQ = True  # enforces EV>0, Eq>0, ND>0 when True\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# -- load & preserve order\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "before_rows = len(df); before_deals = df[\"deal_id\"].nunique()\n",
    "\n",
    "# -- numeric views\n",
    "ev0 = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd0 = pd.to_numeric(df[\"net_debt\"],         errors=\"coerce\")\n",
    "eq0 = pd.to_numeric(df[\"equity\"],           errors=\"coerce\")\n",
    "\n",
    "# -- (1) compute missing member when exactly two present\n",
    "present_cnt = ev0.notna().astype(int) + nd0.notna().astype(int) + eq0.notna().astype(int)\n",
    "calc_mask = (present_cnt == 2)\n",
    "df[\"EV_bridge_calc\"] = np.where(calc_mask, \"Yes\", \"No\")\n",
    "\n",
    "need_ev = calc_mask & ev0.isna()\n",
    "need_nd = calc_mask & nd0.isna()\n",
    "need_eq = calc_mask & eq0.isna()\n",
    "if need_ev.any(): df.loc[need_ev, \"enterprise_value\"] = (eq0 + nd0)[need_ev]\n",
    "if need_nd.any(): df.loc[need_nd, \"net_debt\"]         = (ev0 - eq0)[need_nd]\n",
    "if need_eq.any(): df.loc[need_eq, \"equity\"]           = (ev0 - nd0)[need_eq]\n",
    "\n",
    "# -- (2) flag residual vs. EV with tolerance (hardcoded, inclusive)\n",
    "ev = pd.to_numeric(df[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd = pd.to_numeric(df[\"net_debt\"],         errors=\"coerce\")\n",
    "eq = pd.to_numeric(df[\"equity\"],           errors=\"coerce\")\n",
    "\n",
    "all3 = ev.notna() & nd.notna() & eq.notna()\n",
    "residual = ev - (eq + nd)\n",
    "tol = 1001.0  # units; inclusive keeps residual==1000\n",
    "ok = all3 & (residual.abs() <= tol)\n",
    "\n",
    "df[\"EV_bridge_error\"] = np.where(ok, \"Ok\", \"Error\")\n",
    "df[\"EV_bridge_residual\"] = residual\n",
    "\n",
    "# -- (3) mandatory filter: remove faulty bridges (row-level)\n",
    "before_faulty_rows = len(df)\n",
    "df_ok = df.loc[df[\"EV_bridge_error\"] != \"Error\"].copy()\n",
    "dropped_faulty = before_faulty_rows - len(df_ok)\n",
    "\n",
    "# -- (4) optional deal-wide positivity filter on EV, Equity, and Net Debt (AFTER fill & error drop)\n",
    "if ENFORCE_POSITIVE_EV_AND_EQ:\n",
    "    ev_pos = pd.to_numeric(df_ok[\"enterprise_value\"], errors=\"coerce\") > 0\n",
    "    eq_pos = pd.to_numeric(df_ok[\"equity\"],           errors=\"coerce\") > 0\n",
    "    nd_pos = pd.to_numeric(df_ok[\"net_debt\"],         errors=\"coerce\") > 0\n",
    "    ok_row = ev_pos & eq_pos & nd_pos\n",
    "\n",
    "    ok_deal = ok_row.groupby(df_ok[\"deal_id\"]).all()\n",
    "    keep_deals = set(ok_deal[ok_deal].index)\n",
    "\n",
    "    before_pos_deals = df_ok[\"deal_id\"].nunique()\n",
    "    df_ok = df_ok[df_ok[\"deal_id\"].isin(keep_deals)].copy()\n",
    "    dropped_pos_deals = before_pos_deals - len(keep_deals)\n",
    "else:\n",
    "    dropped_pos_deals = 0\n",
    "\n",
    "# -- optional: enforce minimum rows per deal (default OFF)\n",
    "df_ok = maybe_enforce_min_rows(df_ok, label=\"EV-bridge/positivity\")\n",
    "\n",
    "# -- persist in original order\n",
    "df_ok = df_ok.sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "df_ok.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# -- checks\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "ev_c = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd_c = pd.to_numeric(check[\"net_debt\"],         errors=\"coerce\")\n",
    "eq_c = pd.to_numeric(check[\"equity\"],           errors=\"coerce\")\n",
    "\n",
    "present_cnt_after = ev_c.notna().astype(int) + nd_c.notna().astype(int) + eq_c.notna().astype(int)\n",
    "\n",
    "yes_mask = (check.get(\"EV_bridge_calc\", pd.Series(index=check.index, data=\"No\")) == \"Yes\")\n",
    "assert (present_cnt_after[yes_mask] == 3).all(), \"Some 'Yes' rows still missing EV/ND/Eq.\"\n",
    "assert (present_cnt_after >= 2).all(), \"Found rows with fewer than two of [EV, ND, Eq].\"\n",
    "assert (check[\"EV_bridge_error\"] != \"Error\").all(), \"Faulty bridges remain after filter.\"\n",
    "\n",
    "if ENFORCE_POSITIVE_EV_AND_EQ:\n",
    "    assert (ev_c > 0).all() and (eq_c > 0).all() and (nd_c > 0).all(), \"Non-positive EV/Eq/ND survived positivity filter.\"\n",
    "\n",
    "# -- reporting\n",
    "print(f\"Computed missing EV/ND/Eq where exactly two present. calc_flag rows: {int(calc_mask.sum())}\")\n",
    "print(f\"Dropped {dropped_faulty} rows with EV_bridge_error == 'Error'.\")\n",
    "if ENFORCE_POSITIVE_EV_AND_EQ:\n",
    "    print(f\"Dropped {dropped_pos_deals} deal(s) due to EV<=0 or Equity<=0 or NetDebt<=0 in any row.\")\n",
    "print(f\"Rows now: {len(check)} (from {before_rows}); deals now: {check['deal_id'].nunique()} (from {before_deals}).\")\n",
    "\n",
    "by_status = (check.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "                   .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    check[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=both: no deal-level filtering applied.\n",
      "unique_deals_exited: 688 unique_deals_unexited: 837 unique_deals_total: 1525\n",
      "Dropped 28 rows due to missing/out-of-range reference_date. Kept range [1976-01-01 .. 2025-12-31].\n",
      "FILTER (dates) check passed. Remaining rows: 6293\n",
      "unique_deals_exited: 688 unique_deals_unexited: 837 unique_deals_total: 1525\n",
      "Filtering out 3933 rows (kept 2360 of 6293) using revenue>0, EBITDA>0, and ≥2 of [EV, ND, Eq].\n",
      "FILTER (revenue/EBITDA/trio) check passed. Shape: (2360, 27)\n",
      "unique_deals_exited: 498 unique_deals_unexited: 505 unique_deals_total: 1003\n",
      "Computed missing EV/ND/Eq where exactly two present. calc_flag rows: 921\n",
      "Dropped 574 rows with EV_bridge_error == 'Error'.\n",
      "Dropped 147 deal(s) due to EV<=0 or Equity<=0 or NetDebt<=0 in any row.\n",
      "Rows now: 1382 (from 2360); deals now: 615 (from 1003).\n",
      "unique_deals_exited: 294 unique_deals_unexited: 321 unique_deals_total: 615\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:17:38.406014Z",
     "start_time": "2025-10-15T20:17:38.275436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Date-to-financial matching for exited and unexited deals and currency integrity ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper(s) ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "def select_closest_within_window(frame: pd.DataFrame, target_col: str, ref_col: str = \"_ref_dt\"):\n",
    "    tgt = frame.groupby(\"deal_id\")[target_col].transform(\"first\")\n",
    "    start = tgt - pd.DateOffset(months=3)\n",
    "    end   = tgt + pd.DateOffset(months=3)\n",
    "\n",
    "    in_window = frame[ref_col].ge(start) & frame[ref_col].le(end)\n",
    "    tmp = frame.loc[in_window, [\"deal_id\", \"id\", ref_col, target_col]].copy()\n",
    "    if tmp.empty:\n",
    "        return pd.Series(dtype=object)\n",
    "\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[ref_col] - tmp[target_col]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[ref_col] >= tmp[target_col]).astype(int)\n",
    "\n",
    "    tmp_sorted = tmp.sort_values(\n",
    "        [\"deal_id\", \"_abs_diff_days\", \"_is_after_or_eq\", ref_col],\n",
    "        ascending=[True, True, False, True],\n",
    "    )\n",
    "    winners = tmp_sorted.groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    "    return winners  # index: deal_id, values: id\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ===================== 1) Exited deals: keep entry+exit matched rows within ±3 months =====================\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "df[\"_exit_dt\"]  = pd.to_datetime(df[\"exit_date\"],     errors=\"coerce\")\n",
    "\n",
    "# Masks\n",
    "exited    = df[\"holding_status\"] == \"exited\"\n",
    "unexited  = df[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# Only operate on exited deals\n",
    "df_ex = df.loc[exited].copy()\n",
    "\n",
    "# Entry winner (requires entry_date)\n",
    "entry_winners = select_closest_within_window(df_ex, target_col=\"_entry_dt\")\n",
    "\n",
    "# Exit winner (requires exit_date)\n",
    "exit_winners = select_closest_within_window(df_ex, target_col=\"_exit_dt\")\n",
    "\n",
    "# Deals must have both winners to survive\n",
    "entry_ok_deals = set(entry_winners.index)\n",
    "exit_ok_deals  = set(exit_winners.index)\n",
    "survivor_deals = entry_ok_deals & exit_ok_deals\n",
    "\n",
    "# Drop deals where entry winner == exit winner (entry=exit)\n",
    "coincident_deals = {d for d in survivor_deals if entry_winners[d] == exit_winners[d]}\n",
    "if coincident_deals:\n",
    "    print(f\"Removing {len(coincident_deals)} exited deal(s) where entry and exit map to the same id.\")\n",
    "survivor_deals = survivor_deals - coincident_deals\n",
    "\n",
    "# Keep set for exited deals: union of entry+exit winners\n",
    "keep_ids_exited = set(entry_winners.loc[list(survivor_deals)].tolist()) | set(\n",
    "    exit_winners.loc[list(survivor_deals)].tolist()\n",
    ")\n",
    "\n",
    "# Final keep mask:\n",
    "# - keep all rows for unexited deals (untouched)\n",
    "# - for exited deals: keep only winner ids; drop entire deal if not in survivor_deals\n",
    "keep_mask = unexited | (exited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_exited))\n",
    "\n",
    "before_rows = len(df)\n",
    "before_deals_ex = df.loc[exited, \"deal_id\"].nunique()\n",
    "\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\",\"_exit_dt\"] if c in out.columns])\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# Reporting\n",
    "after_rows = len(out)\n",
    "after_deals_ex = out.loc[out[\"holding_status\"]==\"exited\", \"deal_id\"].nunique()\n",
    "dropped_exited_deals = before_deals_ex - after_deals_ex\n",
    "print(\n",
    "    f\"Exited deals kept: {after_deals_ex} (dropped {dropped_exited_deals} with no entry/exit match in ±3 months). \"\n",
    "    f\"Rows now: {after_rows} (from {before_rows}).\"\n",
    ")\n",
    "\n",
    "# --- Validation for exited ---\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "\n",
    "# 1) Unexited deals untouched in cardinality of rows per deal (relative order not asserted here)\n",
    "\n",
    "rows_per_deal = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_deal == 2).all(), \"Found exited deals with != 2 kept rows.\"\n",
    "\n",
    "# 3) Verify kept rows are within ±3 months of the respective target dates\n",
    "ck = check.loc[ex_mask].copy()\n",
    "ref = pd.to_datetime(ck[\"reference_date\"], errors=\"coerce\")\n",
    "ent = pd.to_datetime(ck[\"entry_date\"], errors=\"coerce\")\n",
    "exi = pd.to_datetime(ck[\"exit_date\"],  errors=\"coerce\")\n",
    "\n",
    "# Tag each row as 'entry_candidate' or 'exit_candidate' by closeness\n",
    "abs_diff_entry = (ref - ent).abs()\n",
    "abs_diff_exit  = (ref - exi).abs()\n",
    "is_entry_like = abs_diff_entry <= abs_diff_exit\n",
    "\n",
    "from pandas import DateOffset\n",
    "ok_window = (\n",
    "    (is_entry_like &\n",
    "     ck[\"reference_date\"].pipe(pd.to_datetime, errors=\"coerce\").ge(ent - DateOffset(months=3)) &\n",
    "     ck[\"reference_date\"].pipe(pd.to_datetime, errors=\"coerce\").le(ent + DateOffset(months=3)))\n",
    "    |\n",
    "    (~is_entry_like &\n",
    "     ck[\"reference_date\"].pipe(pd.to_datetime, errors=\"coerce\").ge(exi - DateOffset(months=3)) &\n",
    "     ck[\"reference_date\"].pipe(pd.to_datetime, errors=\"coerce\").le(exi + DateOffset(months=3)))\n",
    ")\n",
    "\n",
    "assert ok_window.all(), \"Kept exited rows outside ±3 months window.\"\n",
    "\n",
    "# 4) Summarize counts\n",
    "two_rows = int((rows_per_deal == 2).sum())\n",
    "one_row  = int((rows_per_deal == 1).sum())\n",
    "print(f\"Check passed. Exited deals with 2 rows: {two_rows}; with 1 row (entry=exit candidate): {one_row}.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n",
    "\n",
    "# ===================== 2) Unexited deals: keep entry match (±3 months) + latest ref_date ≤ today =====================\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Parse dates\n",
    "df[\"_ref_dt\"]   = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_entry_dt\"] = pd.to_datetime(df[\"entry_date\"],    errors=\"coerce\")\n",
    "\n",
    "today = pd.Timestamp.today().normalize()\n",
    "\n",
    "is_unexited = df[\"holding_status\"] == \"unexited\"\n",
    "is_exited   = df[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Work only on unexited deals\n",
    "df_un = df.loc[is_unexited].copy()\n",
    "\n",
    "# Entry winner within ±3 months (required)\n",
    "entry_winners_un = select_closest_within_window(df_un, target_col=\"_entry_dt\")\n",
    "entry_ok_deals = set(entry_winners_un.index)\n",
    "\n",
    "# Latest ref_date ≤ today (required)\n",
    "ref_le_today = df_un[df_un[\"_ref_dt\"] <= today].copy()\n",
    "latest_ids = (\n",
    "    ref_le_today.sort_values([\"deal_id\", \"_ref_dt\"], ascending=[True, False])\n",
    "                .groupby(\"deal_id\", sort=False)[\"id\"].first()\n",
    ")\n",
    "latest_ok_deals = set(latest_ids.index)\n",
    "\n",
    "# Survivors must have both entry match and a latest≤today\n",
    "survivor_deals = entry_ok_deals & latest_ok_deals\n",
    "\n",
    "# Drop deals where entry winner id == latest id\n",
    "coincident = {d for d in survivor_deals if entry_winners_un[d] == latest_ids[d]}\n",
    "survivor_deals -= coincident\n",
    "\n",
    "# Keep exactly the two ids (entry+latest) for survivors; leave exited deals untouched\n",
    "keep_ids_un = set(entry_winners_un.loc[list(survivor_deals)].tolist()) | set(latest_ids.loc[list(survivor_deals)].tolist())\n",
    "\n",
    "keep_mask = is_exited | (is_unexited & df[\"deal_id\"].isin(survivor_deals) & df[\"id\"].isin(keep_ids_un))\n",
    "\n",
    "before_rows = len(df)\n",
    "out = df.loc[keep_mask].copy()\n",
    "\n",
    "# Drop helpers and save\n",
    "out = out.drop(columns=[c for c in [\"_ref_dt\",\"_entry_dt\"] if c in out.columns])\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "print(f\"Unexited: survivors={len(survivor_deals)}, dropped_coincident={len(coincident)}, rows_now={len(out)} (from {before_rows}).\")\n",
    "\n",
    "# --- Validation for unexited & global constraints ---\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "un_mask = check[\"holding_status\"] == \"unexited\"\n",
    "ex_mask = check[\"holding_status\"] == \"exited\"\n",
    "\n",
    "# Unexited: exactly 2 rows per deal_id\n",
    "rows_per_un = check.loc[un_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_un == 2).all(), \"Unexited deals must have exactly 2 rows.\"\n",
    "\n",
    "# Validate the two rows are entry-match and latest≤today\n",
    "ck_un = check.loc[un_mask].copy()\n",
    "ck_un[\"_ref_dt\"] = pd.to_datetime(ck_un[\"reference_date\"], errors=\"coerce\")\n",
    "ck_un[\"_entry_dt\"] = pd.to_datetime(ck_un[\"entry_date\"], errors=\"coerce\")\n",
    "\n",
    "def entry_winner_verify(frame):\n",
    "    from pandas import DateOffset\n",
    "    tgt = frame.groupby(\"deal_id\")[\"_entry_dt\"].transform(\"first\")\n",
    "    start = tgt - DateOffset(months=3)\n",
    "    end   = tgt + DateOffset(months=3)\n",
    "    in_window = frame[\"_ref_dt\"].ge(start) & frame[\"_ref_dt\"].le(end)\n",
    "    tmp = frame.loc[in_window, [\"deal_id\",\"id\",\"_ref_dt\",\"_entry_dt\"]].copy()\n",
    "    tmp[\"_abs_diff_days\"]  = (tmp[\"_ref_dt\"] - tmp[\"_entry_dt\"]).abs().dt.days\n",
    "    tmp[\"_is_after_or_eq\"] = (tmp[\"_ref_dt\"] >= tmp[\"_entry_dt\"]).astype(int)\n",
    "    tmp = tmp.sort_values([\"deal_id\",\"_abs_diff_days\",\"_is_after_or_eq\",\"_ref_dt\"],\n",
    "                          ascending=[True, True, False, True])\n",
    "    return tmp.groupby(\"deal_id\")[\"id\"].first()\n",
    "\n",
    "entry_verify = entry_winner_verify(ck_un)\n",
    "latest_verify = (ck_un[ck_un[\"_ref_dt\"] <= pd.Timestamp.today().normalize()]\n",
    "                 .sort_values([\"deal_id\",\"_ref_dt\"], ascending=[True, False])\n",
    "                 .groupby(\"deal_id\")[\"id\"].first())\n",
    "\n",
    "for d, grp in ck_un.groupby(\"deal_id\"):\n",
    "    ids = set(grp[\"id\"])\n",
    "    assert d in entry_verify.index and d in latest_verify.index, f\"Deal {d}: missing entry or latest id.\"\n",
    "    assert entry_verify[d] in ids and latest_verify[d] in ids, f\"Deal {d}: kept rows are not entry+latest.\"\n",
    "\n",
    "# Exited: unchanged cardinality constraint (still ≤ 2)\n",
    "rows_per_ex = check.loc[ex_mask].groupby(\"deal_id\")[\"id\"].nunique()\n",
    "\n",
    "# Deal-level row-count report for unexited\n",
    "two_rows_un  = int((rows_per_un == 2).sum())\n",
    "one_row_un   = int((rows_per_un == 1).sum())\n",
    "gt2_rows_un  = int((rows_per_un > 2).sum())\n",
    "total_un     = int(rows_per_un.size)\n",
    "\n",
    "print(f\"Unexited row-counts per deal_id — 2 rows: {two_rows_un}, 1 row: {one_row_un}, >2 rows: {gt2_rows_un}, total: {total_un}\")\n",
    "\n",
    "# Keep your hard guarantees\n",
    "assert one_row_un == 0, \"Unexited deals with exactly 1 row found.\"\n",
    "assert gt2_rows_un == 0, \"Unexited deals with >2 rows found.\"\n",
    "assert (rows_per_un == 2).all(), \"Unexited deals must have exactly 2 rows.\"\n",
    "assert (rows_per_ex <= 2).all(), \"Exited deals show >2 rows after unexited processing.\"\n",
    "\n",
    "# Deal-level row-count report for ALL deals (exited + unexited)\n",
    "rows_per_all = check.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "two_rows_all = int((rows_per_all == 2).sum())\n",
    "one_row_all  = int((rows_per_all == 1).sum())\n",
    "gt2_rows_all = int((rows_per_all > 2).sum())\n",
    "total_all    = int(rows_per_all.size)\n",
    "\n",
    "# ---------- Step 3b: Enforce one currency per deal_id (drop mixed/unknown) ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "if \"reporting_currency_financials\" not in df.columns:\n",
    "    raise KeyError(\"Missing 'reporting_currency_financials'—ensure Step 3 added it.\")\n",
    "\n",
    "cur = df[\"reporting_currency_financials\"].astype(str).str.strip()\n",
    "null_tokens = {\"\", \"nan\", \"na\", \"n/a\", \"none\", \"null\"}\n",
    "cur = cur.mask(cur.str.lower().isin(null_tokens))\n",
    "\n",
    "df[\"_currency\"] = cur\n",
    "\n",
    "# Rule 1: require a defined currency for every row\n",
    "rows_with_null_cur = df[\"_currency\"].isna().sum()\n",
    "\n",
    "# Rule 2: require exactly one distinct currency per deal_id\n",
    "per_deal_nuniq = df.groupby(\"deal_id\")[\"_currency\"].nunique(dropna=False)\n",
    "bad_deals = per_deal_nuniq[(per_deal_nuniq != 1)].index\n",
    "\n",
    "keep_mask = df[\"deal_id\"].isin(bad_deals) == False\n",
    "keep_mask &= df[\"_currency\"].notna()\n",
    "\n",
    "before_deals = df[\"deal_id\"].nunique()\n",
    "before_rows  = len(df)\n",
    "\n",
    "out = df.loc[keep_mask].drop(columns=[\"_currency\"]).reset_index(drop=True)\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "after_deals = out[\"deal_id\"].nunique()\n",
    "after_rows  = len(out)\n",
    "\n",
    "print(f\"Currency consistency: dropped {before_deals - after_deals} deal_id(s) with mixed/unknown currencies; \"\n",
    "      f\"rows: {after_rows} (from {before_rows}).\")\n",
    "\n",
    "# Hard assertions\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "assert check[\"reporting_currency_financials\"].notna().all(), \"Null currency remains.\"\n",
    "nu = check.groupby(\"deal_id\")[\"reporting_currency_financials\"].nunique()\n",
    "assert (nu == 1).all(), \"Found deal(s) with multiple currencies.\"\n",
    "print(\"CURRENCY check passed.\")\n",
    "\n",
    "\n",
    "print(f\"All deals — 2 rows: {two_rows_all}, 1 row: {one_row_all}, >2 rows: {gt2_rows_all}, total: {total_all}\")\n",
    "\n",
    "# Hard guarantees across the whole dataset\n",
    "assert one_row_all == 0, \"Found deals with exactly 1 row.\"\n",
    "assert gt2_rows_all == 0, \"Found deals with >2 rows.\"\n",
    "assert (rows_per_all == 2).all(), \"All deals must have exactly 2 rows.\"\n",
    "\n",
    "print(\"Unexited selection check passed.\")\n",
    "\n",
    "p = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "g = pd.read_csv(p, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\", \"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "id": "394a8f291076b99f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 exited deal(s) where entry and exit map to the same id.\n",
      "Exited deals kept: 189 (dropped 105 with no entry/exit match in ±3 months). Rows now: 1166 (from 1382).\n",
      "Check passed. Exited deals with 2 rows: 189; with 1 row (entry=exit candidate): 0.\n",
      "unique_deals_exited: 189 unique_deals_unexited: 321 unique_deals_total: 510\n",
      "unique_deals: 510\n",
      "Unexited: survivors=196, dropped_coincident=31, rows_now=770 (from 1166).\n",
      "Unexited row-counts per deal_id — 2 rows: 196, 1 row: 0, >2 rows: 0, total: 196\n",
      "Currency consistency: dropped 1 deal_id(s) with mixed/unknown currencies; rows: 768 (from 770).\n",
      "CURRENCY check passed.\n",
      "All deals — 2 rows: 385, 1 row: 0, >2 rows: 0, total: 385\n",
      "Unexited selection check passed.\n",
      "unique_deals_exited: 189 unique_deals_unexited: 195 unique_deals_total: 384\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:17:38.617601Z",
     "start_time": "2025-10-15T20:17:38.409974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Calculate metrics for analysis + then drop unreasonable metrics + interest rate selector ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ============== 1) Add calculated value bridge fundamental columns (row-wise) ============\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "# Exclude faulty deals (as in your current logic)\n",
    "df = df.loc[df[\"deal_id\"] != \"83283299-100b-44c6-a997-ec634d90768d\"].copy()  # revenue > world gdp, database error\n",
    "df = df.loc[df[\"deal_id\"] != \"2376a9b2-d983-48fc-b62a-52fb78bf1409\"].copy()  # distorts TM scaling / broken deal\n",
    "df = df.loc[df[\"deal_id\"] != \"0906412f-6c13-4ae5-a973-2a860ca19a30\"].copy() # 71x Times Money Carve Out, toggle on and off to see effect in \"ValueCreationBySourcingType\", probably broken deal\n",
    "\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "ev = num(df[\"enterprise_value\"])\n",
    "eb = num(df[\"ebitda\"])\n",
    "nd = num(df[\"net_debt\"])\n",
    "eq = num(df[\"equity\"])\n",
    "rv = num(df[\"revenue\"])\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    ebitda_margin = (eb / rv).where(rv != 0)\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    xebitda = (ev / eb).where(eb != 0)\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    de_ratio = (nd / eq).where(eq != 0)\n",
    "\n",
    "# --- dividends & capital_injections at DEAL level from EXIT row (latest ref_date) ---\n",
    "df[\"_ref_dt\"] = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "# rank within deal: 1 = entry (earliest), 2 = exit (latest)\n",
    "df[\"_rank\"] = df.groupby(\"deal_id\")[\"_ref_dt\"].rank(method=\"first\", ascending=True)\n",
    "\n",
    "# equity at exit row (rank 2), per deal\n",
    "exit_eq = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "             .assign(exit_eq=eq[df[\"_rank\"] == 2].values)\n",
    "             .groupby(\"deal_id\")[\"exit_eq\"].first())\n",
    "\n",
    "# broadcast to both rows of each deal\n",
    "dividends = df[\"deal_id\"].map((0.0000001 * exit_eq).to_dict()).fillna(0.0000001)\n",
    "capital_injections = df[\"deal_id\"].map((-0.0000001 * exit_eq).to_dict()).fillna(-0.0000001)\n",
    "# skip dividends and capital injections\n",
    "# --- END dividends/cap injections ---\n",
    "\n",
    "# ============== 2) Interest rate selection  ============\n",
    "rates_path = find_upwards(Path(\"InputData/monthly_interest_rates_curr.csv\"))\n",
    "rates = pd.read_csv(rates_path)\n",
    "\n",
    "# Clean headers and parse dates to month-start\n",
    "rates.columns = [c.strip().upper() for c in rates.columns]\n",
    "assert \"DATE\" in rates.columns, f\"'DATE' column not found in {rates_path.name}\"\n",
    "rates[\"DATE\"] = pd.to_datetime(rates[\"DATE\"], errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# available currency columns in the curve file\n",
    "curve_ccy_cols = [c for c in rates.columns if c != \"DATE\"]\n",
    "curve_ccy_set = set(curve_ccy_cols)\n",
    "\n",
    "# per-deal entry month & currency (take first non-null per deal)\n",
    "entry_dt = pd.to_datetime(df[\"entry_date\"], errors=\"coerce\")\n",
    "deal_meta = (\n",
    "    df.assign(_entry_dt=entry_dt)\n",
    "      .sort_values([\"deal_id\", \"_entry_dt\"])  # stable \"first\"\n",
    "      .groupby(\"deal_id\", as_index=False)\n",
    "      .agg({\n",
    "          \"_entry_dt\": \"first\",\n",
    "          \"reporting_currency_financials\": \"first\",\n",
    "      })\n",
    ")\n",
    "deal_meta[\"entry_month\"] = deal_meta[\"_entry_dt\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "deal_meta[\"ccy_raw\"] = deal_meta[\"reporting_currency_financials\"].astype(str).str.strip()\n",
    "deal_meta[\"ccy_use\"] = deal_meta[\"ccy_raw\"].str.upper().where(\n",
    "    lambda s: s.str.upper().isin(curve_ccy_set),\n",
    "    other=\"EUR\"  # default if currency column not in curve\n",
    ")\n",
    "\n",
    "# lookup base rate per deal: closest row within ±1 month; if none, mark missing-date\n",
    "spread = 0.03\n",
    "miss_ccy = (deal_meta[\"ccy_use\"] != deal_meta[\"ccy_raw\"].str.upper()).sum()\n",
    "miss_date = 0\n",
    "\n",
    "rates_sorted = rates.sort_values(\"DATE\").reset_index(drop=True)\n",
    "\n",
    "base_rates = []\n",
    "for _, r in deal_meta.iterrows():\n",
    "    emon = r[\"entry_month\"]\n",
    "    ccy  = r[\"ccy_use\"]\n",
    "\n",
    "    if pd.isna(emon):\n",
    "        # no entry date -> cannot match a month\n",
    "        base_rates.append(np.nan)\n",
    "        miss_date += 1\n",
    "        continue\n",
    "\n",
    "    start = emon - pd.DateOffset(months=1)\n",
    "    end   = emon + pd.DateOffset(months=1)\n",
    "    window = rates_sorted[(rates_sorted[\"DATE\"] >= start) & (rates_sorted[\"DATE\"] <= end)]\n",
    "\n",
    "    if window.empty or (ccy not in window.columns):\n",
    "        base_rates.append(np.nan)\n",
    "        miss_date += 1\n",
    "        continue\n",
    "\n",
    "    # pick the row with the smallest |date - entry_month|\n",
    "    diffs = (window[\"DATE\"] - emon).abs().dt.days\n",
    "    j = diffs.idxmin()\n",
    "    base = pd.to_numeric(window.loc[j, ccy], errors=\"coerce\")\n",
    "    base_rates.append(base)\n",
    "\n",
    "deal_meta[\"base_rate\"] = pd.to_numeric(pd.Series(base_rates, index=deal_meta.index), errors=\"coerce\")\n",
    "deal_meta[\"interest_rate_deal\"] = deal_meta[\"base_rate\"] + spread  # annual coupon level\n",
    "\n",
    "# broadcast the per-deal rate to both rows\n",
    "interest_rate = df[\"deal_id\"].map(deal_meta.set_index(\"deal_id\")[\"interest_rate_deal\"].to_dict())\n",
    "\n",
    "print(f\"Interest rate assignment: {len(deal_meta) - int(miss_ccy)} deal(s) matched currency column; \"\n",
    "      f\"defaulted to EUR for {int(miss_ccy)} deal(s).\")\n",
    "print(f\"Interest rate assignment: {len(deal_meta) - int(miss_date)} deal(s) matched entry month within ±1; \"\n",
    "      f\"{int(miss_date)} deal(s) missing date match.\")\n",
    "\n",
    "# ============== 3) Holding period and compound total cost of debt ============\n",
    "today = pd.Timestamp.today().normalize()\n",
    "ent = pd.to_datetime(df.get(\"entry_date\"), errors=\"coerce\")\n",
    "exi = pd.to_datetime(df.get(\"exit_date\"),  errors=\"coerce\")\n",
    "is_exited = (df[\"holding_status\"] == \"exited\")\n",
    "\n",
    "days = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "days.loc[is_exited] = (exi - ent).dt.days.loc[is_exited]\n",
    "days.loc[~is_exited] = (today - ent).dt.days.loc[~is_exited]\n",
    "days = days.where(days >= 0)\n",
    "holding_period = days / 365.25\n",
    "\n",
    "# total holding-period cost: (1+r)^HP - 1\n",
    "cost_of_debt = (1 + interest_rate) ** holding_period - 1\n",
    "\n",
    "# write columns\n",
    "df[\"ebitda_margin\"]      = ebitda_margin\n",
    "df[\"xebitda\"]            = xebitda\n",
    "df[\"de_ratio\"]           = de_ratio\n",
    "df[\"dividends\"]          = dividends\n",
    "df[\"capital_injections\"] = capital_injections\n",
    "df[\"interest_rate\"]      = interest_rate\n",
    "df[\"holding_period\"]     = holding_period\n",
    "df[\"cost_of_debt\"]       = cost_of_debt\n",
    "\n",
    "# clean helper cols\n",
    "df = df.drop(columns=[\"_ref_dt\", \"_rank\"])\n",
    "\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "g = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "new_cols = [\"ebitda_margin\",\"xebitda\",\"de_ratio\",\"dividends\",\"capital_injections\",\"interest_rate\",\"holding_period\",\"cost_of_debt\"]\n",
    "nn = {c: int(g[c].notna().sum()) for c in new_cols}\n",
    "print(\"Added columns:\", \", \".join(new_cols))\n",
    "print(\"Non-null counts:\", nn)\n",
    "\n",
    "# ensure availability of by_status for the print (matches prior cells’ semantics)\n",
    "by_status = (g.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n",
    "\n",
    "print(\"Interest Rate NAs: \", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"))[\"interest_rate\"].isna().sum())\n",
    "\n",
    "# ===================== 4) Drop deals with unreasonable financial metrics =====================\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "before_deals = df[\"deal_id\"].nunique()\n",
    "\n",
    "em  = pd.to_numeric(df[\"ebitda_margin\"], errors=\"coerce\")\n",
    "xe  = pd.to_numeric(df[\"xebitda\"],        errors=\"coerce\")\n",
    "der = pd.to_numeric(df[\"de_ratio\"],       errors=\"coerce\")\n",
    "\n",
    "# =================  Specify what deals will get dropped. ==================\n",
    "# Example: xe < 0.1 | xe > 250 means that deals with xEBITDA <0.1 or >250 will be dropped.\n",
    "\n",
    "v_em  = (em > 1)\n",
    "abs_xe = xe.abs()\n",
    "v_xe  = xe.isna() | (abs_xe < 0.1) | (abs_xe > 250)\n",
    "v_der = der.isna() | (der < -1) | (der > 20)\n",
    "\n",
    "bad_em_deals  = set(df.loc[v_em,  \"deal_id\"].dropna().unique())\n",
    "bad_xe_deals  = set(df.loc[v_xe,  \"deal_id\"].dropna().unique())\n",
    "bad_der_deals = set(df.loc[v_der, \"deal_id\"].dropna().unique())\n",
    "bad_deals = bad_em_deals | bad_xe_deals | bad_der_deals\n",
    "\n",
    "keep_mask = ~df[\"deal_id\"].isin(bad_deals)\n",
    "out = df.loc[keep_mask].reset_index(drop=True)\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "after_rows  = len(out)\n",
    "after_deals = out[\"deal_id\"].nunique()\n",
    "print(\n",
    "    f\"Dropped {before_deals - after_deals} deal_id(s). \"\n",
    "    f\"[ebitda_margin>1: {len(bad_em_deals)}, \"\n",
    "    f\"xebitda NaN/|xebitda|<0.1/|xebitda|>250: {len(bad_xe_deals)}, \"\n",
    "    f\"de_ratio NaN/<-1/>20: {len(bad_der_deals)}]  Rows: {after_rows} (from {before_rows}).\"\n",
    ")\n",
    "\n",
    "# ---- checks ----\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "em2  = pd.to_numeric(check[\"ebitda_margin\"], errors=\"coerce\")\n",
    "xe2  = pd.to_numeric(check[\"xebitda\"],        errors=\"coerce\")\n",
    "der2 = pd.to_numeric(check[\"de_ratio\"],       errors=\"coerce\")\n",
    "\n",
    "assert not (em2 > 1).any(), \"Remaining rows with ebitda_margin > 1.\"\n",
    "\n",
    "abs_xe2 = xe2.abs()\n",
    "assert not (xe2.isna() | (abs_xe2 < 0.1) | (abs_xe2 > 250)).any(), \"Remaining rows with invalid xebitda.\"\n",
    "\n",
    "assert not (der2.isna() | (der2 < -1) | (der2 > 20)).any(), \"Remaining rows with invalid de_ratio.\"\n",
    "rows_per_deal = check.groupby(\"deal_id\")[\"id\"].nunique()\n",
    "assert (rows_per_deal == 2).all(), \"Each remaining deal must have exactly 2 rows.\"\n",
    "\n",
    "g = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "id": "dbaee67da1b8de4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interest rate assignment: 361 deal(s) matched currency column; defaulted to EUR for 20 deal(s).\n",
      "Interest rate assignment: 381 deal(s) matched entry month within ±1; 0 deal(s) missing date match.\n",
      "Added columns: ebitda_margin, xebitda, de_ratio, dividends, capital_injections, interest_rate, holding_period, cost_of_debt\n",
      "Non-null counts: {'ebitda_margin': 762, 'xebitda': 762, 'de_ratio': 762, 'dividends': 762, 'capital_injections': 762, 'interest_rate': 762, 'holding_period': 762, 'cost_of_debt': 762}\n",
      "unique_deals_exited: 187 unique_deals_unexited: 194 unique_deals_total: 381\n",
      "Interest Rate NAs:  0\n",
      "Dropped 7 deal_id(s). [ebitda_margin>1: 1, xebitda NaN/|xebitda|<0.1/|xebitda|>250: 5, de_ratio NaN/<-1/>20: 2]  Rows: 748 (from 762).\n",
      "unique_deals_exited: 184 unique_deals_unexited: 190 unique_deals_total: 374\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
