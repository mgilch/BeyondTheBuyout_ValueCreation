{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-03T14:33:42.303417Z",
     "start_time": "2025-10-03T14:33:42.200012Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#----- Start the data transformation by adding \"holding_status\" -----#\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    # Helpful diagnostics\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "before_rows = len(df)\n",
    "\n",
    "# Normalize exit_date blanks to NA (handles \"\", \"NaT\" strings)\n",
    "if \"exit_date\" not in df.columns:\n",
    "    raise KeyError(\"Column 'exit_date' not found in working.csv. Run the earlier ADD_COLUMNS step from 'deal' first.\")\n",
    "norm = df[\"exit_date\"].copy()\n",
    "if norm.dtype == object:\n",
    "    norm = norm.replace({\"\": pd.NA, \"NaT\": pd.NA, \"nat\": pd.NA, \"None\": pd.NA})\n",
    "\n",
    "# Determine holding_status: any non-null exit_date => exited; else unexited\n",
    "is_exited = norm.notna()\n",
    "df[\"holding_status\"] = is_exited.map({True: \"exited\", False: \"unexited\"})\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(\"Added column: holding_status\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "\n",
    "# Row-level coverage\n",
    "exited_count = (check[\"holding_status\"] == \"exited\").sum()\n",
    "unexited_count = (check[\"holding_status\"] == \"unexited\").sum()\n",
    "total_after = len(check)\n",
    "assert exited_count + unexited_count == total_after, \"Status coverage failed: counts don't add up to total rows.\"\n",
    "\n",
    "# Deal-level consistency: each deal_id should have a single status\n",
    "status_per_deal = check.groupby(\"deal_id\")[\"holding_status\"].nunique(dropna=False)\n",
    "mixed = status_per_deal[status_per_deal > 1]\n",
    "assert mixed.empty, f\"{len(mixed)} deal_id(s) have mixed exited/unexited rows.\"\n",
    "\n",
    "print(f\"Check passed. exited={exited_count}, unexited={unexited_count}, total_rows={total_after}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added column: holding_status\n",
      "Check passed. exited=2511, unexited=2445, total_rows=4956\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T14:33:42.404289Z",
     "start_time": "2025-10-03T14:33:42.312815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "#----- Filter for unreasonable dates -----#\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# Parse dates (tolerant). Expect ISO strings like \"YYYY-MM-DD\" from earlier steps.\n",
    "ref = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "lower = pd.Timestamp(1980, 1, 1).normalize()\n",
    "q_end = pd.Timestamp.today().to_period(\"Q-DEC\").end_time.normalize()\n",
    "date_ok = ref.isna() | ((ref >= lower) & (ref <= q_end))\n",
    "\n",
    "# --- Revenue non-zero (treat non-numeric/NA as \"keep\")\n",
    "rev_num = pd.to_numeric(df[\"revenue\"], errors=\"coerce\")\n",
    "rev_ok = rev_num.fillna(np.inf) != 0\n",
    "\n",
    "# Keep if reference_date is NA OR within [lower, q_end]; drop otherwise.\n",
    "keep_mask = date_ok & rev_ok\n",
    "\n",
    "# Preserve original order\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "\n",
    "dropped = before_rows - len(after)\n",
    "print(f\"Dropped {dropped} rows outside [{lower.date()} .. {q_end.date()}] or with revenue == 0.\")\n",
    "\n",
    "# Save\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "ref2 = pd.to_datetime(check[\"reference_date\"], errors=\"coerce\")\n",
    "\n",
    "# 1) Dates within bounds (for non-null)\n",
    "assert ((ref2.dropna() >= lower) & (ref2.dropna() <= q_end)).all(), \"Found dates outside bounds.\"\n",
    "\n",
    "# 2) No revenue == 0\n",
    "rev2 = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "assert not (rev2 == 0).any(), \"Found rows with revenue == 0.\"\n",
    "\n",
    "print(f\"FILTER check passed. Remaining rows: {len(check)}\")\n"
   ],
   "id": "635e1b3e20b82d56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 31 rows outside [1980-01-01 .. 2025-12-31] or with revenue == 0.\n",
      "FILTER check passed. Remaining rows: 4925\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T14:33:42.471949Z",
     "start_time": "2025-10-03T14:33:42.413700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "before_rows = len(df)\n",
    "\n",
    "# Temporary numeric views for presence checks (does not change df)\n",
    "def num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "rev_num   = num(df[\"revenue\"])\n",
    "ebitda_num= num(df[\"ebitda\"])\n",
    "ev_num    = num(df[\"enterprise_value\"])\n",
    "nd_num    = num(df[\"net_debt\"])\n",
    "eq_num    = num(df[\"equity\"])\n",
    "\n",
    "# Presence logic\n",
    "rev_ok    = rev_num.notna()\n",
    "ebitda_ok = ebitda_num.notna()\n",
    "trio_non_null = ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)\n",
    "trio_ok   = trio_non_null >= 2\n",
    "\n",
    "keep_mask = rev_ok & ebitda_ok & trio_ok\n",
    "\n",
    "# Preserve original order exactly\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "after = df.loc[keep_mask].sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True)\n",
    "\n",
    "dropped = before_rows - len(after)\n",
    "print(f\"Filtering out {dropped} rows (kept {len(after)} of {before_rows}).\")\n",
    "\n",
    "# Save\n",
    "after.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "# Recompute numeric presence on the saved data for verification\n",
    "rev_num   = pd.to_numeric(check[\"revenue\"], errors=\"coerce\")\n",
    "ebitda_num= pd.to_numeric(check[\"ebitda\"], errors=\"coerce\")\n",
    "ev_num    = pd.to_numeric(check[\"enterprise_value\"], errors=\"coerce\")\n",
    "nd_num    = pd.to_numeric(check[\"net_debt\"], errors=\"coerce\")\n",
    "eq_num    = pd.to_numeric(check[\"equity\"], errors=\"coerce\")\n",
    "\n",
    "assert rev_num.notna().all(), \"Found rows with empty revenue after filtering.\"\n",
    "assert ebitda_num.notna().all(), \"Found rows with empty ebitda after filtering.\"\n",
    "assert ((ev_num.notna().astype(int) + nd_num.notna().astype(int) + eq_num.notna().astype(int)) >= 2).all(), \\\n",
    "       \"Found rows with fewer than two of [enterprise_value, net_debt, equity] present.\"\n",
    "\n",
    "print(\"FILTER check passed. Shape:\", check.shape)\n"
   ],
   "id": "fbe9eecaf8b30ee7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 2760 rows (kept 2165 of 4925).\n",
      "FILTER check passed. Shape: (2165, 28)\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
