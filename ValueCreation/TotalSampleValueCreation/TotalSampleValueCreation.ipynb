{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-06T01:13:53.990247Z",
     "start_time": "2025-10-06T01:13:53.914513Z"
    }
   },
   "source": [
    "# Value Creation Bridge aligned to Achleitner et al. 2010; Achleitner, Braun & Puche 2015; Söffge & Braun 2017)\n",
    "\n",
    "# === Core Value-Bridge Items: times_money and tm_unlevered (exit-row div/cap injection) ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ---- load + checks ----\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# Order within deal: 1 = entry (earliest), 2 = exit (latest)\n",
    "df[\"_ref_dt\"] = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_rank\"]   = df.groupby(\"deal_id\")[\"_ref_dt\"].rank(method=\"first\", ascending=True)\n",
    "\n",
    "eq  = num(df[\"equity\"])\n",
    "der = num(df[\"de_ratio\"])\n",
    "\n",
    "# Entry / Exit equity\n",
    "equity_entry = (df.loc[df[\"_rank\"] == 1, [\"deal_id\"]]\n",
    "                  .assign(equity_entry=eq[df[\"_rank\"] == 1].values)\n",
    "                  .groupby(\"deal_id\")[\"equity_entry\"].first())\n",
    "equity_exit  = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "                  .assign(equity_exit=eq[df[\"_rank\"] == 2].values)\n",
    "                  .groupby(\"deal_id\")[\"equity_exit\"].first())\n",
    "\n",
    "# ---- deal-level inputs (exit-row dividends & capital injections) ----\n",
    "div_exit = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "              .assign(div_exit=num(df[\"dividends\"])[df[\"_rank\"] == 2].values)\n",
    "              .groupby(\"deal_id\")[\"div_exit\"].first())\n",
    "\n",
    "cap_exit = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "              .assign(cap_exit=num(df[\"capital_injections\"])[df[\"_rank\"] == 2].values)\n",
    "              .groupby(\"deal_id\")[\"cap_exit\"].first())\n",
    "\n",
    "# Averages unchanged\n",
    "der_avg = df.groupby(\"deal_id\")[\"de_ratio\"].apply(lambda s: num(s).mean())\n",
    "cod_avg = df.groupby(\"deal_id\")[\"cost_of_debt\"].apply(lambda s: num(s).mean())\n",
    "\n",
    "# ---- formulas (deal level) ----\n",
    "d_equity = equity_exit - equity_entry\n",
    "net_capital_gain = d_equity + div_exit + cap_exit\n",
    "invested_capital = equity_entry - cap_exit\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    times_money = (net_capital_gain / invested_capital).where(invested_capital != 0)\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    tm_unlevered = (times_money + cod_avg * der_avg) / (1 + der_avg)\n",
    "\n",
    "leverage_effect = times_money - tm_unlevered\n",
    "\n",
    "# ---- broadcast to both rows ----\n",
    "deal_metrics = pd.DataFrame({\n",
    "    \"deal_id\": d_equity.index,\n",
    "    \"d_equity\": d_equity.values,\n",
    "    \"net_capital_gain\": net_capital_gain.values,\n",
    "    \"invested_capital\": invested_capital.values,\n",
    "    \"times_money\": times_money.values,\n",
    "    \"tm_unlevered\": tm_unlevered.values,\n",
    "    \"leverage_effect\": leverage_effect.values,\n",
    "}).set_index(\"deal_id\")\n",
    "\n",
    "out = df.drop(columns=[\"_ref_dt\",\"_rank\"]).merge(\n",
    "    deal_metrics, left_on=\"deal_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# ---- persist + checks ----\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "chk = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "assert chk.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Row cardinality changed.\"\n",
    "for c in [\"d_equity\",\"net_capital_gain\",\"invested_capital\",\"times_money\",\"tm_unlevered\",\"leverage_effect\"]:\n",
    "    assert c in chk.columns, f\"Missing column: {c}\"\n",
    "\n",
    "print(\"Deal-level metrics added (exit-row div/cap applied):\",\n",
    "      [\"d_equity\",\"net_capital_gain\",\"invested_capital\",\"times_money\",\"tm_unlevered\",\"leverage_effect\"])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal-level metrics added (exit-row div/cap applied): ['d_equity', 'net_capital_gain', 'invested_capital', 'times_money', 'tm_unlevered', 'leverage_effect']\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T01:13:54.056947Z",
     "start_time": "2025-10-06T01:13:53.998935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Absolute Value Drivers: Multiple Effect, FCF Effect, EBITDA Effect ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ---- load + guarantees ----\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "# ---- ordering within deal: entry (earliest) vs exit (latest) ----\n",
    "df[\"_ref_dt\"] = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "# rank ascending: 1 = entry (earliest), 2 = exit (latest)\n",
    "df[\"_rank\"] = df.groupby(\"deal_id\")[\"_ref_dt\"].rank(method=\"first\", ascending=True)\n",
    "\n",
    "# convenience\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "xe  = num(df[\"xebitda\"])\n",
    "eb  = num(df[\"ebitda\"])\n",
    "rv  = num(df[\"revenue\"])\n",
    "mg  = num(df[\"ebitda_margin\"])\n",
    "nd  = num(df[\"net_debt\"])\n",
    "div = num(df[\"dividends\"])\n",
    "cap = num(df[\"capital_injections\"])\n",
    "\n",
    "# ---- pick entry / exit values per deal ----\n",
    "# entry = rank 1; exit = rank 2\n",
    "def pick(series, rank_val, name):\n",
    "    s = series[df[\"_rank\"] == rank_val]\n",
    "    out = df.loc[df[\"_rank\"] == rank_val, [\"deal_id\"]].assign(**{name: s.values})\n",
    "    return out.groupby(\"deal_id\")[name].first()\n",
    "\n",
    "xebitda_entry  = pick(xe, 1, \"xebitda_entry\")\n",
    "xebitda_exit   = pick(xe, 2, \"xebitda_exit\")\n",
    "ebitda_entry   = pick(eb, 1, \"ebitda_entry\")\n",
    "ebitda_exit    = pick(eb, 2, \"ebitda_exit\")\n",
    "revenue_entry  = pick(rv, 1, \"revenue_entry\")\n",
    "revenue_exit   = pick(rv, 2, \"revenue_exit\")\n",
    "margin_entry   = pick(mg, 1, \"margin_entry\")\n",
    "margin_exit    = pick(mg, 2, \"margin_exit\")\n",
    "net_debt_entry = pick(nd, 1, \"net_debt_entry\")\n",
    "net_debt_exit  = pick(nd, 2, \"net_debt_exit\")\n",
    "# dividends / capital_injections from EXIT row, as specified\n",
    "div_exit       = pick(div, 2, \"div_exit\")\n",
    "cap_exit       = pick(cap, 2, \"cap_exit\")\n",
    "\n",
    "# sanitize non-finite drivers used in products\n",
    "xebitda_entry  = xebitda_entry.where(np.isfinite(xebitda_entry))\n",
    "margin_entry   = margin_entry.where(np.isfinite(margin_entry))\n",
    "\n",
    "# ---- deltas (exit - entry), except d_debt = entry - exit (deleverage positive) ----\n",
    "d_multiple = xebitda_exit - xebitda_entry\n",
    "d_ebitda   = ebitda_exit  - ebitda_entry\n",
    "d_revenue  = revenue_exit - revenue_entry\n",
    "d_margin   = margin_exit  - margin_entry\n",
    "d_debt     = net_debt_entry - net_debt_exit  # entry − exit (intentional)\n",
    "\n",
    "# ---- effects ----\n",
    "# 1) multiple_effect = ΔMultiple × EBITDA_entry\n",
    "multiple_effect = d_multiple * ebitda_entry\n",
    "\n",
    "# 2) fcf_effect = d_debt + dividends_exit + capital_injections_exit\n",
    "fcf_effect = d_debt + div_exit + cap_exit\n",
    "\n",
    "# 3) multiple_ebitda_combination_effect = ΔMultiple × ΔEBITDA\n",
    "multiple_ebitda_combination_effect = d_multiple * d_ebitda\n",
    "\n",
    "# 4) ebitda_effect = ΔEBITDA × Multiple_entry\n",
    "ebitda_effect = d_ebitda * xebitda_entry\n",
    "\n",
    "# 5) sales_effect = ΔRevenue × margin_entry × Multiple_entry\n",
    "sales_effect = d_revenue * margin_entry * xebitda_entry\n",
    "\n",
    "# 6) margin_effect = ΔMargin × revenue_entry × Multiple_entry\n",
    "margin_effect = d_margin * revenue_entry * xebitda_entry\n",
    "\n",
    "# 7) sales_margin_combination_effect = ΔRevenue × ΔMargin × Multiple_entry\n",
    "sales_margin_combination_effect = d_revenue * d_margin * xebitda_entry\n",
    "\n",
    "# ---- bundle per-deal results and broadcast to both rows ----\n",
    "deal_effects = pd.DataFrame({\n",
    "    \"deal_id\": d_multiple.index,\n",
    "    \"multiple_effect\": multiple_effect.values,\n",
    "    \"fcf_effect\": fcf_effect.values,\n",
    "    \"multiple_ebitda_combination_effect\": multiple_ebitda_combination_effect.values,\n",
    "    \"ebitda_effect\": ebitda_effect.values,\n",
    "    \"sales_effect\": sales_effect.values,\n",
    "    \"margin_effect\": margin_effect.values,\n",
    "    \"sales_margin_combination_effect\": sales_margin_combination_effect.values,\n",
    "}).set_index(\"deal_id\")\n",
    "\n",
    "out = df.drop(columns=[\"_ref_dt\", \"_rank\"]).merge(\n",
    "    deal_effects, left_on=\"deal_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# ---- persist + minimal checks ----\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "chk = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "assert chk.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Row cardinality changed.\"\n",
    "for c in [\n",
    "    \"multiple_effect\",\"fcf_effect\",\"multiple_ebitda_combination_effect\",\n",
    "    \"ebitda_effect\",\"sales_effect\",\"margin_effect\",\"sales_margin_combination_effect\"\n",
    "]:\n",
    "    assert c in chk.columns, f\"Missing column: {c}\"\n",
    "\n",
    "print(\"Deal-level effects added:\",\n",
    "      [\"multiple_effect\",\"fcf_effect\",\"multiple_ebitda_combination_effect\",\n",
    "       \"ebitda_effect\",\"sales_effect\",\"margin_effect\",\"sales_margin_combination_effect\"])\n"
   ],
   "id": "22539dc9d5c9bd1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal-level effects added: ['multiple_effect', 'fcf_effect', 'multiple_ebitda_combination_effect', 'ebitda_effect', 'sales_effect', 'margin_effect', 'sales_margin_combination_effect']\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T01:13:54.112469Z",
     "start_time": "2025-10-06T01:13:54.066931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Relative Value Drivers: Multiple Effect, FCF Effect, EBITDA Effect ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ---- load + guarantee ----\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "# ---- relative effects ----\n",
    "effects = [\n",
    "    \"multiple_effect\",\n",
    "    \"fcf_effect\",\n",
    "    \"multiple_ebitda_combination_effect\",\n",
    "    \"ebitda_effect\",\n",
    "    \"sales_effect\",\n",
    "    \"margin_effect\",\n",
    "    \"sales_margin_combination_effect\",\n",
    "]\n",
    "\n",
    "ncg = pd.to_numeric(df[\"net_capital_gain\"], errors=\"coerce\")\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    for e in effects:\n",
    "        val = pd.to_numeric(df[e], errors=\"coerce\")\n",
    "        df[f\"relative_{e}\"] = (val / ncg).where(ncg != 0)\n",
    "\n",
    "# ---- persist ----\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "print(\"Computed relative effects for:\", effects)\n"
   ],
   "id": "87af134034fc5e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed relative effects for: ['multiple_effect', 'fcf_effect', 'multiple_ebitda_combination_effect', 'ebitda_effect', 'sales_effect', 'margin_effect', 'sales_margin_combination_effect']\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T01:13:54.190080Z",
     "start_time": "2025-10-06T01:13:54.135022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === QA + TM_unlevered-scaled (tmu_contrib_) driver contributions, including EBITDA sub-breakdown ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        cand = parent / rel_path\n",
    "        if cand.exists():\n",
    "            return cand.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ---- load + guarantee ----\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# --- required columns\n",
    "abs_main = [\"multiple_effect\",\"ebitda_effect\",\"multiple_ebitda_combination_effect\",\"fcf_effect\"]\n",
    "abs_sub  = [\"sales_effect\",\"margin_effect\",\"sales_margin_combination_effect\"]\n",
    "rel_all  = [f\"relative_{c}\" for c in abs_main + abs_sub]\n",
    "\n",
    "rel = df[rel_all].apply(num)\n",
    "tm  = num(df[\"times_money\"])\n",
    "tmu = num(df[\"tm_unlevered\"])\n",
    "ic  = num(df[\"invested_capital\"])\n",
    "\n",
    "# ---------- (A) TM-scale (levered) check: (Σ abs main)/IC ≈ TM ----------\n",
    "sum_abs_main = df[abs_main].apply(num).sum(axis=1, min_count=1)\n",
    "tm_from_abs  = (sum_abs_main / ic).where(ic != 0)\n",
    "diff_tm = tm - tm_from_abs\n",
    "tol_tm  = 0.01 * tm.abs()\n",
    "ok_tm_row  = (diff_tm.abs() <= tol_tm) & np.isfinite(diff_tm) & np.isfinite(tol_tm)\n",
    "ok_tm_deal = ok_tm_row.groupby(df[\"deal_id\"]).all()\n",
    "df[\"qa_tm_diff\"] = diff_tm\n",
    "\n",
    "# ---------- (B) TM_unlevered via relative shares: Σ (relative_i * tmu) ≈ tmu ----------\n",
    "# produce TM_unlevered-scaled contributions for ALL effects (main + sub)\n",
    "for c in abs_main + abs_sub:\n",
    "    df[f\"tmu_contrib_{c}\"] = rel[f\"relative_{c}\"] * tmu\n",
    "\n",
    "tmu_from_rel = df[[f\"tmu_contrib_{c}\" for c in abs_main]].sum(axis=1, min_count=1)\n",
    "diff_tmu = tmu - tmu_from_rel\n",
    "tol_tmu  = 0.01 * tmu.abs()\n",
    "ok_tmu_row  = (diff_tmu.abs() <= tol_tmu) & np.isfinite(diff_tmu) & np.isfinite(tol_tmu)\n",
    "ok_tmu_deal = ok_tmu_row.groupby(df[\"deal_id\"]).all()\n",
    "df[\"qa_tmu_diff\"] = diff_tmu\n",
    "\n",
    "# ---------- (C) EBITDA sub-breakdown on TM_unlevered scale ----------\n",
    "tmu_contrib_ebitda = df[\"tmu_contrib_ebitda_effect\"]\n",
    "tmu_contrib_subsum = df[[\"tmu_contrib_sales_effect\",\"tmu_contrib_margin_effect\",\"tmu_contrib_sales_margin_combination_effect\"]].sum(axis=1, min_count=1)\n",
    "\n",
    "diff_tmu_ebitda = tmu_contrib_ebitda - tmu_contrib_subsum\n",
    "# tolerance: ±1% of |tmu_contrib_ebitda| with small floor\n",
    "tol_tmu_ebitda = (0.01 * tmu_contrib_ebitda.abs()).where(lambda s: s > 1e-9, 1e-9)\n",
    "ok_tmu_ebitda_row = (diff_tmu_ebitda.abs() <= tol_tmu_ebitda) & np.isfinite(diff_tmu_ebitda) & np.isfinite(tol_tmu_ebitda)\n",
    "ok_tmu_ebitda_deal = ok_tmu_ebitda_row.groupby(df[\"deal_id\"]).all()\n",
    "df[\"qa_tmu_ebitda_sub_diff\"] = diff_tmu_ebitda\n",
    "\n",
    "# ---- persist ----\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# ---- summaries (deal-level) ----\n",
    "def summarize(name, mask):\n",
    "    ok = int(mask.sum()); out = int((~mask).sum())\n",
    "    print(f\"{name}: deals within ±1% = {ok} | outside ±1% = {out}\")\n",
    "\n",
    "summarize(\"TM (levered) identity\", ok_tm_deal)\n",
    "summarize(\"TM_unlevered (sum of tmu_contrib main effects)\", ok_tmu_deal)\n",
    "summarize(\"EBITDA sub-breakdown on TM_unlevered scale\", ok_tmu_ebitda_deal)\n",
    "\n",
    "print(\"tmu_contrib_ columns written for:\", abs_main + abs_sub)\n"
   ],
   "id": "4f1a352374dc41e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TM (levered) identity: deals within ±1% = 413 | outside ±1% = 0\n",
      "TM_unlevered (sum of tmu_contrib main effects): deals within ±1% = 413 | outside ±1% = 0\n",
      "EBITDA sub-breakdown on TM_unlevered scale: deals within ±1% = 413 | outside ±1% = 0\n",
      "tmu_contrib_ columns written for: ['multiple_effect', 'ebitda_effect', 'multiple_ebitda_combination_effect', 'fcf_effect', 'sales_effect', 'margin_effect', 'sales_margin_combination_effect']\n"
     ]
    }
   ],
   "execution_count": 72
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
