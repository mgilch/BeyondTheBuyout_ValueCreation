{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-07T01:15:17.707900Z",
     "start_time": "2025-10-07T01:15:17.628390Z"
    }
   },
   "source": [
    "# Value Creation Bridge aligned to Achleitner et al. 2010; Achleitner, Braun & Puche 2015; SÃ¶ffge & Braun 2017)\n",
    "\n",
    "# === Core Value-Bridge Items: times_money and tm_unlevered (exit-row div/cap injection) ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ---- load + checks ----\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# Order within deal: 1 = entry (earliest), 2 = exit (latest)\n",
    "df[\"_ref_dt\"] = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_rank\"]   = df.groupby(\"deal_id\")[\"_ref_dt\"].rank(method=\"first\", ascending=True)\n",
    "\n",
    "eq  = num(df[\"equity\"])\n",
    "\n",
    "# Entry / Exit equity\n",
    "equity_entry = (df.loc[df[\"_rank\"] == 1, [\"deal_id\"]]\n",
    "                  .assign(equity_entry=eq[df[\"_rank\"] == 1].values)\n",
    "                  .groupby(\"deal_id\")[\"equity_entry\"].first())\n",
    "equity_exit  = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "                  .assign(equity_exit=eq[df[\"_rank\"] == 2].values)\n",
    "                  .groupby(\"deal_id\")[\"equity_exit\"].first())\n",
    "\n",
    "# ---- deal-level inputs (exit-row dividends & capital injections) ----\n",
    "div_exit = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "              .assign(div_exit=num(df[\"dividends\"])[df[\"_rank\"] == 2].values)\n",
    "              .groupby(\"deal_id\")[\"div_exit\"].first())\n",
    "\n",
    "cap_exit = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "              .assign(cap_exit=num(df[\"capital_injections\"])[df[\"_rank\"] == 2].values)\n",
    "              .groupby(\"deal_id\")[\"cap_exit\"].first())\n",
    "\n",
    "div_exit = div_exit.fillna(0.0)\n",
    "cap_exit = cap_exit.fillna(0.0)\n",
    "\n",
    "# ---- dtype & presence guards (drop-in right after div_exit/cap_exit fillna) ----\n",
    "# Ensure numeric dtype and no missing equity at entry/exit (these are the two rows you rely on)\n",
    "assert equity_entry.notna().all(), \"Equity missing on entry rows.\"\n",
    "assert equity_exit.notna().all(),  \"Equity missing on exit rows.\"\n",
    "\n",
    "# Make sure dividends / cap injections are numeric after fill\n",
    "div_exit  = pd.to_numeric(div_exit, errors=\"coerce\").fillna(0.0)\n",
    "cap_exit  = pd.to_numeric(cap_exit, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "\n",
    "# ---- average D/E as entry/exit two-point mean; clip at 0 for unlevering ----\n",
    "der_series = num(df[\"de_ratio\"])\n",
    "der_entry  = df.loc[df[\"_rank\"] == 1, [\"deal_id\"]].assign(der=der_series[df[\"_rank\"] == 1].values) \\\n",
    "                 .groupby(\"deal_id\")[\"der\"].first()\n",
    "der_exit   = df.loc[df[\"_rank\"] == 2, [\"deal_id\"]].assign(der=der_series[df[\"_rank\"] == 2].values) \\\n",
    "                 .groupby(\"deal_id\")[\"der\"].first()\n",
    "der_avg    = pd.concat([der_entry, der_exit], axis=1).mean(axis=1).clip(lower=0.0)\n",
    "\n",
    "# ---- cost of debt: take the (deal-level) compounded total, not a mean ----\n",
    "cod_spread = df.groupby(\"deal_id\")[\"cost_of_debt\"].agg(lambda s: float(s.max() - s.min()))\n",
    "assert (cod_spread.abs() < 1e-12).all(), \"cost_of_debt differs across rows; confirm intent.\"\n",
    "\n",
    "cod_avg = df.groupby(\"deal_id\")[\"cost_of_debt\"].first()\n",
    "\n",
    "# ---- formulas (deal level) ----\n",
    "d_equity = equity_exit - equity_entry\n",
    "net_capital_gain = d_equity + div_exit + cap_exit\n",
    "invested_capital = equity_entry - cap_exit\n",
    "\n",
    "# ---- invested capital guard: require strictly positive IC ----\n",
    "valid_ic = invested_capital > 0\n",
    "if not valid_ic.all():\n",
    "    bad = (~valid_ic).sum()\n",
    "    print(f\"Dropping {bad} deal(s) with invested_capital <= 0.\")\n",
    "    # Hard drop of invalid deals from all deal-level vectors:\n",
    "    equity_entry      = equity_entry[valid_ic]\n",
    "    equity_exit       = equity_exit[valid_ic]\n",
    "    div_exit          = div_exit[valid_ic]\n",
    "    cap_exit          = cap_exit[valid_ic]\n",
    "    d_equity          = d_equity[valid_ic]\n",
    "    net_capital_gain  = net_capital_gain[valid_ic]\n",
    "    invested_capital  = invested_capital[valid_ic]\n",
    "    der_avg           = der_avg[valid_ic]\n",
    "    cod_avg           = cod_avg[valid_ic]\n",
    "\n",
    "# Remove invalid deals from the row-level frame as well\n",
    "survivor_deals = set(invested_capital.index)  # index currently is deal_id for valid deals\n",
    "df = df[df[\"deal_id\"].isin(survivor_deals)].copy()\n",
    "\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    times_money = (net_capital_gain / invested_capital).where(invested_capital != 0)\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    tm_unlevered = (times_money + cod_avg * der_avg) / (1 + der_avg)\n",
    "\n",
    "leverage_effect = times_money - tm_unlevered\n",
    "\n",
    "# ---- broadcast to both rows ----\n",
    "deal_metrics = pd.DataFrame({\n",
    "    \"deal_id\": d_equity.index,\n",
    "    \"d_equity\": d_equity.values,\n",
    "    \"net_capital_gain\": net_capital_gain.values,\n",
    "    \"invested_capital\": invested_capital.values,\n",
    "    \"times_money\": times_money.values,\n",
    "    \"tm_unlevered\": tm_unlevered.values,\n",
    "    \"leverage_effect\": leverage_effect.values,\n",
    "}).set_index(\"deal_id\")\n",
    "\n",
    "out = df.drop(columns=[\"_ref_dt\",\"_rank\"]).merge(\n",
    "    deal_metrics, left_on=\"deal_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# ---- persist + checks ----\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "chk = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "# exactly 2 rows per surviving deal\n",
    "assert chk.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Row cardinality changed.\"\n",
    "# no NaNs in core outputs\n",
    "req = [\"d_equity\",\"net_capital_gain\",\"invested_capital\",\"times_money\",\"tm_unlevered\",\"leverage_effect\"]\n",
    "assert set(req).issubset(chk.columns), \"Missing expected columns.\"\n",
    "assert chk[req].notna().all().all(), \"NaNs in deal-level outputs after merge.\"\n",
    "\n",
    "print(\"Deal-level metrics added (exit-row div/cap applied):\",\n",
    "      [\"d_equity\",\"net_capital_gain\",\"invested_capital\",\"times_money\",\"tm_unlevered\",\"leverage_effect\"])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal-level metrics added (exit-row div/cap applied): ['d_equity', 'net_capital_gain', 'invested_capital', 'times_money', 'tm_unlevered', 'leverage_effect']\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T01:15:17.780865Z",
     "start_time": "2025-10-07T01:15:17.721303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Absolute Value Drivers: Multiple, FCF, EBITDA (PBA-aligned) ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -- helper --\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- CWD: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# -- load + guarantees --\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "# -- order within deal: 1 = entry (earliest), 2 = exit/latest --\n",
    "df[\"_ref_dt\"] = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_rank\"] = df.groupby(\"deal_id\")[\"_ref_dt\"].rank(method=\"first\", ascending=True)\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "xe  = num(df[\"xebitda\"])\n",
    "eb  = num(df[\"ebitda\"])\n",
    "rv  = num(df[\"revenue\"])\n",
    "mg  = num(df[\"ebitda_margin\"])\n",
    "nd  = num(df[\"net_debt\"])\n",
    "div = num(df[\"dividends\"])\n",
    "cap = num(df[\"capital_injections\"])\n",
    "\n",
    "# -- pick entry / exit values per deal --\n",
    "def pick(series, rank_val, name):\n",
    "    s = series[df[\"_rank\"] == rank_val]\n",
    "    out = df.loc[df[\"_rank\"] == rank_val, [\"deal_id\"]].assign(**{name: s.values})\n",
    "    return out.groupby(\"deal_id\")[name].first()\n",
    "\n",
    "xebitda_entry  = pick(xe, 1, \"xebitda_entry\")\n",
    "xebitda_exit   = pick(xe, 2, \"xebitda_exit\")\n",
    "ebitda_entry   = pick(eb, 1, \"ebitda_entry\")\n",
    "ebitda_exit    = pick(eb, 2, \"ebitda_exit\")\n",
    "revenue_entry  = pick(rv, 1, \"revenue_entry\")\n",
    "revenue_exit   = pick(rv, 2, \"revenue_exit\")\n",
    "margin_entry   = pick(mg, 1, \"margin_entry\")\n",
    "margin_exit    = pick(mg, 2, \"margin_exit\")\n",
    "net_debt_entry = pick(nd, 1, \"net_debt_entry\")\n",
    "net_debt_exit  = pick(nd, 2, \"net_debt_exit\")\n",
    "div_exit       = pick(div, 2, \"div_exit\")\n",
    "cap_exit       = pick(cap, 2, \"cap_exit\")\n",
    "\n",
    "# -- driver sanity and required-data mask (drop undefined cores) --\n",
    "xebitda_entry  = xebitda_entry.where(np.isfinite(xebitda_entry))\n",
    "xebitda_exit   = xebitda_exit.where(np.isfinite(xebitda_exit))\n",
    "margin_entry   = margin_entry.where(np.isfinite(margin_entry))\n",
    "\n",
    "need = pd.Series(True, index=xebitda_entry.index)\n",
    "need &= xebitda_entry.notna()\n",
    "need &= xebitda_exit.notna()\n",
    "need &= margin_entry.notna()\n",
    "need &= ebitda_entry.notna() & ebitda_exit.notna()\n",
    "need &= revenue_entry.notna() & revenue_exit.notna()\n",
    "need &= net_debt_entry.notna() & net_debt_exit.notna()\n",
    "need &= div_exit.notna() & cap_exit.notna()\n",
    "\n",
    "if not need.all():\n",
    "    dropped = int((~need).sum())\n",
    "    print(f\"Dropping {dropped} deal(s) due to undefined core drivers.\")\n",
    "    xebitda_entry  = xebitda_entry[need]\n",
    "    xebitda_exit   = xebitda_exit[need]\n",
    "    ebitda_entry   = ebitda_entry[need]\n",
    "    ebitda_exit    = ebitda_exit[need]\n",
    "    revenue_entry  = revenue_entry[need]\n",
    "    revenue_exit   = revenue_exit[need]\n",
    "    margin_entry   = margin_entry[need]\n",
    "    margin_exit    = margin_exit[need]\n",
    "    net_debt_entry = net_debt_entry[need]\n",
    "    net_debt_exit  = net_debt_exit[need]\n",
    "    div_exit       = div_exit[need]\n",
    "    cap_exit       = cap_exit[need]\n",
    "    survivor_deals = set(need[need].index)\n",
    "    df = df[df[\"deal_id\"].isin(survivor_deals)].copy()\n",
    "\n",
    "# -- deltas (exit - entry); deleveraging defined as entry â exit (positive if debt reduced) --\n",
    "d_multiple = xebitda_exit - xebitda_entry\n",
    "d_ebitda   = ebitda_exit  - ebitda_entry\n",
    "d_revenue  = revenue_exit - revenue_entry\n",
    "d_margin   = margin_exit  - margin_entry\n",
    "d_debt     = net_debt_entry - net_debt_exit\n",
    "\n",
    "# -- effects (PBA absolute) --\n",
    "multiple_effect = d_multiple * ebitda_entry\n",
    "fcf_effect = d_debt + div_exit + cap_exit\n",
    "multiple_ebitda_combination_effect = d_multiple * d_ebitda\n",
    "ebitda_effect = d_ebitda * xebitda_entry\n",
    "sales_effect = d_revenue * margin_entry * xebitda_entry\n",
    "margin_effect = d_margin * revenue_entry * xebitda_entry\n",
    "sales_margin_combination_effect = d_revenue * d_margin * xebitda_entry\n",
    "\n",
    "# -- reconciliation to equity bridge (diagnostic residual) --\n",
    "ev_entry = xebitda_entry * ebitda_entry\n",
    "ev_exit  = xebitda_exit  * ebitda_exit\n",
    "delta_ev = ev_exit - ev_entry\n",
    "\n",
    "net_cap_gain_implied = delta_ev + (net_debt_entry - net_debt_exit) + div_exit + cap_exit\n",
    "sum_effects = multiple_effect + ebitda_effect + multiple_ebitda_combination_effect + fcf_effect\n",
    "effects_residual = sum_effects - net_cap_gain_implied\n",
    "effects_residual_rel = effects_residual / net_cap_gain_implied.replace(0, np.nan).abs()\n",
    "\n",
    "# -- bundle per-deal results and broadcast to both rows --\n",
    "deal_effects = pd.DataFrame({\n",
    "    \"deal_id\": d_multiple.index,\n",
    "    \"multiple_effect\": multiple_effect.values,\n",
    "    \"fcf_effect\": fcf_effect.values,\n",
    "    \"multiple_ebitda_combination_effect\": multiple_ebitda_combination_effect.values,\n",
    "    \"ebitda_effect\": ebitda_effect.values,\n",
    "    \"sales_effect\": sales_effect.values,\n",
    "    \"margin_effect\": margin_effect.values,\n",
    "    \"sales_margin_combination_effect\": sales_margin_combination_effect.values,\n",
    "    \"effects_residual\": effects_residual.values,\n",
    "    \"effects_residual_rel\": effects_residual_rel.fillna(0.0).values,\n",
    "}).set_index(\"deal_id\")\n",
    "\n",
    "out = df.drop(columns=[\"_ref_dt\", \"_rank\"]).merge(\n",
    "    deal_effects, left_on=\"deal_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# -- persist + checks --\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "chk = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "assert chk.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Row cardinality changed.\"\n",
    "req = [\n",
    "    \"multiple_effect\",\"fcf_effect\",\"multiple_ebitda_combination_effect\",\n",
    "    \"ebitda_effect\",\"sales_effect\",\"margin_effect\",\"sales_margin_combination_effect\",\n",
    "    \"effects_residual\",\"effects_residual_rel\"\n",
    "]\n",
    "assert set(req).issubset(chk.columns), \"Missing effect columns.\"\n",
    "assert chk[req].notna().all().all(), \"NaNs in effects after merge.\"\n",
    "\n",
    "bad_rec = (chk.groupby(\"deal_id\")[\"effects_residual\"].first().abs() > 1e-6) & \\\n",
    "          (chk.groupby(\"deal_id\")[\"effects_residual_rel\"].first().abs() > 1e-6)\n",
    "if bad_rec.any():\n",
    "    print(f\"Warning: {int(bad_rec.sum())} deal(s) fail effect reconciliation (abs/rel tolerance).\")\n",
    "\n",
    "print(\"Deal-level effects added:\",\n",
    "      [\"multiple_effect\",\"fcf_effect\",\"multiple_ebitda_combination_effect\",\n",
    "       \"ebitda_effect\",\"sales_effect\",\"margin_effect\",\"sales_margin_combination_effect\",\n",
    "       \"effects_residual\",\"effects_residual_rel\"])\n"
   ],
   "id": "22539dc9d5c9bd1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal-level effects added: ['multiple_effect', 'fcf_effect', 'multiple_ebitda_combination_effect', 'ebitda_effect', 'sales_effect', 'margin_effect', 'sales_margin_combination_effect', 'effects_residual', 'effects_residual_rel']\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T01:15:17.832135Z",
     "start_time": "2025-10-07T01:15:17.791340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Relative Value Drivers: shares of net capital gain (deal-level, with residual) ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        p = parent / rel_path\n",
    "        if p.exists():\n",
    "            return p.resolve()\n",
    "    raise FileNotFoundError(f\"Couldn't locate {rel_path} from {Path.cwd()}\")\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ---- load + guarantees ----\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "# Effects computed earlier (absolute, per deal but broadcast to both rows)\n",
    "effects = [\n",
    "    \"multiple_effect\",\n",
    "    \"fcf_effect\",\n",
    "    \"multiple_ebitda_combination_effect\",\n",
    "    \"ebitda_effect\",\n",
    "    \"sales_effect\",\n",
    "    \"margin_effect\",\n",
    "    \"sales_margin_combination_effect\",\n",
    "]\n",
    "# include reconciliation residual from previous cell\n",
    "effects_all = effects + [\"effects_residual\"]\n",
    "\n",
    "# ---- compute relative shares (per deal), robust to tiny denominators ----\n",
    "ncg = pd.to_numeric(df[\"net_capital_gain\"], errors=\"coerce\")\n",
    "# Treat ultra-small |NCG| as zero to avoid exploding ratios\n",
    "eps = 1e-12\n",
    "valid_den = ncg.abs() > eps\n",
    "\n",
    "for e in effects_all:\n",
    "    num = pd.to_numeric(df[e], errors=\"coerce\")\n",
    "    rel = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "    rel[valid_den] = num[valid_den] / ncg[valid_den]\n",
    "    df[f\"relative_{e}\"] = rel\n",
    "\n",
    "# ---- per-deal sanity: sums should be ~1 (only where denom is valid) ----\n",
    "grp = df.groupby(\"deal_id\")\n",
    "rel_sum = grp[[f\"relative_{e}\" for e in effects_all]].first().sum(axis=1)\n",
    "ok = rel_sum[valid_den.groupby(df[\"deal_id\"]).first()].sub(1.0).abs() <= 1e-6\n",
    "if not ok.all():\n",
    "    bad = (~ok).sum()\n",
    "    print(f\"Warning: {bad} deal(s) have relative effects that do not sum to 1 within tolerance.\")\n",
    "\n",
    "# ---- persist ----\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(\"Computed relative effects (including residual) for:\",\n",
    "      effects_all)\n"
   ],
   "id": "87af134034fc5e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 354 deal(s) have relative effects that do not sum to 1 within tolerance.\n",
      "Computed relative effects (including residual) for: ['multiple_effect', 'fcf_effect', 'multiple_ebitda_combination_effect', 'ebitda_effect', 'sales_effect', 'margin_effect', 'sales_margin_combination_effect', 'effects_residual']\n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T01:15:17.898187Z",
     "start_time": "2025-10-07T01:15:17.841924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === QA + TM_unlevered-scaled (tmu_contrib_) driver contributions, incl. EBITDA sub-breakdown ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        cand = parent / rel_path\n",
    "        if cand.exists():\n",
    "            return cand.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ---- load + guarantee ----\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "print(\"Deals in QA scope:\", df[\"deal_id\"].nunique())\n",
    "\n",
    "num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# --- required columns\n",
    "abs_main = [\"multiple_effect\",\"ebitda_effect\",\"multiple_ebitda_combination_effect\",\"fcf_effect\"]\n",
    "abs_sub  = [\"sales_effect\",\"margin_effect\",\"sales_margin_combination_effect\"]\n",
    "rel_all  = [f\"relative_{c}\" for c in abs_main + abs_sub]\n",
    "\n",
    "rel = df[rel_all].apply(num)\n",
    "tm  = num(df[\"times_money\"])\n",
    "tmu = num(df[\"tm_unlevered\"])\n",
    "ic  = num(df[\"invested_capital\"])\n",
    "\n",
    "# ---- driver completeness mask (NaN/Inf guard); do not drop, just exclude from QA tallies\n",
    "drivers = abs_main + abs_sub + [\"times_money\",\"tm_unlevered\",\"invested_capital\"]\n",
    "drv_ok_row = df[drivers].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).notna().all(axis=1)\n",
    "drivers_ok_deal = drv_ok_row.groupby(df[\"deal_id\"]).all()\n",
    "\n",
    "# ---------- (A) TM-scale (levered) check: (Î£ abs main)/IC â TM ----------\n",
    "sum_abs_main = df[abs_main].apply(num).sum(axis=1, min_count=1)\n",
    "tm_from_abs  = (sum_abs_main / ic).where(ic != 0)\n",
    "\n",
    "diff_tm = tm - tm_from_abs\n",
    "# 1% band with floor to avoid zero-tolerance near TMâ0\n",
    "tol_tm  = np.maximum(1e-9, 0.01 * tm.abs())\n",
    "\n",
    "ok_tm_row  = (diff_tm.abs() <= tol_tm) & np.isfinite(diff_tm) & np.isfinite(tol_tm)\n",
    "ok_tm_deal = ok_tm_row.groupby(df[\"deal_id\"]).all() & drivers_ok_deal\n",
    "df[\"qa_tm_diff\"] = diff_tm\n",
    "\n",
    "# ---------- (B) TM_unlevered via relative shares: Î£ (relative_main * tmu) â tmu ----------\n",
    "for c in abs_main + abs_sub:\n",
    "    df[f\"tmu_contrib_{c}\"] = rel[f\"relative_{c}\"] * tmu\n",
    "\n",
    "tmu_from_rel = df[[f\"tmu_contrib_{c}\" for c in abs_main]].sum(axis=1, min_count=1)\n",
    "diff_tmu = tmu - tmu_from_rel\n",
    "tol_tmu  = np.maximum(1e-9, 0.01 * tmu.abs())\n",
    "\n",
    "ok_tmu_row  = (diff_tmu.abs() <= tol_tmu) & np.isfinite(diff_tmu) & np.isfinite(tol_tmu)\n",
    "ok_tmu_deal = ok_tmu_row.groupby(df[\"deal_id\"]).all() & drivers_ok_deal\n",
    "df[\"qa_tmu_diff\"] = diff_tmu\n",
    "\n",
    "# ---------- (C) EBITDA sub-breakdown on TM_unlevered scale ----------\n",
    "tmu_contrib_ebitda = df[\"tmu_contrib_ebitda_effect\"]\n",
    "tmu_contrib_subsum = df[[\n",
    "    \"tmu_contrib_sales_effect\",\"tmu_contrib_margin_effect\",\"tmu_contrib_sales_margin_combination_effect\"\n",
    "]].sum(axis=1, min_count=1)\n",
    "\n",
    "diff_tmu_ebitda = tmu_contrib_ebitda - tmu_contrib_subsum\n",
    "tol_tmu_ebitda  = np.maximum(1e-9, 0.01 * tmu_contrib_ebitda.abs())\n",
    "ok_tmu_ebitda_row  = (diff_tmu_ebitda.abs() <= tol_tmu_ebitda) & np.isfinite(diff_tmu_ebitda) & np.isfinite(tol_tmu_ebitda)\n",
    "ok_tmu_ebitda_deal = ok_tmu_ebitda_row.groupby(df[\"deal_id\"]).all() & drivers_ok_deal\n",
    "df[\"qa_tmu_ebitda_sub_diff\"] = diff_tmu_ebitda\n",
    "\n",
    "# ---------- Deal-level diffs (single value per deal), merged back for convenience ----------\n",
    "deal_tm_diff          = diff_tm.groupby(df[\"deal_id\"]).first()\n",
    "deal_tmu_diff         = diff_tmu.groupby(df[\"deal_id\"]).first()\n",
    "deal_tmu_ebitda_diff  = diff_tmu_ebitda.groupby(df[\"deal_id\"]).first()\n",
    "\n",
    "df = df.merge(\n",
    "    pd.DataFrame({\n",
    "        \"deal_id\": deal_tm_diff.index,\n",
    "        \"qa_tm_diff_deal\": deal_tm_diff.values,\n",
    "        \"qa_tmu_diff_deal\": deal_tmu_diff.values,\n",
    "        \"qa_tmu_ebitda_sub_diff_deal\": deal_tmu_ebitda_diff.values,\n",
    "    }),\n",
    "    on=\"deal_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# ---- persist ----\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# ---- summaries (deal-level) ----\n",
    "def summarize(name, mask):\n",
    "    # mask is a Series indexed by deal_id\n",
    "    ok = int(mask.sum())\n",
    "    out = int((~mask).sum())\n",
    "    print(f\"{name}: deals within Â±1% = {ok} | outside Â±1% = {out}\")\n",
    "\n",
    "summarize(\"TM (levered) identity\", ok_tm_deal)\n",
    "summarize(\"TM_unlevered (sum of tmu_contrib main effects)\", ok_tmu_deal)\n",
    "summarize(\"EBITDA sub-breakdown on TM_unlevered scale\", ok_tmu_ebitda_deal)\n",
    "\n",
    "print(\"tmu_contrib_ columns written for:\", abs_main + abs_sub)\n"
   ],
   "id": "4f1a352374dc41e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deals in QA scope: 356\n",
      "TM (levered) identity: deals within Â±1% = 356 | outside Â±1% = 0\n",
      "TM_unlevered (sum of tmu_contrib main effects): deals within Â±1% = 356 | outside Â±1% = 0\n",
      "EBITDA sub-breakdown on TM_unlevered scale: deals within Â±1% = 356 | outside Â±1% = 0\n",
      "tmu_contrib_ columns written for: ['multiple_effect', 'ebitda_effect', 'multiple_ebitda_combination_effect', 'fcf_effect', 'sales_effect', 'margin_effect', 'sales_margin_combination_effect']\n"
     ]
    }
   ],
   "execution_count": 141
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
