{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-09T16:13:18.314215Z",
     "start_time": "2025-10-09T16:13:18.026037Z"
    }
   },
   "source": [
    "# Value Creation Bridge aligned to Achleitner et al. 2010; Achleitner, Braun & Puche 2015; Söffge & Braun 2017)\n",
    "\n",
    "# === Core Value-Bridge Items: times_money and tm_unlevered (exit-row div/cap injection) ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helper ----\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ---- load + checks ----\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# Order within deal: 1 = entry (earliest), 2 = exit (latest)\n",
    "df[\"_ref_dt\"] = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_rank\"]   = df.groupby(\"deal_id\")[\"_ref_dt\"].rank(method=\"first\", ascending=True)\n",
    "\n",
    "eq  = num(df[\"equity\"])\n",
    "\n",
    "# Entry / Exit equity\n",
    "equity_entry = (df.loc[df[\"_rank\"] == 1, [\"deal_id\"]]\n",
    "                  .assign(equity_entry=eq[df[\"_rank\"] == 1].values)\n",
    "                  .groupby(\"deal_id\")[\"equity_entry\"].first())\n",
    "equity_exit  = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "                  .assign(equity_exit=eq[df[\"_rank\"] == 2].values)\n",
    "                  .groupby(\"deal_id\")[\"equity_exit\"].first())\n",
    "\n",
    "# ---- deal-level inputs (exit-row dividends & capital injections) ----\n",
    "div_exit = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "              .assign(div_exit=num(df[\"dividends\"])[df[\"_rank\"] == 2].values)\n",
    "              .groupby(\"deal_id\")[\"div_exit\"].first())\n",
    "\n",
    "cap_exit = (df.loc[df[\"_rank\"] == 2, [\"deal_id\"]]\n",
    "              .assign(cap_exit=num(df[\"capital_injections\"])[df[\"_rank\"] == 2].values)\n",
    "              .groupby(\"deal_id\")[\"cap_exit\"].first())\n",
    "\n",
    "div_exit = div_exit.fillna(0.0)\n",
    "cap_exit = cap_exit.fillna(0.0)\n",
    "\n",
    "# ---- dtype & presence guards (drop-in right after div_exit/cap_exit fillna) ----\n",
    "# Ensure numeric dtype and no missing equity at entry/exit (these are the two rows you rely on)\n",
    "assert equity_entry.notna().all(), \"Equity missing on entry rows.\"\n",
    "assert equity_exit.notna().all(),  \"Equity missing on exit rows.\"\n",
    "\n",
    "# Make sure dividends / cap injections are numeric after fill\n",
    "div_exit  = pd.to_numeric(div_exit, errors=\"coerce\").fillna(0.0)\n",
    "cap_exit  = pd.to_numeric(cap_exit, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "\n",
    "# ---- average D/E as entry/exit two-point mean; clip at 0 for unlevering ----\n",
    "der_series = num(df[\"de_ratio\"])\n",
    "der_entry  = df.loc[df[\"_rank\"] == 1, [\"deal_id\"]].assign(der=der_series[df[\"_rank\"] == 1].values) \\\n",
    "                 .groupby(\"deal_id\")[\"der\"].first()\n",
    "der_exit   = df.loc[df[\"_rank\"] == 2, [\"deal_id\"]].assign(der=der_series[df[\"_rank\"] == 2].values) \\\n",
    "                 .groupby(\"deal_id\")[\"der\"].first()\n",
    "der_avg    = pd.concat([der_entry, der_exit], axis=1).mean(axis=1).clip(lower=0.0)\n",
    "\n",
    "# ---- cost of debt: take the (deal-level) compounded total, not a mean ----\n",
    "cod_spread = df.groupby(\"deal_id\")[\"cost_of_debt\"].agg(lambda s: float(s.max() - s.min()))\n",
    "assert (cod_spread.abs() < 1e-12).all(), \"cost_of_debt differs across rows; confirm intent.\"\n",
    "\n",
    "cod_avg = df.groupby(\"deal_id\")[\"cost_of_debt\"].first()\n",
    "\n",
    "# ---- formulas (deal level) ----\n",
    "d_equity = equity_exit - equity_entry\n",
    "net_capital_gain = d_equity + div_exit + cap_exit\n",
    "invested_capital = equity_entry - cap_exit\n",
    "\n",
    "# ---- invested capital guard: require strictly positive IC ----\n",
    "valid_ic = invested_capital > 0\n",
    "if not valid_ic.all():\n",
    "    bad = (~valid_ic).sum()\n",
    "    print(f\"Dropping {bad} deal(s) with invested_capital <= 0.\")\n",
    "    # Hard drop of invalid deals from all deal-level vectors:\n",
    "    equity_entry      = equity_entry[valid_ic]\n",
    "    equity_exit       = equity_exit[valid_ic]\n",
    "    div_exit          = div_exit[valid_ic]\n",
    "    cap_exit          = cap_exit[valid_ic]\n",
    "    d_equity          = d_equity[valid_ic]\n",
    "    net_capital_gain  = net_capital_gain[valid_ic]\n",
    "    invested_capital  = invested_capital[valid_ic]\n",
    "    der_avg           = der_avg[valid_ic]\n",
    "    cod_avg           = cod_avg[valid_ic]\n",
    "\n",
    "# Remove invalid deals from the row-level frame as well\n",
    "survivor_deals = set(invested_capital.index)  # index currently is deal_id for valid deals\n",
    "df = df[df[\"deal_id\"].isin(survivor_deals)].copy()\n",
    "\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    times_money = (net_capital_gain / invested_capital).where(invested_capital != 0)\n",
    "\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    tm_unlevered = (times_money + cod_avg * der_avg) / (1 + der_avg)\n",
    "\n",
    "leverage_effect = times_money - tm_unlevered\n",
    "\n",
    "# ---- broadcast to both rows ----\n",
    "deal_metrics = pd.DataFrame({\n",
    "    \"deal_id\": d_equity.index,\n",
    "    \"d_equity\": d_equity.values,\n",
    "    \"net_capital_gain\": net_capital_gain.values,\n",
    "    \"invested_capital\": invested_capital.values,\n",
    "    \"times_money\": times_money.values,\n",
    "    \"tm_unlevered\": tm_unlevered.values,\n",
    "    \"leverage_effect\": leverage_effect.values,\n",
    "}).set_index(\"deal_id\")\n",
    "\n",
    "out = df.drop(columns=[\"_ref_dt\",\"_rank\"]).merge(\n",
    "    deal_metrics, left_on=\"deal_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# ---- persist + checks ----\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "chk = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "# exactly 2 rows per surviving deal\n",
    "assert chk.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Row cardinality changed.\"\n",
    "# no NaNs in core outputs\n",
    "req = [\"d_equity\",\"net_capital_gain\",\"invested_capital\",\"times_money\",\"tm_unlevered\",\"leverage_effect\"]\n",
    "assert set(req).issubset(chk.columns), \"Missing expected columns.\"\n",
    "assert chk[req].notna().all().all(), \"NaNs in deal-level outputs after merge.\"\n",
    "\n",
    "print(\"Deal-level metrics added (exit-row div/cap applied):\",\n",
    "      [\"d_equity\",\"net_capital_gain\",\"invested_capital\",\"times_money\",\"tm_unlevered\",\"leverage_effect\"])\n",
    "\n",
    "# === Optional post-filter: drop short-hold deals with |TM| ~ 0 ===\n",
    "APPLY_TM_SHORT_HOLD_FILTER = True   # toggle\n",
    "TM_ABS_THRESHOLD = 0.10             # remove if |TM| < this AND holding period ≤ threshold\n",
    "HOLD_DAYS_THRESHOLD = 365\n",
    "\n",
    "if APPLY_TM_SHORT_HOLD_FILTER:\n",
    "    # Reload from disk to keep this independent of the calculation above\n",
    "    chk = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "    # Parse dates and compute per-deal holding period in days\n",
    "    ref_dt = pd.to_datetime(chk[\"reference_date\"], errors=\"coerce\")\n",
    "    g = chk.assign(_ref_dt=ref_dt).groupby(\"deal_id\", as_index=True)\n",
    "\n",
    "    # Per-deal metrics (times_money should be identical across the two rows)\n",
    "    tm_per_deal = pd.to_numeric(g[\"times_money\"].first(), errors=\"coerce\")\n",
    "    hold_days   = (g[\"_ref_dt\"].max() - g[\"_ref_dt\"].min()).dt.days\n",
    "\n",
    "    # Identify deals to drop: |TM| < threshold AND holding period ≤ threshold\n",
    "    tiny_tm = tm_per_deal.abs() < TM_ABS_THRESHOLD\n",
    "    short_hold = hold_days <= HOLD_DAYS_THRESHOLD\n",
    "    to_drop_mask = tiny_tm & short_hold\n",
    "    drop_ids = set(tm_per_deal.index[to_drop_mask])\n",
    "\n",
    "    if drop_ids:\n",
    "        before = int(chk[\"deal_id\"].nunique())\n",
    "        cleaned = chk[~chk[\"deal_id\"].isin(drop_ids)].copy()\n",
    "        cleaned.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "        # Post-write integrity checks\n",
    "        chk2 = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "        assert chk2.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Row cardinality changed improperly.\"\n",
    "        req = [\"d_equity\",\"net_capital_gain\",\"invested_capital\",\"times_money\",\"tm_unlevered\",\"leverage_effect\"]\n",
    "        assert set(req).issubset(chk2.columns), \"Missing expected columns after filter.\"\n",
    "        assert chk2[req].notna().all().all(), \"NaNs in deal-level outputs after filter.\"\n",
    "\n",
    "        after = int(chk2[\"deal_id\"].nunique())\n",
    "        print(f\"TM short-hold filter applied: removed {before - after} deal(s) \"\n",
    "              f\"(abs(TM) < {TM_ABS_THRESHOLD} and hold ≤ {HOLD_DAYS_THRESHOLD} days).\")\n",
    "    else:\n",
    "        print(\"TM short-hold filter applied: no deals met removal criteria.\")\n",
    "else:\n",
    "    print(\"TM short-hold filter skipped (toggle off).\")\n",
    "\n",
    "g = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "by_status = (g.drop_duplicates([\"deal_id\",\"holding_status\"])\n",
    "               .groupby(\"holding_status\")[\"deal_id\"].nunique())\n",
    "print(\n",
    "    \"unique_deals_exited:\",   int(by_status.get(\"exited\", 0)),\n",
    "    \"unique_deals_unexited:\", int(by_status.get(\"unexited\", 0)),\n",
    "    \"unique_deals_total:\",    g[\"deal_id\"].nunique()\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal-level metrics added (exit-row div/cap applied): ['d_equity', 'net_capital_gain', 'invested_capital', 'times_money', 'tm_unlevered', 'leverage_effect']\n",
      "TM short-hold filter applied: removed 9 deal(s) (abs(TM) < 0.1 and hold ≤ 365 days).\n",
      "unique_deals_exited: 210 unique_deals_unexited: 135 unique_deals_total: 345\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T16:13:18.377232Z",
     "start_time": "2025-10-09T16:13:18.326014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Absolute Value Drivers: Multiple, FCF, EBITDA (PBA-aligned) ===\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -- helper --\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- CWD: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# -- load + guarantees --\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "# -- order within deal: 1 = entry (earliest), 2 = exit/latest --\n",
    "df[\"_ref_dt\"] = pd.to_datetime(df[\"reference_date\"], errors=\"coerce\")\n",
    "df[\"_rank\"] = df.groupby(\"deal_id\")[\"_ref_dt\"].rank(method=\"first\", ascending=True)\n",
    "\n",
    "def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "xe  = num(df[\"xebitda\"])\n",
    "eb  = num(df[\"ebitda\"])\n",
    "rv  = num(df[\"revenue\"])\n",
    "mg  = num(df[\"ebitda_margin\"])\n",
    "nd  = num(df[\"net_debt\"])\n",
    "div = num(df[\"dividends\"])\n",
    "cap = num(df[\"capital_injections\"])\n",
    "\n",
    "# -- pick entry / exit values per deal --\n",
    "def pick(series, rank_val, name):\n",
    "    s = series[df[\"_rank\"] == rank_val]\n",
    "    out = df.loc[df[\"_rank\"] == rank_val, [\"deal_id\"]].assign(**{name: s.values})\n",
    "    return out.groupby(\"deal_id\")[name].first()\n",
    "\n",
    "xebitda_entry  = pick(xe, 1, \"xebitda_entry\")\n",
    "xebitda_exit   = pick(xe, 2, \"xebitda_exit\")\n",
    "ebitda_entry   = pick(eb, 1, \"ebitda_entry\")\n",
    "ebitda_exit    = pick(eb, 2, \"ebitda_exit\")\n",
    "revenue_entry  = pick(rv, 1, \"revenue_entry\")\n",
    "revenue_exit   = pick(rv, 2, \"revenue_exit\")\n",
    "margin_entry   = pick(mg, 1, \"margin_entry\")\n",
    "margin_exit    = pick(mg, 2, \"margin_exit\")\n",
    "net_debt_entry = pick(nd, 1, \"net_debt_entry\")\n",
    "net_debt_exit  = pick(nd, 2, \"net_debt_exit\")\n",
    "div_exit       = pick(div, 2, \"div_exit\")\n",
    "cap_exit       = pick(cap, 2, \"cap_exit\")\n",
    "\n",
    "# -- driver sanity and required-data mask (drop undefined cores) --\n",
    "xebitda_entry  = xebitda_entry.where(np.isfinite(xebitda_entry))\n",
    "xebitda_exit   = xebitda_exit.where(np.isfinite(xebitda_exit))\n",
    "margin_entry   = margin_entry.where(np.isfinite(margin_entry))\n",
    "\n",
    "need = pd.Series(True, index=xebitda_entry.index)\n",
    "need &= xebitda_entry.notna()\n",
    "need &= xebitda_exit.notna()\n",
    "need &= margin_entry.notna()\n",
    "need &= ebitda_entry.notna() & ebitda_exit.notna()\n",
    "need &= revenue_entry.notna() & revenue_exit.notna()\n",
    "need &= net_debt_entry.notna() & net_debt_exit.notna()\n",
    "need &= div_exit.notna() & cap_exit.notna()\n",
    "\n",
    "if not need.all():\n",
    "    dropped = int((~need).sum())\n",
    "    print(f\"Dropping {dropped} deal(s) due to undefined core drivers.\")\n",
    "    xebitda_entry  = xebitda_entry[need]\n",
    "    xebitda_exit   = xebitda_exit[need]\n",
    "    ebitda_entry   = ebitda_entry[need]\n",
    "    ebitda_exit    = ebitda_exit[need]\n",
    "    revenue_entry  = revenue_entry[need]\n",
    "    revenue_exit   = revenue_exit[need]\n",
    "    margin_entry   = margin_entry[need]\n",
    "    margin_exit    = margin_exit[need]\n",
    "    net_debt_entry = net_debt_entry[need]\n",
    "    net_debt_exit  = net_debt_exit[need]\n",
    "    div_exit       = div_exit[need]\n",
    "    cap_exit       = cap_exit[need]\n",
    "    survivor_deals = set(need[need].index)\n",
    "    df = df[df[\"deal_id\"].isin(survivor_deals)].copy()\n",
    "\n",
    "# -- deltas (exit - entry); deleveraging defined as entry − exit (positive if debt reduced) --\n",
    "d_multiple = xebitda_exit - xebitda_entry\n",
    "d_ebitda   = ebitda_exit  - ebitda_entry\n",
    "d_revenue  = revenue_exit - revenue_entry\n",
    "d_margin   = margin_exit  - margin_entry\n",
    "d_debt     = net_debt_entry - net_debt_exit\n",
    "\n",
    "# -- effects (PBA absolute) --\n",
    "multiple_effect = d_multiple * ebitda_entry\n",
    "fcf_effect = d_debt + div_exit + cap_exit\n",
    "multiple_ebitda_combination_effect = d_multiple * d_ebitda\n",
    "ebitda_effect = d_ebitda * xebitda_entry\n",
    "sales_effect = d_revenue * margin_entry * xebitda_entry\n",
    "margin_effect = d_margin * revenue_entry * xebitda_entry\n",
    "sales_margin_combination_effect = d_revenue * d_margin * xebitda_entry\n",
    "\n",
    "# -- reconciliation to equity bridge (diagnostic residual) --\n",
    "ev_entry = xebitda_entry * ebitda_entry\n",
    "ev_exit  = xebitda_exit  * ebitda_exit\n",
    "delta_ev = ev_exit - ev_entry\n",
    "\n",
    "net_cap_gain_implied = delta_ev + (net_debt_entry - net_debt_exit) + div_exit + cap_exit\n",
    "sum_effects = multiple_effect + ebitda_effect + multiple_ebitda_combination_effect + fcf_effect\n",
    "effects_residual = sum_effects - net_cap_gain_implied\n",
    "effects_residual_rel = effects_residual / net_cap_gain_implied.replace(0, np.nan).abs()\n",
    "\n",
    "# -- bundle per-deal results and broadcast to both rows --\n",
    "deal_effects = pd.DataFrame({\n",
    "    \"deal_id\": d_multiple.index,\n",
    "    \"multiple_effect\": multiple_effect.values,\n",
    "    \"fcf_effect\": fcf_effect.values,\n",
    "    \"multiple_ebitda_combination_effect\": multiple_ebitda_combination_effect.values,\n",
    "    \"ebitda_effect\": ebitda_effect.values,\n",
    "    \"sales_effect\": sales_effect.values,\n",
    "    \"margin_effect\": margin_effect.values,\n",
    "    \"sales_margin_combination_effect\": sales_margin_combination_effect.values,\n",
    "    \"effects_residual\": effects_residual.values,\n",
    "    \"effects_residual_rel\": effects_residual_rel.fillna(0.0).values,\n",
    "}).set_index(\"deal_id\")\n",
    "\n",
    "out = df.drop(columns=[\"_ref_dt\", \"_rank\"]).merge(\n",
    "    deal_effects, left_on=\"deal_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# -- persist + checks --\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "chk = pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})\n",
    "assert chk.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Row cardinality changed.\"\n",
    "req = [\n",
    "    \"multiple_effect\",\"fcf_effect\",\"multiple_ebitda_combination_effect\",\n",
    "    \"ebitda_effect\",\"sales_effect\",\"margin_effect\",\"sales_margin_combination_effect\",\n",
    "    \"effects_residual\",\"effects_residual_rel\"\n",
    "]\n",
    "assert set(req).issubset(chk.columns), \"Missing effect columns.\"\n",
    "assert chk[req].notna().all().all(), \"NaNs in effects after merge.\"\n",
    "\n",
    "bad_rec = (chk.groupby(\"deal_id\")[\"effects_residual\"].first().abs() > 1e-6) & \\\n",
    "          (chk.groupby(\"deal_id\")[\"effects_residual_rel\"].first().abs() > 1e-6)\n",
    "if bad_rec.any():\n",
    "    print(f\"Warning: {int(bad_rec.sum())} deal(s) fail effect reconciliation (abs/rel tolerance).\")\n",
    "\n",
    "print(\"Deal-level effects added:\",\n",
    "      [\"multiple_effect\",\"fcf_effect\",\"multiple_ebitda_combination_effect\",\n",
    "       \"ebitda_effect\",\"sales_effect\",\"margin_effect\",\"sales_margin_combination_effect\",\n",
    "       \"effects_residual\",\"effects_residual_rel\"])\n"
   ],
   "id": "22539dc9d5c9bd1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal-level effects added: ['multiple_effect', 'fcf_effect', 'multiple_ebitda_combination_effect', 'ebitda_effect', 'sales_effect', 'margin_effect', 'sales_margin_combination_effect', 'effects_residual', 'effects_residual_rel']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T16:13:18.444787Z",
     "start_time": "2025-10-09T16:13:18.388582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === TMU-scaled contributions + pct_of_tm_* columns ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        p = parent / rel_path\n",
    "        if p.exists():\n",
    "            return p.resolve()\n",
    "    raise FileNotFoundError(f\"Couldn't locate {rel_path} from {Path.cwd()}\")\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# ---- load + guarantees ----\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert df.groupby(\"deal_id\")[\"id\"].nunique().eq(2).all(), \"Each deal_id must have exactly 2 rows.\"\n",
    "\n",
    "num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# Absolute effects (currency)\n",
    "abs_main = [\"multiple_effect\",\"fcf_effect\",\"multiple_ebitda_combination_effect\",\"ebitda_effect\"]\n",
    "abs_sub  = [\"sales_effect\",\"margin_effect\",\"sales_margin_combination_effect\"]\n",
    "all_abs  = abs_main + abs_sub\n",
    "\n",
    "IC  = num(df[\"invested_capital\"])\n",
    "TM  = num(df[\"times_money\"])\n",
    "TMU = num(df[\"tm_unlevered\"])\n",
    "\n",
    "# Remove any legacy tm_contrib_* columns if present\n",
    "tm_cols_legacy = [c for c in df.columns if c.startswith(\"tm_contrib_\")]\n",
    "if tm_cols_legacy:\n",
    "    df = df.drop(columns=tm_cols_legacy)\n",
    "\n",
    "# ---- (1) Compute levered contributions (in memory only) ----\n",
    "levered_contrib = {}\n",
    "for c in all_abs:\n",
    "    levered_contrib[c] = num(df[c]) / IC\n",
    "\n",
    "# ---- (2) Proportional rescale to TMU ----\n",
    "eps = 1e-12\n",
    "valid_tm = TM.abs() > eps\n",
    "scale = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "scale[valid_tm] = (TMU / TM)[valid_tm]\n",
    "\n",
    "for c in all_abs:\n",
    "    df[f\"tmu_contrib_{c}\"] = levered_contrib[c] * scale\n",
    "\n",
    "# Optional: if TM≈0 and TMU≈0, set contributions to 0\n",
    "zero_tm_but_zero_tmu = (~valid_tm) & (TMU.abs() <= eps)\n",
    "if zero_tm_but_zero_tmu.any():\n",
    "    for c in all_abs:\n",
    "        df.loc[zero_tm_but_zero_tmu, f\"tmu_contrib_{c}\"] = 0.0\n",
    "\n",
    "# ---- (3) pct_of_tm_* = (tmu_contrib_* / TM) * 100; guard |TM|~0 ----\n",
    "for c in all_abs:\n",
    "    pct_col = f\"pct_of_tm_{c}\"\n",
    "    tmu_col = f\"tmu_contrib_{c}\"\n",
    "    df[pct_col] = np.where(valid_tm, (df[tmu_col] / TM), np.nan)\n",
    "\n",
    "# ---- (4) Reorder: place each pct_of_tm_* immediately after its tmu_contrib_* ----\n",
    "# Build new order while preserving other columns' relative order\n",
    "cols = list(df.columns)\n",
    "new_cols = []\n",
    "seen = set()\n",
    "\n",
    "for col in cols:\n",
    "    if col.startswith(\"tmu_contrib_\"):\n",
    "        base = col.removeprefix(\"tmu_contrib_\")\n",
    "        pct_col = f\"pct_of_tm_{base}\"\n",
    "        new_cols.append(col); seen.add(col)\n",
    "        if pct_col in df.columns:\n",
    "            new_cols.append(pct_col); seen.add(pct_col)\n",
    "    elif col not in seen:\n",
    "        new_cols.append(col); seen.add(col)\n",
    "\n",
    "df = df[new_cols]\n",
    "\n",
    "# ---- (5) QA identities (recompute levered/TMU sums; nothing written from these intermediates) ----\n",
    "tm_from_contribs = pd.DataFrame({c: levered_contrib[c] for c in abs_main}).sum(axis=1, min_count=1)\n",
    "diff_tm  = TM  - tm_from_contribs\n",
    "tol_tm   = np.maximum(1e-9, 0.01 * TM.abs())\n",
    "ok_tm_row  = (diff_tm.abs() <= tol_tm) & np.isfinite(diff_tm)\n",
    "\n",
    "tmu_from_contribs = df[[f\"tmu_contrib_{c}\" for c in abs_main]].sum(axis=1, min_count=1)\n",
    "diff_tmu = TMU - tmu_from_contribs\n",
    "tol_tmu  = np.maximum(1e-9, 0.01 * TMU.abs())\n",
    "ok_tmu_row  = (diff_tmu.abs() <= tol_tmu) & np.isfinite(diff_tmu)\n",
    "\n",
    "tmu_contrib_ebitda = df[\"tmu_contrib_ebitda_effect\"]\n",
    "tmu_contrib_subsum = df[[\n",
    "    \"tmu_contrib_sales_effect\",\"tmu_contrib_margin_effect\",\"tmu_contrib_sales_margin_combination_effect\"\n",
    "]].sum(axis=1, min_count=1)\n",
    "diff_tmu_ebitda = tmu_contrib_ebitda - tmu_contrib_subsum\n",
    "tol_tmu_ebitda  = np.maximum(1e-9, 0.01 * tmu_contrib_ebitda.abs())\n",
    "ok_tmu_ebitda_row = (diff_tmu_ebitda.abs() <= tol_tmu_ebitda) & np.isfinite(diff_tmu_ebitda)\n",
    "\n",
    "ok_tm_deal         = ok_tm_row.groupby(df[\"deal_id\"]).all()\n",
    "ok_tmu_deal        = ok_tmu_row.groupby(df[\"deal_id\"]).all()\n",
    "ok_tmu_ebitda_deal = ok_tmu_ebitda_row.groupby(df[\"deal_id\"]).all()\n",
    "\n",
    "# Persist QA diffs for diagnostics\n",
    "df[\"qa_tm_diff\"] = diff_tm\n",
    "df[\"qa_tmu_diff\"] = diff_tmu\n",
    "df[\"qa_tmu_ebitda_sub_diff\"] = diff_tmu_ebitda\n",
    "\n",
    "# ---- persist ----\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "\n",
    "# ---- summaries ----\n",
    "def summarize(name, mask):\n",
    "    print(f\"{name}: deals within ±1% = {int(mask.sum())} | outside ±1% = {int((~mask).sum())}\")\n",
    "\n",
    "summarize(\"TM (levered) identity\", ok_tm_deal)\n",
    "summarize(\"TM_unlevered (sum of tmu_contrib main effects)\", ok_tmu_deal)\n",
    "summarize(\"EBITDA sub-breakdown on TMU scale\", ok_tmu_ebitda_deal)\n",
    "\n",
    "print(\"Wrote tmu_contrib_* and pct_of_tm_* (percent of levered TM) columns.\")\n"
   ],
   "id": "4f1a352374dc41e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TM (levered) identity: deals within ±1% = 344 | outside ±1% = 1\n",
      "TM_unlevered (sum of tmu_contrib main effects): deals within ±1% = 344 | outside ±1% = 1\n",
      "EBITDA sub-breakdown on TMU scale: deals within ±1% = 345 | outside ±1% = 0\n",
      "Wrote tmu_contrib_* and pct_of_tm_* (percent of levered TM) columns.\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
