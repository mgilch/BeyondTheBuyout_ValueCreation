{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-08T21:02:52.258148Z",
     "start_time": "2025-10-08T21:02:47.989130Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# ---------- Helpers ---------- #\n",
    "REL_PATH = Path(\"InputData/CoreData.xlsx\")\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    \"\"\"\n",
    "    Starting at cwd, walk up to `max_up` parents to find `rel_path`.\n",
    "    Returns the resolved path if found; raises FileNotFoundError otherwise.\n",
    "    \"\"\"\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "# ---------- Step 1: Read Excel / basic checks ---------- #\n",
    "INPUT_XLSX = find_upwards(REL_PATH)\n",
    "xfile = pd.ExcelFile(INPUT_XLSX)\n",
    "sheets = xfile.sheet_names\n",
    "print(\"Resolved path:\", INPUT_XLSX)\n",
    "print(\"Sheets:\", sheets)\n",
    "\n",
    "assert isinstance(sheets, list), \"Expected a list\"\n",
    "assert sheets and all(isinstance(s, str) and s.strip() for s in sheets), \"Sheet names must be non-empty strings\"\n",
    "assert len(sheets) == len(set(sheets)), \"Duplicate sheet names detected\"\n",
    "print(\"Check 1 passed.\")\n",
    "\n",
    "# Quick read of key sheet + preview\n",
    "SHEET = \"deal_time_series\"\n",
    "assert SHEET in sheets, f\"'{SHEET}' not found. Available sheets: {sheets}\"\n",
    "dts = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, header=0)\n",
    "print(\"Column names:\", list(dts.columns))\n",
    "display(dts.head(5))\n",
    "assert isinstance(dts, pd.DataFrame), \"Expected a pandas DataFrame.\"\n",
    "assert not dts.empty, \"Sheet loaded but contains no data.\"\n",
    "assert all(isinstance(c, str) and c.strip() for c in dts.columns), \"Invalid/empty column names.\"\n",
    "print(f\"Check 2 passed. Shape: {dts.shape}. Showing 5 data rows above.\")\n",
    "\n",
    "# ---------- Step 2: Init working.csv with id ---------- #\n",
    "TARGET_DIR = (find_upwards(Path(\"ValueCreation\")) / \"Data\")\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TARGET_CSV = TARGET_DIR / \"working.csv\"\n",
    "\n",
    "assert SHEET in sheets, f\"Sheet '{SHEET}' not found.\"\n",
    "usecols = [\"id\"]\n",
    "raw = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, usecols=usecols)\n",
    "df = raw[[\"id\"]]\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Wrote {len(df):,} rows to {TARGET_CSV}\")\n",
    "\n",
    "# Post-write checks\n",
    "assert TARGET_CSV.exists(), f\"Missing output: {TARGET_CSV}\"\n",
    "check_df = pd.read_csv(TARGET_CSV)\n",
    "assert list(check_df.columns) == [\"id\"], list(check_df.columns)\n",
    "assert len(check_df) == len(raw), f\"Row count changed: raw={len(raw)} vs written={len(check_df)}\"\n",
    "assert check_df[\"id\"].tolist() == raw[\"id\"].tolist(), \"Row order changed.\"\n",
    "assert check_df[\"id\"].notna().all(), \"Null id found.\"\n",
    "assert not check_df[\"id\"].duplicated().any(), \"Duplicate id values found.\"\n",
    "print(\"INIT check passed. Shape:\", check_df.shape)\n",
    "\n",
    "# ---------- Step 3: Add columns from deal_time_series ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "requested = [\n",
    "    \"deal_id\", \"reference_date\", \"enterprise_value\", \"net_debt\", \"equity\",\n",
    "    \"reporting_currency_financials\", \"reference_period_type_prefix\",\n",
    "    \"reference_period_type_suffix\", \"revenue\", \"ebitda\",\n",
    "    \"ownership_economic_percentage\", \"data_room_name\",\n",
    "]\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=SHEET,\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Normalize reference_date to yyyy-mm-dd (handles Excel serials and strings)\n",
    "if \"reference_date\" in src.columns:\n",
    "    s = src[\"reference_date\"]\n",
    "    if np.issubdtype(s.dtype, np.number):\n",
    "        dt = pd.to_datetime(s, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    else:\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    src[\"reference_date\"] = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]]\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns: {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "missing = [c for c in requested if c not in after.columns]\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "_ = pd.to_datetime(after[\"reference_date\"], errors=\"coerce\")\n",
    "print(\"ADD_COLUMNS (deal_time_series extra fields) check passed. Shape:\", after.shape)\n",
    "print(\"unique_deals:\", pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n",
    "\n",
    "# ---------- Step 4: Add columns from deal ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "requested = [\n",
    "    \"fund_id\", \"name\", \"entry_date\", \"entry_transaction_type\",\n",
    "    \"sourcing_type\", \"investment_role\", \"exit_date\", \"exit_transaction_type\",\n",
    "]\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"deal\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]].rename(columns={\"id\": \"deal_id\"})\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"deal_id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'deal': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "missing = [c for c in to_add if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "print(\"ADD_COLUMNS (deal) check passed. Shape:\", after.shape)\n",
    "print(\"unique_deals:\", pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n",
    "\n",
    "# ---------- Step 5: Add columns from fund ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str, \"fund_id\": str})\n",
    "\n",
    "requested = [\"name\", \"vintage_year\", \"investment_theme\", \"size\", \"fund_generation\", \"fund_family_generation\"]\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"fund\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "src = src.rename(columns={\"id\": \"fund_id\", \"name\": \"fund_name\"})\n",
    "to_add = [c if c != \"name\" else \"fund_name\" for c in requested]\n",
    "to_add = [c for c in to_add if c not in working.columns]\n",
    "src = src[[\"fund_id\", *to_add]]\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"fund_id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'fund': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str, \"fund_id\": str})\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "expected = [c if c != \"name\" else \"fund_name\" for c in [\"name\",\"vintage_year\",\"investment_theme\",\"size\",\"fund_generation\",\"fund_family_generation\"]]\n",
    "missing = [c for c in expected if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "print(\"ADD_COLUMNS (fund) check passed. Shape:\", after.shape)\n",
    "print(\"unique_deals:\", pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved path: /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/InputData/CoreData.xlsx\n",
      "Sheets: ['Metadata', 'dashboard', 'general_partner', 'fund', 'fund_cash_flow', 'capital_account', 'deal', 'deal_time_series', 'deal_cash_flow', 'deal_partner', 'deal_acquirer', 'deal_vendor', 'organization', 'person']\n",
      "Check 1 passed.\n",
      "Column names: ['id', 'total_value', 'ebitda', 'reference_period_type_suffix', 'moic_gross', 'data_room_id', 'created_by_user_id', 'recurring_revenue', 'bridge_financing', 'reporting_currency_financials', 'irr_net', 'reference_period_type_prefix', 'moic_net', 'data_room_name', 'realized_value', 'irr_gross', 'ebitda_adjusted', 'net_debt', 'ebitda_multiple', 'enterprise_value_valuation_rationale', 'is_main', 'equity', 'reporting_currency_valuation', 'management_equity_percentage', 'revenue_multiple', 'recurring_revenue_percentage', 'quarterly_company_update', '_created_at_utc', 'enterprise_value', 'enterprise_value_valuation_multiple', '_year', 'reference_date', 'irr_net_unlevered', 'capex', 'total_investment_cost', 'deal_revision_id', 'reported_date', 'deal_id', 'unrealized_value', 'predicted_sentiment', 'ebit', 'ebitda_adjusted_note', 'ebitda_margin', 'cumulative_addons', 'moic_net_unlevered', 'revenue', 'fund_equity_invested', '_quarter', '_revision_id', 'ownership_economic_percentage', 'enterprise_value_valuation_amount']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                     id  total_value      ebitda  \\\n",
       "0                               111dts1          NaN        10.0   \n",
       "1                               111dts2          NaN        15.0   \n",
       "2  b0cd7032-72f6-46d0-ae21-7a0ca81297eb          NaN         NaN   \n",
       "3  997bb98e-9ab1-47be-b04b-767d225f60a9          NaN   7600000.0   \n",
       "4  1eab0e13-6d1d-4c99-a8a0-6c32b56de012          NaN  34000000.0   \n",
       "\n",
       "  reference_period_type_suffix  moic_gross  \\\n",
       "0                          NaN         NaN   \n",
       "1                          NaN         NaN   \n",
       "2                       Actual         NaN   \n",
       "3                       Actual         NaN   \n",
       "4                       Actual         NaN   \n",
       "\n",
       "                           data_room_id                    created_by_user_id  \\\n",
       "0                                   NaN                                   NaN   \n",
       "1                                   NaN                                   NaN   \n",
       "2  203ffba5-3ebb-454a-844c-87cee656bd95  25bd1583-7869-465c-9dc4-664685cd3a6c   \n",
       "3  d40592d4-9127-4e77-a8c7-9da4755a6105  25bd1583-7869-465c-9dc4-664685cd3a6c   \n",
       "4  d40592d4-9127-4e77-a8c7-9da4755a6105  25bd1583-7869-465c-9dc4-664685cd3a6c   \n",
       "\n",
       "   recurring_revenue  bridge_financing reporting_currency_financials  ...  \\\n",
       "0                NaN               NaN                           USD  ...   \n",
       "1                NaN               NaN                           USD  ...   \n",
       "2                NaN               NaN                           USD  ...   \n",
       "3                NaN               NaN                           EUR  ...   \n",
       "4                NaN               NaN                           EUR  ...   \n",
       "\n",
       "   ebitda_adjusted_note ebitda_margin  cumulative_addons moic_net_unlevered  \\\n",
       "0                   NaN           NaN                NaN                NaN   \n",
       "1                   NaN           NaN                NaN                NaN   \n",
       "2                   NaN           NaN                NaN                NaN   \n",
       "3                   NaN         0.046                NaN                NaN   \n",
       "4                   NaN         0.121                NaN                NaN   \n",
       "\n",
       "       revenue  fund_equity_invested  _quarter  \\\n",
       "0        100.0                   NaN       NaN   \n",
       "1        120.0                   NaN       NaN   \n",
       "2          NaN                   NaN       1.0   \n",
       "3  164000000.0                   NaN       1.0   \n",
       "4  280000000.0                   NaN       2.0   \n",
       "\n",
       "                           _revision_id  ownership_economic_percentage  \\\n",
       "0                                   NaN                            1.0   \n",
       "1                                   NaN                            1.0   \n",
       "2  cccb3423-eb71-452a-92f1-3b4a64100646                            NaN   \n",
       "3  64ef422a-f3cc-44fe-bf2b-fe5955950008                            NaN   \n",
       "4  3e0b9962-f308-420f-9c62-5fa24f5b2e7e                            NaN   \n",
       "\n",
       "  enterprise_value_valuation_amount  \n",
       "0                               NaN  \n",
       "1                               NaN  \n",
       "2                               NaN  \n",
       "3                               NaN  \n",
       "4                               NaN  \n",
       "\n",
       "[5 rows x 51 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>total_value</th>\n",
       "      <th>ebitda</th>\n",
       "      <th>reference_period_type_suffix</th>\n",
       "      <th>moic_gross</th>\n",
       "      <th>data_room_id</th>\n",
       "      <th>created_by_user_id</th>\n",
       "      <th>recurring_revenue</th>\n",
       "      <th>bridge_financing</th>\n",
       "      <th>reporting_currency_financials</th>\n",
       "      <th>...</th>\n",
       "      <th>ebitda_adjusted_note</th>\n",
       "      <th>ebitda_margin</th>\n",
       "      <th>cumulative_addons</th>\n",
       "      <th>moic_net_unlevered</th>\n",
       "      <th>revenue</th>\n",
       "      <th>fund_equity_invested</th>\n",
       "      <th>_quarter</th>\n",
       "      <th>_revision_id</th>\n",
       "      <th>ownership_economic_percentage</th>\n",
       "      <th>enterprise_value_valuation_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111dts1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111dts2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cd7032-72f6-46d0-ae21-7a0ca81297eb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203ffba5-3ebb-454a-844c-87cee656bd95</td>\n",
       "      <td>25bd1583-7869-465c-9dc4-664685cd3a6c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cccb3423-eb71-452a-92f1-3b4a64100646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>997bb98e-9ab1-47be-b04b-767d225f60a9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7600000.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d40592d4-9127-4e77-a8c7-9da4755a6105</td>\n",
       "      <td>25bd1583-7869-465c-9dc4-664685cd3a6c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EUR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64ef422a-f3cc-44fe-bf2b-fe5955950008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1eab0e13-6d1d-4c99-a8a0-6c32b56de012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34000000.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d40592d4-9127-4e77-a8c7-9da4755a6105</td>\n",
       "      <td>25bd1583-7869-465c-9dc4-664685cd3a6c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EUR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3e0b9962-f308-420f-9c62-5fa24f5b2e7e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 2 passed. Shape: (4958, 51). Showing 5 data rows above.\n",
      "Wrote 4,958 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv\n",
      "INIT check passed. Shape: (4958, 1)\n",
      "Added columns: ['deal_id', 'reference_date', 'enterprise_value', 'net_debt', 'equity', 'reporting_currency_financials', 'reference_period_type_prefix', 'reference_period_type_suffix', 'revenue', 'ebitda', 'ownership_economic_percentage', 'data_room_name']. Wrote 4,958 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv.\n",
      "ADD_COLUMNS (deal_time_series extra fields) check passed. Shape: (4958, 13)\n",
      "unique_deals: 1165\n",
      "Added columns from 'deal': ['fund_id', 'name', 'entry_date', 'entry_transaction_type', 'sourcing_type', 'investment_role', 'exit_date', 'exit_transaction_type']. Wrote 4,958 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv.\n",
      "ADD_COLUMNS (deal) check passed. Shape: (4958, 21)\n",
      "unique_deals: 1165\n",
      "Added columns from 'fund': ['fund_name', 'vintage_year', 'investment_theme', 'size', 'fund_generation', 'fund_family_generation']. Wrote 4,958 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv.\n",
      "ADD_COLUMNS (fund) check passed. Shape: (4958, 27)\n",
      "unique_deals: 1165\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
