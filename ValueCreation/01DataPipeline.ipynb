{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ----- Read Excel file ----- #\n",
    "\n",
    "REL_PATH = Path(\"InputData/CoreData.xlsx\")\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    \"\"\"\n",
    "    Starting at cwd, walk up to `max_up` parents to find `rel_path`.\n",
    "    Returns the resolved path if found; raises FileNotFoundError otherwise.\n",
    "    \"\"\"\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    # Helpful diagnostics\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "INPUT_XLSX = find_upwards(REL_PATH)\n",
    "\n",
    "import pandas as pd\n",
    "xfile = pd.ExcelFile(INPUT_XLSX)\n",
    "sheets = xfile.sheet_names\n",
    "print(\"Resolved path:\", INPUT_XLSX)\n",
    "print(\"Sheets:\", sheets)\n",
    "\n",
    "assert isinstance(sheets, list), \"Expected a list\"\n",
    "assert sheets and all(isinstance(s, str) and s.strip() for s in sheets), \"Sheet names must be non-empty strings\"\n",
    "assert len(sheets) == len(set(sheets)), \"Duplicate sheet names detected\"\n",
    "print(\"Check 1 passed.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "#----- Quickly check whether also the sheets get read correctly -----#\n",
    "\n",
    "SHEET = \"deal_time_series\"\n",
    "\n",
    "# Guard: make sure the sheet exists (uses `sheets` from Step 1)\n",
    "assert SHEET in sheets, f\"'{SHEET}' not found. Available sheets: {sheets}\"\n",
    "\n",
    "# Read with first row as header\n",
    "dts = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, header=0)\n",
    "\n",
    "# Display column names and first 10 data rows (i.e., Excel rows 2â€“11)\n",
    "print(\"Column names:\", list(dts.columns))\n",
    "display(dts.head(5))\n",
    "\n",
    "# --- Check 2\n",
    "assert isinstance(dts, pd.DataFrame), \"Expected a pandas DataFrame.\"\n",
    "assert not dts.empty, \"Sheet loaded but contains no data.\"\n",
    "assert all(isinstance(c, str) and c.strip() for c in dts.columns), \"Invalid/empty column names.\"\n",
    "print(f\"Check 2 passed. Shape: {dts.shape}. Showing 5 data rows above.\")\n"
   ],
   "id": "858ab65c4fbda592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#----- Create output CSV and take the deal_time_series_id as starter column -----#\n",
    "\n",
    "# Config\n",
    "SHEET = \"deal_time_series\"\n",
    "TARGET_DIR = (find_upwards(Path(\"ValueCreation\")) / \"Data\")\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TARGET_CSV = TARGET_DIR / \"working.csv\"\n",
    "\n",
    "assert SHEET in sheets, f\"Sheet '{SHEET}' not found.\"\n",
    "\n",
    "# Load only the key column; no filtering, no sorting\n",
    "usecols = [\"id\"]\n",
    "raw = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, usecols=usecols)\n",
    "\n",
    "# Preserve order exactly as in the sheet\n",
    "df = raw[[\"id\"]]\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Wrote {len(df):,} rows to {TARGET_CSV}\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "assert TARGET_CSV.exists(), f\"Missing output: {TARGET_CSV}\"\n",
    "check_df = pd.read_csv(TARGET_CSV)\n",
    "\n",
    "# 1) Columns exactly as specified and in order\n",
    "assert list(check_df.columns) == [\"id\"], list(check_df.columns)\n",
    "\n",
    "# 2) Row count preserved\n",
    "assert len(check_df) == len(raw), f\"Row count changed: raw={len(raw)} vs written={len(check_df)}\"\n",
    "\n",
    "# 3) Order preserved: id sequence identical pre/post write\n",
    "assert check_df[\"id\"].tolist() == raw[\"id\"].tolist(), \"Row order changed.\"\n",
    "\n",
    "# 4) Key integrity: non-null and unique\n",
    "assert check_df[\"id\"].notna().all(), \"Null id found.\"\n",
    "assert not check_df[\"id\"].duplicated().any(), \"Duplicate id values found.\"\n",
    "\n",
    "print(\"INIT check passed. Shape:\", check_df.shape)\n"
   ],
   "id": "f61dcef70e669d4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----- Add all needed columns from deal_time_series to the output CSV -----#\n",
    "\n",
    "SHEET = \"deal_time_series\"\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load working file and source\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "requested = [\n",
    "    \"deal_id\", \"reference_date\", \"enterprise_value\", \"net_debt\", \"equity\", \"reporting_currency_financials\", \"reference_period_type_prefix\", \"reference_period_type_suffix\", \"revenue\", \"ebitda\", \"ownership_economic_percentage\", \"data_room_name\",\n",
    "]\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=SHEET,\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Parse Excel-serial 'reference_date' to ISO yyyy-mm-dd (CSV-friendly)\n",
    "if \"reference_date\" in src.columns:\n",
    "    s = src[\"reference_date\"]\n",
    "    if np.issubdtype(s.dtype, np.number):\n",
    "        dt = pd.to_datetime(s, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    else:\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    src[\"reference_date\"] = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Only add columns not already present\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]]\n",
    "\n",
    "# Preserve original row order\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "\n",
    "# Left-join on id\n",
    "out = working.merge(src, on=\"id\", how=\"left\")\n",
    "\n",
    "# Restore order and drop helper\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns: {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "# Row count preserved\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "\n",
    "# Order preserved\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "\n",
    "# Requested columns present\n",
    "missing = [c for c in requested if c not in after.columns]\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "\n",
    "# reference_date parses or is blank\n",
    "_ = pd.to_datetime(after[\"reference_date\"], errors=\"coerce\")\n",
    "print(\"ADD_COLUMNS (deal_time_series extra fields) check passed. Shape:\", after.shape)\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "33d57f70b1226003",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----- Add all needed columns from deal to the output CSV -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "requested = [\n",
    "    \"fund_id\", \"name\", \"entry_date\", \"entry_transaction_type\",\n",
    "    \"sourcing_type\", \"investment_role\", \"exit_date\", \"exit_transaction_type\",\n",
    "]\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"deal\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Only add columns that aren't already present\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]].rename(columns={\"id\": \"deal_id\"})\n",
    "\n",
    "# Preserve original order\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "\n",
    "# Left-join on deal_id\n",
    "out = working.merge(src, on=\"deal_id\", how=\"left\")\n",
    "\n",
    "# Restore order and drop helper\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'deal': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Row count preserved\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "\n",
    "# Order preserved\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "\n",
    "# Requested columns present\n",
    "missing = [c for c in to_add if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "\n",
    "print(\"ADD_COLUMNS (deal) check passed. Shape:\", after.shape)\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "2eaa115911473d0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----- Add all needed columns from fund to the output CSV -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load working file\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str, \"fund_id\": str})\n",
    "\n",
    "requested = [\"name\", \"vintage_year\", \"investment_theme\", \"size\", \"fund_generation\", \"fund_family_generation\"]\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"fund\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Rename to avoid collision and to align join key\n",
    "src = src.rename(columns={\"id\": \"fund_id\", \"name\": \"fund_name\"})\n",
    "\n",
    "# Only add columns that aren't already present\n",
    "to_add = [c if c != \"name\" else \"fund_name\" for c in requested]\n",
    "to_add = [c for c in to_add if c not in working.columns]\n",
    "src = src[[\"fund_id\", *to_add]]\n",
    "\n",
    "# Preserve original order\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "\n",
    "# Left join on fund_id\n",
    "out = working.merge(src, on=\"fund_id\", how=\"left\")\n",
    "\n",
    "# Restore order and drop helper\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'fund': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str, \"fund_id\": str})\n",
    "\n",
    "# Row count preserved\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "\n",
    "# Order preserved\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "\n",
    "# Requested columns (with rename) present\n",
    "expected = [c if c != \"name\" else \"fund_name\" for c in [\"name\",\"vintage_year\",\"investment_theme\",\"size\",\"fund_generation\",\"fund_family_generation\"]]\n",
    "missing = [c for c in expected if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "\n",
    "print(\"ADD_COLUMNS (fund) check passed. Shape:\", after.shape)\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "43ce7d784abcbea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#----- Quick test to get all rows of one deal_id in the output CSV -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "deal_id_value = \"49328606-087c-4288-972a-614c19bd519e\"\n",
    "# \"49328606-087c-4288-972a-614c19bd519e\" #NA test DEAL\n",
    "# \"30f4104d-0343-4031-a729-ec81b646861a\" Visma DEAL\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "subset = df.loc[df[\"deal_id\"] == deal_id_value]\n",
    "\n",
    "print(f\"Rows for deal_id={deal_id_value}: {len(subset)}\")\n",
    "display(subset)\n",
    "\n",
    "assert not subset.empty, \"No rows found for the specified deal_id.\"\n",
    "print(\"Filter check passed.\")\n",
    "\n",
    "\"\"\"from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "name_value = \"Visma DEAL\"\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "subset = df.loc[df[\"name\"].astype(str).str.strip() == name_value]\n",
    "\n",
    "print(f'Rows for name=\"{name_value}\": {len(subset)}')\n",
    "display(subset)\n",
    "\n",
    "assert \"name\" in df.columns, \"Column 'name' not found.\"\n",
    "assert not subset.empty, f'No rows found for name=\"{name_value}\".'\n",
    "print(\"Filter check passed.\")\"\"\"\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "232e1c91981822e4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
