{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T20:17:34.797408Z",
     "start_time": "2025-10-15T20:17:27.130403Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# ---------- Helpers ---------- #\n",
    "REL_PATH = Path(\"InputData/CoreData.xlsx\")\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "# ---------- Step 1: Read Excel / basic checks ---------- #\n",
    "INPUT_XLSX = find_upwards(REL_PATH)\n",
    "xfile = pd.ExcelFile(INPUT_XLSX)\n",
    "sheets = xfile.sheet_names\n",
    "print(\"Resolved path:\", INPUT_XLSX)\n",
    "print(\"Sheets:\", sheets)\n",
    "\n",
    "assert isinstance(sheets, list), \"Expected a list\"\n",
    "assert sheets and all(isinstance(s, str) and s.strip() for s in sheets), \"Sheet names must be non-empty strings\"\n",
    "assert len(sheets) == len(set(sheets)), \"Duplicate sheet names detected\"\n",
    "print(\"Check 1 passed.\")\n",
    "\n",
    "SHEET = \"deal_time_series\"\n",
    "assert SHEET in sheets, f\"'{SHEET}' not found. Available sheets: {sheets}\"\n",
    "dts = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, header=0)\n",
    "print(\"Column names:\", list(dts.columns))\n",
    "assert isinstance(dts, pd.DataFrame), \"Expected a pandas DataFrame.\"\n",
    "assert not dts.empty, \"Sheet loaded but contains no data.\"\n",
    "assert all(isinstance(c, str) and c.strip() for c in dts.columns), \"Invalid/empty column names.\"\n",
    "print(f\"Check 2 passed. Shape: {dts.shape}. Showing 5 data rows above.\")\n",
    "\n",
    "# ---------- Step 2: Init working.csv with id ---------- #\n",
    "TARGET_DIR = (find_upwards(Path(\"ValueCreation\")) / \"Data\")\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TARGET_CSV = TARGET_DIR / \"working.csv\"\n",
    "\n",
    "usecols = [\"id\"]\n",
    "raw = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, usecols=usecols)\n",
    "df = raw[[\"id\"]]\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Wrote {len(df):,} rows to {TARGET_CSV}\")\n",
    "\n",
    "assert TARGET_CSV.exists(), f\"Missing output: {TARGET_CSV}\"\n",
    "check_df = pd.read_csv(TARGET_CSV)\n",
    "assert list(check_df.columns) == [\"id\"], list(check_df.columns)\n",
    "assert len(check_df) == len(raw), f\"Row count changed: raw={len(raw)} vs written={len(check_df)}\"\n",
    "assert check_df[\"id\"].tolist() == raw[\"id\"].tolist(), \"Row order changed.\"\n",
    "assert check_df[\"id\"].notna().all(), \"Null id found.\"\n",
    "assert not check_df[\"id\"].duplicated().any(), \"Duplicate id values found.\"\n",
    "print(\"INIT check passed. Shape:\", check_df.shape)\n",
    "\n",
    "# ---------- Step 3: Add columns from deal_time_series ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "requested = [\n",
    "    \"deal_id\", \"reference_date\", \"enterprise_value\", \"net_debt\", \"equity\",\n",
    "    \"reporting_currency_financials\", \"reference_period_type_prefix\",\n",
    "    \"reference_period_type_suffix\", \"revenue\", \"ebitda\",\n",
    "    \"ownership_economic_percentage\",\n",
    "]\n",
    "# \"data_room_name\",\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=SHEET,\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "assert src[\"id\"].is_unique, \"deal_time_series: duplicate id values would explode rows on merge.\"\n",
    "\n",
    "# Normalize reference_date robustly\n",
    "if \"reference_date\" in src.columns:\n",
    "    s = src[\"reference_date\"]\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    ser = pd.to_numeric(s, errors=\"coerce\")\n",
    "    is_serialish = (dt.isna() & ser.gt(20000)).mean() > 0.5\n",
    "    if is_serialish:\n",
    "        dt = pd.to_datetime(ser, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    src[\"reference_date\"] = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]]\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns: {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "missing = [c for c in requested if c not in after.columns]\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "_ = pd.to_datetime(after[\"reference_date\"], errors=\"coerce\")\n",
    "print(\"ADD_COLUMNS (deal_time_series extra fields) check passed. Shape:\", after.shape)\n",
    "print(\"unique_deals:\", pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n",
    "\n",
    "# ---------- Step 4: Add columns from deal ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "requested = [\n",
    "    \"name\", \"entry_date\", \"sourcing_type\", \"entry_transaction_type\",\n",
    "     \"investment_role\", \"exit_date\", \"exit_transaction_type\", \"fund_id\",\n",
    "]\n",
    "#\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"deal\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ").rename(columns={\"id\": \"deal_id\"})\n",
    "assert src[\"deal_id\"].is_unique, \"deal: duplicate deal_id values would explode rows on merge.\"\n",
    "\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "# IMPORTANT: select with 'deal_id' (already renamed), not 'id'\n",
    "src = src[[\"deal_id\", *to_add]]\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"deal_id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'deal': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "missing = [c for c in to_add if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "print(\"ADD_COLUMNS (deal) check passed. Shape:\", after.shape)\n",
    "print(\"unique_deals:\", pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n",
    "\n",
    "# ---------- Step 5: Add columns from fund ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str, \"fund_id\": str})\n",
    "\n",
    "requested = [\"name\", \"investment_theme\", \"vintage_year\", \"size\", \"fund_generation\", \"fund_family_generation\"]\n",
    "#\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"fund\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ").rename(columns={\"id\": \"fund_id\", \"name\": \"fund_name\"})\n",
    "assert src[\"fund_id\"].is_unique, \"fund: duplicate fund_id values would explode rows on merge.\"\n",
    "\n",
    "to_add = [c if c != \"name\" else \"fund_name\" for c in requested]\n",
    "to_add = [c for c in to_add if c not in working.columns]\n",
    "src = src[[\"fund_id\", *to_add]]\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"fund_id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'fund': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str, \"fund_id\": str})\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "expected = [c if c != \"name\" else \"fund_name\" for c in [\"name\",\"vintage_year\",\"investment_theme\",\"size\",\"fund_generation\",\"fund_family_generation\"]]\n",
    "missing = [c for c in expected if c not in after.columns]\n",
    "print(\"ADD_COLUMNS (fund) check passed. Shape:\", after.shape)\n",
    "print(\"unique_deals:\", pd.read_csv(TARGET_CSV, dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved path: /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/InputData/CoreData.xlsx\n",
      "Sheets: ['general_partner', 'fund', 'fund_cash_flow', 'capital_account', 'deal', 'deal_time_series', 'deal_cash_flow', 'deal_partner', 'deal_acquirer', 'deal_vendor', 'organization', 'person']\n",
      "Check 1 passed.\n",
      "Column names: ['id', 'deal_revision_id', 'enterprise_value', 'equity', 'net_debt', 'revenue', 'ebit', 'ebitda', 'capex', 'ebitda_multiple', 'unrealized_value', 'realized_value', 'total_value', 'total_investment_cost', 'irr_gross', 'bridge_financing', 'ownership_economic_percentage', 'reporting_currency_valuation', 'reporting_currency_financials', 'reported_date', 'reference_period_type_prefix', 'reference_date', 'reference_period_type_suffix', 'quarterly_company_update', '_year', '_quarter', 'predicted_sentiment', 'enterprise_value_valuation_rationale', 'enterprise_value_valuation_multiple', 'enterprise_value_valuation_amount', 'ebitda_adjusted', 'ebitda_margin', 'ebitda_adjusted_note', 'revenue_multiple', 'recurring_revenue', 'recurring_revenue_percentage', 'fund_equity_invested', 'cumulative_addons', 'management_equity_percentage', 'irr_net', 'irr_net_unlevered', 'moic_gross', 'moic_net', 'moic_net_unlevered', 'deal_id', 'employees_count', '_created_at_utc', 'created_by_user_id', '_revision_id', 'is_main', 'data_room_id']\n",
      "Check 2 passed. Shape: (6321, 51). Showing 5 data rows above.\n",
      "Wrote 6,321 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv\n",
      "INIT check passed. Shape: (6321, 1)\n",
      "Added columns: ['deal_id', 'reference_date', 'enterprise_value', 'net_debt', 'equity', 'reporting_currency_financials', 'reference_period_type_prefix', 'reference_period_type_suffix', 'revenue', 'ebitda', 'ownership_economic_percentage']. Wrote 6,321 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv.\n",
      "ADD_COLUMNS (deal_time_series extra fields) check passed. Shape: (6321, 12)\n",
      "unique_deals: 1525\n",
      "Added columns from 'deal': ['name', 'entry_date', 'sourcing_type', 'entry_transaction_type', 'investment_role', 'exit_date', 'exit_transaction_type', 'fund_id']. Wrote 6,321 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv.\n",
      "ADD_COLUMNS (deal) check passed. Shape: (6321, 20)\n",
      "unique_deals: 1525\n",
      "Added columns from 'fund': ['fund_name', 'investment_theme', 'vintage_year', 'size', 'fund_generation', 'fund_family_generation']. Wrote 6,321 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv.\n",
      "ADD_COLUMNS (fund) check passed. Shape: (6321, 26)\n",
      "unique_deals: 1525\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
