{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-04T22:17:18.239387Z",
     "start_time": "2025-10-04T22:17:16.697155Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ----- Read Excel file ----- #\n",
    "\n",
    "REL_PATH = Path(\"InputData/CoreData.xlsx\")\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    \"\"\"\n",
    "    Starting at cwd, walk up to `max_up` parents to find `rel_path`.\n",
    "    Returns the resolved path if found; raises FileNotFoundError otherwise.\n",
    "    \"\"\"\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    # Helpful diagnostics\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "INPUT_XLSX = find_upwards(REL_PATH)\n",
    "\n",
    "import pandas as pd\n",
    "xfile = pd.ExcelFile(INPUT_XLSX)\n",
    "sheets = xfile.sheet_names\n",
    "print(\"Resolved path:\", INPUT_XLSX)\n",
    "print(\"Sheets:\", sheets)\n",
    "\n",
    "assert isinstance(sheets, list), \"Expected a list\"\n",
    "assert sheets and all(isinstance(s, str) and s.strip() for s in sheets), \"Sheet names must be non-empty strings\"\n",
    "assert len(sheets) == len(set(sheets)), \"Duplicate sheet names detected\"\n",
    "print(\"Check 1 passed.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved path: /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/InputData/CoreData.xlsx\n",
      "Sheets: ['Metadata', 'dashboard', 'general_partner', 'fund', 'fund_cash_flow', 'capital_account', 'deal', 'deal_time_series', 'deal_cash_flow', 'deal_partner', 'deal_acquirer', 'deal_vendor', 'organization', 'person']\n",
      "Check 1 passed.\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T22:17:21.095794Z",
     "start_time": "2025-10-04T22:17:18.252022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "#----- Quickly check whether also the sheets get read correctly -----#\n",
    "\n",
    "SHEET = \"deal_time_series\"\n",
    "\n",
    "# Guard: make sure the sheet exists (uses `sheets` from Step 1)\n",
    "assert SHEET in sheets, f\"'{SHEET}' not found. Available sheets: {sheets}\"\n",
    "\n",
    "# Read with first row as header\n",
    "dts = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, header=0)\n",
    "\n",
    "# Display column names and first 10 data rows (i.e., Excel rows 2â€“11)\n",
    "print(\"Column names:\", list(dts.columns))\n",
    "display(dts.head(5))\n",
    "\n",
    "# --- Check 2\n",
    "assert isinstance(dts, pd.DataFrame), \"Expected a pandas DataFrame.\"\n",
    "assert not dts.empty, \"Sheet loaded but contains no data.\"\n",
    "assert all(isinstance(c, str) and c.strip() for c in dts.columns), \"Invalid/empty column names.\"\n",
    "print(f\"Check 2 passed. Shape: {dts.shape}. Showing 5 data rows above.\")\n"
   ],
   "id": "858ab65c4fbda592",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['id', 'total_value', 'ebitda', 'reference_period_type_suffix', 'moic_gross', 'data_room_id', 'created_by_user_id', 'recurring_revenue', 'bridge_financing', 'reporting_currency_financials', 'irr_net', 'reference_period_type_prefix', 'moic_net', 'data_room_name', 'realized_value', 'irr_gross', 'ebitda_adjusted', 'net_debt', 'ebitda_multiple', 'enterprise_value_valuation_rationale', 'is_main', 'equity', 'reporting_currency_valuation', 'management_equity_percentage', 'revenue_multiple', 'recurring_revenue_percentage', 'quarterly_company_update', '_created_at_utc', 'enterprise_value', 'enterprise_value_valuation_multiple', '_year', 'reference_date', 'irr_net_unlevered', 'capex', 'total_investment_cost', 'deal_revision_id', 'reported_date', 'deal_id', 'unrealized_value', 'predicted_sentiment', 'ebit', 'ebitda_adjusted_note', 'ebitda_margin', 'cumulative_addons', 'moic_net_unlevered', 'revenue', 'fund_equity_invested', '_quarter', '_revision_id', 'ownership_economic_percentage', 'enterprise_value_valuation_amount']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                     id  total_value      ebitda  \\\n",
       "0  b0cd7032-72f6-46d0-ae21-7a0ca81297eb          NaN         NaN   \n",
       "1  997bb98e-9ab1-47be-b04b-767d225f60a9          NaN   7600000.0   \n",
       "2  1eab0e13-6d1d-4c99-a8a0-6c32b56de012          NaN  34000000.0   \n",
       "3  9ae2e65b-ec9d-4215-8982-f39658e9fa1e   19839000.0  13747260.0   \n",
       "4  fa677592-875d-413e-91fe-c8af1dd99f63   40736000.0  18894000.0   \n",
       "\n",
       "  reference_period_type_suffix  moic_gross  \\\n",
       "0                       Actual         NaN   \n",
       "1                       Actual         NaN   \n",
       "2                       Actual         NaN   \n",
       "3                          NaN         NaN   \n",
       "4                          NaN         NaN   \n",
       "\n",
       "                           data_room_id                    created_by_user_id  \\\n",
       "0  203ffba5-3ebb-454a-844c-87cee656bd95  25bd1583-7869-465c-9dc4-664685cd3a6c   \n",
       "1  d40592d4-9127-4e77-a8c7-9da4755a6105  25bd1583-7869-465c-9dc4-664685cd3a6c   \n",
       "2  d40592d4-9127-4e77-a8c7-9da4755a6105  25bd1583-7869-465c-9dc4-664685cd3a6c   \n",
       "3  9d92005d-7097-4949-88ea-8eb6ff688a86  38ed8bb8-d707-4652-94c0-6a094d116b50   \n",
       "4  9d92005d-7097-4949-88ea-8eb6ff688a86  38ed8bb8-d707-4652-94c0-6a094d116b50   \n",
       "\n",
       "   recurring_revenue  bridge_financing reporting_currency_financials  ...  \\\n",
       "0                NaN               NaN                           USD  ...   \n",
       "1                NaN               NaN                           EUR  ...   \n",
       "2                NaN               NaN                           EUR  ...   \n",
       "3                NaN               NaN                           EUR  ...   \n",
       "4                NaN               NaN                           EUR  ...   \n",
       "\n",
       "   ebitda_adjusted_note ebitda_margin  cumulative_addons moic_net_unlevered  \\\n",
       "0                   NaN           NaN                NaN                NaN   \n",
       "1                   NaN         0.046                NaN                NaN   \n",
       "2                   NaN         0.121                NaN                NaN   \n",
       "3                   NaN           NaN                NaN                NaN   \n",
       "4                   NaN           NaN                NaN                NaN   \n",
       "\n",
       "       revenue  fund_equity_invested  _quarter  \\\n",
       "0          NaN                   NaN         1   \n",
       "1  164000000.0                   NaN         1   \n",
       "2  280000000.0                   NaN         2   \n",
       "3   48663900.0            19839000.0         3   \n",
       "4  393249800.0            40736000.0         2   \n",
       "\n",
       "                           _revision_id  ownership_economic_percentage  \\\n",
       "0  cccb3423-eb71-452a-92f1-3b4a64100646                            NaN   \n",
       "1  64ef422a-f3cc-44fe-bf2b-fe5955950008                            NaN   \n",
       "2  3e0b9962-f308-420f-9c62-5fa24f5b2e7e                            NaN   \n",
       "3  9ae2e65b-ec9d-4215-8982-f39658e9fa1e                            1.0   \n",
       "4  fa677592-875d-413e-91fe-c8af1dd99f63                            0.5   \n",
       "\n",
       "  enterprise_value_valuation_amount  \n",
       "0                               NaN  \n",
       "1                               NaN  \n",
       "2                               NaN  \n",
       "3                               NaN  \n",
       "4                               NaN  \n",
       "\n",
       "[5 rows x 51 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>total_value</th>\n",
       "      <th>ebitda</th>\n",
       "      <th>reference_period_type_suffix</th>\n",
       "      <th>moic_gross</th>\n",
       "      <th>data_room_id</th>\n",
       "      <th>created_by_user_id</th>\n",
       "      <th>recurring_revenue</th>\n",
       "      <th>bridge_financing</th>\n",
       "      <th>reporting_currency_financials</th>\n",
       "      <th>...</th>\n",
       "      <th>ebitda_adjusted_note</th>\n",
       "      <th>ebitda_margin</th>\n",
       "      <th>cumulative_addons</th>\n",
       "      <th>moic_net_unlevered</th>\n",
       "      <th>revenue</th>\n",
       "      <th>fund_equity_invested</th>\n",
       "      <th>_quarter</th>\n",
       "      <th>_revision_id</th>\n",
       "      <th>ownership_economic_percentage</th>\n",
       "      <th>enterprise_value_valuation_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0cd7032-72f6-46d0-ae21-7a0ca81297eb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203ffba5-3ebb-454a-844c-87cee656bd95</td>\n",
       "      <td>25bd1583-7869-465c-9dc4-664685cd3a6c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>cccb3423-eb71-452a-92f1-3b4a64100646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>997bb98e-9ab1-47be-b04b-767d225f60a9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7600000.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d40592d4-9127-4e77-a8c7-9da4755a6105</td>\n",
       "      <td>25bd1583-7869-465c-9dc4-664685cd3a6c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EUR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>64ef422a-f3cc-44fe-bf2b-fe5955950008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1eab0e13-6d1d-4c99-a8a0-6c32b56de012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34000000.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d40592d4-9127-4e77-a8c7-9da4755a6105</td>\n",
       "      <td>25bd1583-7869-465c-9dc4-664685cd3a6c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EUR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3e0b9962-f308-420f-9c62-5fa24f5b2e7e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9ae2e65b-ec9d-4215-8982-f39658e9fa1e</td>\n",
       "      <td>19839000.0</td>\n",
       "      <td>13747260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9d92005d-7097-4949-88ea-8eb6ff688a86</td>\n",
       "      <td>38ed8bb8-d707-4652-94c0-6a094d116b50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EUR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48663900.0</td>\n",
       "      <td>19839000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>9ae2e65b-ec9d-4215-8982-f39658e9fa1e</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fa677592-875d-413e-91fe-c8af1dd99f63</td>\n",
       "      <td>40736000.0</td>\n",
       "      <td>18894000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9d92005d-7097-4949-88ea-8eb6ff688a86</td>\n",
       "      <td>38ed8bb8-d707-4652-94c0-6a094d116b50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EUR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>393249800.0</td>\n",
       "      <td>40736000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>fa677592-875d-413e-91fe-c8af1dd99f63</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 2 passed. Shape: (4956, 51). Showing 5 data rows above.\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T22:17:23.689448Z",
     "start_time": "2025-10-04T22:17:21.105051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#----- Create output CSV and take the deal_time_series_id as starter column -----#\n",
    "\n",
    "# Config\n",
    "SHEET = \"deal_time_series\"\n",
    "TARGET_DIR = (find_upwards(Path(\"ValueCreation\")) / \"Data\")\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TARGET_CSV = TARGET_DIR / \"working.csv\"\n",
    "\n",
    "assert SHEET in sheets, f\"Sheet '{SHEET}' not found.\"\n",
    "\n",
    "# Load only the key column; no filtering, no sorting\n",
    "usecols = [\"id\"]\n",
    "raw = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, usecols=usecols)\n",
    "\n",
    "# Preserve order exactly as in the sheet\n",
    "df = raw[[\"id\"]]\n",
    "\n",
    "# Persist\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Wrote {len(df):,} rows to {TARGET_CSV}\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "assert TARGET_CSV.exists(), f\"Missing output: {TARGET_CSV}\"\n",
    "check_df = pd.read_csv(TARGET_CSV)\n",
    "\n",
    "# 1) Columns exactly as specified and in order\n",
    "assert list(check_df.columns) == [\"id\"], list(check_df.columns)\n",
    "\n",
    "# 2) Row count preserved\n",
    "assert len(check_df) == len(raw), f\"Row count changed: raw={len(raw)} vs written={len(check_df)}\"\n",
    "\n",
    "# 3) Order preserved: id sequence identical pre/post write\n",
    "assert check_df[\"id\"].tolist() == raw[\"id\"].tolist(), \"Row order changed.\"\n",
    "\n",
    "# 4) Key integrity: non-null and unique\n",
    "assert check_df[\"id\"].notna().all(), \"Null id found.\"\n",
    "assert not check_df[\"id\"].duplicated().any(), \"Duplicate id values found.\"\n",
    "\n",
    "print(\"INIT check passed. Shape:\", check_df.shape)\n"
   ],
   "id": "f61dcef70e669d4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 4,956 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv\n",
      "INIT check passed. Shape: (4956, 1)\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T22:17:26.186472Z",
     "start_time": "2025-10-04T22:17:23.693185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----- Add all needed columns from deal_time_series to the output CSV -----#\n",
    "\n",
    "SHEET = \"deal_time_series\"\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load working file and source\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "requested = [\n",
    "    \"deal_id\", \"reference_date\", \"enterprise_value\", \"net_debt\", \"equity\", \"reporting_currency_financials\", \"reference_period_type_prefix\", \"reference_period_type_suffix\", \"revenue\", \"ebitda\", \"ownership_economic_percentage\", \"data_room_name\",\n",
    "]\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=SHEET,\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Parse Excel-serial 'reference_date' to ISO yyyy-mm-dd (CSV-friendly)\n",
    "if \"reference_date\" in src.columns:\n",
    "    s = src[\"reference_date\"]\n",
    "    if np.issubdtype(s.dtype, np.number):\n",
    "        dt = pd.to_datetime(s, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    else:\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    src[\"reference_date\"] = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Only add columns not already present\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]]\n",
    "\n",
    "# Preserve original row order\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "\n",
    "# Left-join on id\n",
    "out = working.merge(src, on=\"id\", how=\"left\")\n",
    "\n",
    "# Restore order and drop helper\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns: {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "# Row count preserved\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "\n",
    "# Order preserved\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "\n",
    "# Requested columns present\n",
    "missing = [c for c in requested if c not in after.columns]\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "\n",
    "# reference_date parses or is blank\n",
    "_ = pd.to_datetime(after[\"reference_date\"], errors=\"coerce\")\n",
    "print(\"ADD_COLUMNS (deal_time_series extra fields) check passed. Shape:\", after.shape)\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "33d57f70b1226003",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added columns: ['deal_id', 'reference_date', 'enterprise_value', 'net_debt', 'equity', 'reporting_currency_financials', 'reference_period_type_prefix', 'reference_period_type_suffix', 'revenue', 'ebitda', 'ownership_economic_percentage', 'data_room_name']. Wrote 4,956 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv.\n",
      "ADD_COLUMNS (deal_time_series extra fields) check passed. Shape: (4956, 13)\n",
      "unique_deals: 1164\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T22:17:28.021638Z",
     "start_time": "2025-10-04T22:17:26.202886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----- Add all needed columns from deal to the output CSV -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "requested = [\n",
    "    \"fund_id\", \"name\", \"entry_date\", \"entry_transaction_type\",\n",
    "    \"sourcing_type\", \"investment_role\", \"exit_date\", \"exit_transaction_type\",\n",
    "]\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"deal\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Only add columns that aren't already present\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]].rename(columns={\"id\": \"deal_id\"})\n",
    "\n",
    "# Preserve original order\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "\n",
    "# Left-join on deal_id\n",
    "out = working.merge(src, on=\"deal_id\", how=\"left\")\n",
    "\n",
    "# Restore order and drop helper\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'deal': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "\n",
    "# Row count preserved\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "\n",
    "# Order preserved\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "\n",
    "# Requested columns present\n",
    "missing = [c for c in to_add if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "\n",
    "print(\"ADD_COLUMNS (deal) check passed. Shape:\", after.shape)\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "2eaa115911473d0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added columns from 'deal': ['fund_id', 'name', 'entry_date', 'entry_transaction_type', 'sourcing_type', 'investment_role', 'exit_date', 'exit_transaction_type']. Wrote 4,956 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv.\n",
      "ADD_COLUMNS (deal) check passed. Shape: (4956, 21)\n",
      "unique_deals: 1164\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T22:17:29.618165Z",
     "start_time": "2025-10-04T22:17:28.025548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----- Add all needed columns from fund to the output CSV -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "# Load working file\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str, \"fund_id\": str})\n",
    "\n",
    "requested = [\"name\", \"vintage_year\", \"investment_theme\", \"size\", \"fund_generation\", \"fund_family_generation\"]\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"fund\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Rename to avoid collision and to align join key\n",
    "src = src.rename(columns={\"id\": \"fund_id\", \"name\": \"fund_name\"})\n",
    "\n",
    "# Only add columns that aren't already present\n",
    "to_add = [c if c != \"name\" else \"fund_name\" for c in requested]\n",
    "to_add = [c for c in to_add if c not in working.columns]\n",
    "src = src[[\"fund_id\", *to_add]]\n",
    "\n",
    "# Preserve original order\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "\n",
    "# Left join on fund_id\n",
    "out = working.merge(src, on=\"fund_id\", how=\"left\")\n",
    "\n",
    "# Restore order and drop helper\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "\n",
    "# Save\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'fund': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str, \"fund_id\": str})\n",
    "\n",
    "# Row count preserved\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "\n",
    "# Order preserved\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "\n",
    "# Requested columns (with rename) present\n",
    "expected = [c if c != \"name\" else \"fund_name\" for c in [\"name\",\"vintage_year\",\"investment_theme\",\"size\",\"fund_generation\",\"fund_family_generation\"]]\n",
    "missing = [c for c in expected if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "\n",
    "print(\"ADD_COLUMNS (fund) check passed. Shape:\", after.shape)\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "43ce7d784abcbea1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added columns from 'fund': ['fund_name', 'vintage_year', 'investment_theme', 'size', 'fund_generation', 'fund_family_generation']. Wrote 4,956 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/ValueCreation/Data/working.csv.\n",
      "ADD_COLUMNS (fund) check passed. Shape: (4956, 27)\n",
      "unique_deals: 1164\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T22:17:29.665125Z",
     "start_time": "2025-10-04T22:17:29.622272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#----- Quick test to get all rows of one deal_id in the output CSV -----#\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "deal_id_value = \"49328606-087c-4288-972a-614c19bd519e\"\n",
    "# \"49328606-087c-4288-972a-614c19bd519e\" #NA test DEAL\n",
    "# \"30f4104d-0343-4031-a729-ec81b646861a\" Visma DEAL\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "subset = df.loc[df[\"deal_id\"] == deal_id_value]\n",
    "\n",
    "print(f\"Rows for deal_id={deal_id_value}: {len(subset)}\")\n",
    "display(subset)\n",
    "\n",
    "assert not subset.empty, \"No rows found for the specified deal_id.\"\n",
    "print(\"Filter check passed.\")\n",
    "\n",
    "\"\"\"from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "TARGET_CSV = (find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\")\n",
    "\n",
    "name_value = \"Visma DEAL\"\n",
    "\n",
    "df = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"deal_id\": str})\n",
    "subset = df.loc[df[\"name\"].astype(str).str.strip() == name_value]\n",
    "\n",
    "print(f'Rows for name=\"{name_value}\": {len(subset)}')\n",
    "display(subset)\n",
    "\n",
    "assert \"name\" in df.columns, \"Column 'name' not found.\"\n",
    "assert not subset.empty, f'No rows found for name=\"{name_value}\".'\n",
    "print(\"Filter check passed.\")\"\"\"\n",
    "\n",
    "print(\"unique_deals:\", pd.read_csv((find_upwards(Path(\"ValueCreation\")) / \"Data\" / \"working.csv\"), dtype={\"deal_id\": str})[\"deal_id\"].nunique())\n"
   ],
   "id": "232e1c91981822e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows for deal_id=49328606-087c-4288-972a-614c19bd519e: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                        id  \\\n",
       "4069  693b03b7-f6f0-4540-8b7c-07e1ac4fff65   \n",
       "\n",
       "                                   deal_id reference_date  enterprise_value  \\\n",
       "4069  49328606-087c-4288-972a-614c19bd519e     2024-03-01               NaN   \n",
       "\n",
       "      net_debt  equity reporting_currency_financials  \\\n",
       "4069       NaN     NaN                           EUR   \n",
       "\n",
       "     reference_period_type_prefix reference_period_type_suffix  revenue  ...  \\\n",
       "4069                          NaN                          NaN      NaN  ...   \n",
       "\n",
       "      sourcing_type  investment_role exit_date exit_transaction_type  \\\n",
       "4069            NaN              NaN       NaN                   NaN   \n",
       "\n",
       "     fund_name vintage_year investment_theme size fund_generation  \\\n",
       "4069       NaN          NaN              NaN  NaN             NaN   \n",
       "\n",
       "     fund_family_generation  \n",
       "4069                    NaN  \n",
       "\n",
       "[1 rows x 27 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>deal_id</th>\n",
       "      <th>reference_date</th>\n",
       "      <th>enterprise_value</th>\n",
       "      <th>net_debt</th>\n",
       "      <th>equity</th>\n",
       "      <th>reporting_currency_financials</th>\n",
       "      <th>reference_period_type_prefix</th>\n",
       "      <th>reference_period_type_suffix</th>\n",
       "      <th>revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>sourcing_type</th>\n",
       "      <th>investment_role</th>\n",
       "      <th>exit_date</th>\n",
       "      <th>exit_transaction_type</th>\n",
       "      <th>fund_name</th>\n",
       "      <th>vintage_year</th>\n",
       "      <th>investment_theme</th>\n",
       "      <th>size</th>\n",
       "      <th>fund_generation</th>\n",
       "      <th>fund_family_generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4069</th>\n",
       "      <td>693b03b7-f6f0-4540-8b7c-07e1ac4fff65</td>\n",
       "      <td>49328606-087c-4288-972a-614c19bd519e</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EUR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 27 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter check passed.\n",
      "unique_deals: 1164\n"
     ]
    }
   ],
   "execution_count": 84
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
