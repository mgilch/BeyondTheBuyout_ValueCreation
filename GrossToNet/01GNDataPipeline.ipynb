{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-13T19:53:59.313052Z",
     "start_time": "2025-10-13T19:53:50.052982Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Helpers ---------- #\n",
    "REL_PATH = Path(\"InputData/CoreData.xlsx\")\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "# ---------- Step 1: Read Excel / basic checks ---------- #\n",
    "INPUT_XLSX = find_upwards(REL_PATH)\n",
    "xfile = pd.ExcelFile(INPUT_XLSX)\n",
    "sheets = xfile.sheet_names\n",
    "print(\"Resolved path:\", INPUT_XLSX)\n",
    "print(\"Sheets:\", sheets)\n",
    "\n",
    "assert isinstance(sheets, list) and sheets, \"Sheet names missing.\"\n",
    "assert all(isinstance(s, str) and s.strip() for s in sheets), \"Invalid sheet name(s).\"\n",
    "assert len(sheets) == len(set(sheets)), \"Duplicate sheet names detected.\"\n",
    "\n",
    "SHEET = \"deal_cash_flow\"\n",
    "assert SHEET in sheets, f\"'{SHEET}' not found. Available sheets: {sheets}\"\n",
    "\n",
    "# Load only the header first to inspect columns safely\n",
    "probe = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, nrows=0)\n",
    "cols = list(probe.columns)\n",
    "print(\"Columns:\", cols)\n",
    "assert \"id\" in cols, f\"'id' column not found in '{SHEET}'.\"\n",
    "\n",
    "# ---------- Step 2: Initialize GrossToNet/Data/deal_cash_flow.csv with id ---------- #\n",
    "TARGET_DIR = (find_upwards(Path(\"GrossToNet\")) / \"Data\")\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TARGET_CSV = TARGET_DIR / \"deal_cash_flow.csv\"\n",
    "\n",
    "raw = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, usecols=[\"id\"], dtype={\"id\": str})\n",
    "df = raw[[\"id\"]]\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Wrote {len(df):,} rows to {TARGET_CSV}\")\n",
    "\n",
    "# ---------- Step 3: Integrity checks on the written CSV ---------- #\n",
    "assert TARGET_CSV.exists(), f\"Missing output: {TARGET_CSV}\"\n",
    "check_df = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "assert list(check_df.columns) == [\"id\"], f\"Unexpected columns: {list(check_df.columns)}\"\n",
    "assert len(check_df) == len(raw), f\"Row count changed: raw={len(raw)} vs written={len(check_df)}\"\n",
    "assert check_df[\"id\"].tolist() == raw[\"id\"].tolist(), \"Row order changed.\"\n",
    "assert check_df[\"id\"].notna().all(), \"Null id found.\"\n",
    "assert not check_df[\"id\"].duplicated().any(), \"Duplicate id values found.\"\n",
    "\n",
    "print(\"INIT (deal_cash_flow id seed) check passed. Shape:\", check_df.shape)\n",
    "\n",
    "# ---------- Step 3: Add columns from deal_cash_flow ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"GrossToNet\")) / \"Data\" / \"deal_cash_flow.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "requested = [\"cash_flow_type\", \"currency\", \"cash_flow\", \"cash_flow_date\", \"deal_id\"]\n",
    "\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"deal_cash_flow\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Ensure one row per id to avoid row-multiplying merges.\n",
    "assert src[\"id\"].is_unique, \"deal_cash_flow: duplicate id values would explode rows on merge.\"\n",
    "\n",
    "# Normalize cash_flow_date if present (handle Excel serials and strings).\n",
    "if \"cash_flow_date\" in src.columns:\n",
    "    s = src[\"cash_flow_date\"]\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    ser = pd.to_numeric(s, errors=\"coerce\")\n",
    "    # Heuristic: many Excel serials are > 20000\n",
    "    is_serialish = (dt.isna() & ser.gt(20000)).mean() > 0.5\n",
    "    if is_serialish:\n",
    "        dt = pd.to_datetime(ser, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    src[\"cash_flow_date\"] = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]]\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'deal_cash_flow': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "# Post-merge checks\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "missing = [c for c in to_add if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "_ = pd.to_datetime(after[\"cash_flow_date\"], errors=\"coerce\")  # smoke-check\n",
    "print(\"ADD_COLUMNS (deal_cash_flow) check passed. Shape:\", after.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved path: /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/InputData/CoreData.xlsx\n",
      "Sheets: ['Metadata', 'dashboard', 'general_partner', 'fund', 'fund_cash_flow', 'capital_account', 'deal', 'deal_time_series', 'deal_cash_flow', 'deal_partner', 'deal_acquirer', 'deal_vendor', 'organization', 'person']\n",
      "Columns: ['id', 'notes', 'cash_flow_type', '_created_at_utc', 'adjustment', 'data_room_id', 'created_by_user_id', 'cash_flow_subtype', 'currency', '_revision_id', 'cash_flow', 'cash_flow_date', 'deal_revision_id', 'deal_id', 'data_room_name']\n",
      "Wrote 14,289 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/deal_cash_flow.csv\n",
      "INIT (deal_cash_flow id seed) check passed. Shape: (14289, 1)\n",
      "Added columns from 'deal_cash_flow': ['cash_flow_type', 'currency', 'cash_flow', 'cash_flow_date', 'deal_id']. Wrote 14,289 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/deal_cash_flow.csv.\n",
      "ADD_COLUMNS (deal_cash_flow) check passed. Shape: (14289, 6)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:54:05.101355Z",
     "start_time": "2025-10-13T19:53:59.318203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- Build fund_cash_flow.csv (init + add columns) ---------- #\n",
    "TARGET_DIR = (find_upwards(Path(\"GrossToNet\")) / \"Data\")\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TARGET_CSV = TARGET_DIR / \"fund_cash_flow.csv\"\n",
    "\n",
    "SHEET_FUND_CF = \"fund_cash_flow\"\n",
    "assert SHEET_FUND_CF in sheets, f\"'{SHEET_FUND_CF}' not found. Available sheets: {sheets}\"\n",
    "\n",
    "# Init with id\n",
    "raw_id = pd.read_excel(INPUT_XLSX, sheet_name=SHEET_FUND_CF, usecols=[\"id\"], dtype={\"id\": str})\n",
    "df_init = raw_id[[\"id\"]]\n",
    "df_init.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"[fund_cf] Wrote {len(df_init):,} id rows to {TARGET_CSV}\")\n",
    "\n",
    "# Verify init\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert list(check.columns) == [\"id\"], f\"Unexpected columns: {list(check.columns)}\"\n",
    "assert len(check) == len(df_init), \"Row count changed at init.\"\n",
    "assert check[\"id\"].tolist() == df_init[\"id\"].tolist(), \"Order changed at init.\"\n",
    "assert check[\"id\"].notna().all(), \"Null id at init.\"\n",
    "assert not check[\"id\"].duplicated().any(), \"Duplicate id at init.\"\n",
    "print(\"[fund_cf] INIT check passed.\", check.shape)\n",
    "\n",
    "# Add columns from fund_cash_flow\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "requested = [\"cash_flow_type\", \"fund_id\", \"cash_flow\", \"cash_flow_date\"]\n",
    "\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=SHEET_FUND_CF,\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Ensure one row per id\n",
    "assert src[\"id\"].is_unique, \"fund_cash_flow: duplicate id values would explode rows on merge.\"\n",
    "\n",
    "# Normalize cash_flow_date (handle strings and Excel serials)\n",
    "if \"cash_flow_date\" in src.columns:\n",
    "    s = src[\"cash_flow_date\"]\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    ser = pd.to_numeric(s, errors=\"coerce\")\n",
    "    is_serialish = (dt.isna() & ser.gt(20000)).mean() > 0.5\n",
    "    if is_serialish:\n",
    "        dt = pd.to_datetime(ser, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    src[\"cash_flow_date\"] = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]]\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"[fund_cf] Added columns: {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "# Post-merge checks\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert len(after) == len(working), \"Row count changed after merge.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed after merge.\"\n",
    "missing = [c for c in to_add if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "_ = pd.to_datetime(after[\"cash_flow_date\"], errors=\"coerce\")\n",
    "print(\"[fund_cf] ADD_COLUMNS check passed. Shape:\", after.shape)\n",
    "\n",
    "# ---------- Enrich fund_cash_flow.csv with fund_currency from fund sheet ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"GrossToNet\")) / \"Data\" / \"fund_cash_flow.csv\")\n",
    "\n",
    "# Load current fund_cash_flow\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"fund_id\": str})\n",
    "\n",
    "# Pull mapping: fund.id -> fund.reported_currency\n",
    "fund_map = (\n",
    "    pd.read_excel(\n",
    "        INPUT_XLSX,\n",
    "        sheet_name=\"fund\",\n",
    "        usecols=[\"id\", \"reported_currency\"],\n",
    "        dtype={\"id\": str},\n",
    "    )\n",
    "    .rename(columns={\"id\": \"fund_id\", \"reported_currency\": \"fund_currency\"})\n",
    ")\n",
    "\n",
    "# Robustness: ensure one row per fund_id in mapping\n",
    "assert fund_map[\"fund_id\"].notna().all(), \"Null fund_id in fund sheet.\"\n",
    "assert fund_map[\"fund_id\"].is_unique, \"Duplicate fund_id in fund sheet; cannot map currency uniquely.\"\n",
    "\n",
    "# Normalize currency text\n",
    "fund_map[\"fund_currency\"] = fund_map[\"fund_currency\"].astype(str).str.strip().str.upper()\n",
    "fund_map.loc[fund_map[\"fund_currency\"].isin([\"\", \"NAN\", \"NONE\"]), \"fund_currency\"] = pd.NA\n",
    "\n",
    "# Merge while preserving row order\n",
    "to_add = [\"fund_currency\"]\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = (\n",
    "    working.merge(fund_map[[\"fund_id\", \"fund_currency\"]], on=\"fund_id\", how=\"left\")\n",
    "           .sort_values(\"_ord\")\n",
    "           .drop(columns=\"_ord\")\n",
    ")\n",
    "\n",
    "# Write back\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"[fund_cf] Added column: {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "# -------- QA -------- #\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"fund_id\": str})\n",
    "\n",
    "# 1) Column exists\n",
    "assert \"fund_currency\" in after.columns, \"fund_currency missing after merge.\"\n",
    "\n",
    "# 2) Coverage: all non-null fund_id should have a currency\n",
    "missing_cur = after[after[\"fund_id\"].notna() & after[\"fund_id\"].astype(str).str.strip().ne(\"\") & after[\"fund_currency\"].isna()]\n",
    "assert missing_cur.empty, f\"Missing fund_currency for some fund_id values:\\n{missing_cur['fund_id'].value_counts().head()}\"\n",
    "\n",
    "# 3) Basic validity: currency looks like ISO code (3 letters)\n",
    "non_iso = after[\"fund_currency\"].dropna().astype(str).str.fullmatch(r\"[A-Z]{3}\") == False\n",
    "assert (~non_iso).all(), \"Some fund_currency values are not 3-letter codes.\"\n",
    "\n",
    "# 4) Order preserved and row count unchanged\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Row order changed.\"\n",
    "\n",
    "# Summary\n",
    "print(\"[fund_cf] Currency coverage:\",\n",
    "      int(after[\"fund_currency\"].notna().sum()), \"/\", len(after))\n",
    "print(\"[fund_cf] Unique currencies:\", sorted(after[\"fund_currency\"].dropna().unique().tolist()))\n"
   ],
   "id": "df8c842bbb3ac6d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fund_cf] Wrote 7,563 id rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/fund_cash_flow.csv\n",
      "[fund_cf] INIT check passed. (7563, 1)\n",
      "[fund_cf] Added columns: ['cash_flow_type', 'fund_id', 'cash_flow', 'cash_flow_date']. Wrote 7,563 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/fund_cash_flow.csv.\n",
      "[fund_cf] ADD_COLUMNS check passed. Shape: (7563, 5)\n",
      "[fund_cf] Added column: ['fund_currency']. Wrote 7,563 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/fund_cash_flow.csv.\n",
      "[fund_cf] Currency coverage: 7563 / 7563\n",
      "[fund_cf] Unique currencies: ['DKK', 'EUR', 'GBP', 'SEK', 'USD']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:54:05.230993Z",
     "start_time": "2025-10-13T19:54:05.105864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- Clean cash-flow CSVs by group-level rules + QA ---------- #\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = (find_upwards(Path(\"GrossToNet\")) / \"Data\")\n",
    "DEALS_CSV = DATA_DIR / \"deal_cash_flow.csv\"\n",
    "FUNDS_CSV = DATA_DIR / \"fund_cash_flow.csv\"\n",
    "\n",
    "# Dates\n",
    "MIN_DATE = pd.Timestamp(\"1976-01-01\")\n",
    "TODAY    = pd.Timestamp.today().normalize()  # uses system local time\n",
    "\n",
    "# Allowed types\n",
    "ALLOWED_DEAL_TYPES = {\"investment\", \"proceed\", \"fair value\"}\n",
    "ALLOWED_FUND_TYPES = {\"contribution\", \"distribution\", \"nav\"}\n",
    "\n",
    "# --- Load ---\n",
    "deals = pd.read_csv(DEALS_CSV, dtype={\"id\": str, \"deal_id\": str, \"currency\": str})\n",
    "funds = pd.read_csv(FUNDS_CSV, dtype={\"id\": str, \"fund_id\": str})\n",
    "\n",
    "# Parse dates (tolerate bad values; we’ll filter them)\n",
    "deals[\"cash_flow_date\"] = pd.to_datetime(deals[\"cash_flow_date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "funds[\"cash_flow_date\"] = pd.to_datetime(funds[\"cash_flow_date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# --- Row-level invalidity flags (then escalate to group-level) ---\n",
    "# Deals: drop entire deal_id if ANY row violates\n",
    "deal_type_std = deals[\"cash_flow_type\"].astype(str).str.strip().str.lower()\n",
    "deal_invalid_type = ~deal_type_std.isin(ALLOWED_DEAL_TYPES)\n",
    "deal_missing_required = (\n",
    "    deals[\"currency\"].isna()\n",
    "    | deals[\"cash_flow\"].isna()\n",
    "    | deals[\"cash_flow_date\"].isna()\n",
    ")\n",
    "deal_invalid_date = (~deals[\"cash_flow_date\"].between(MIN_DATE, TODAY)) | deals[\"cash_flow_date\"].isna()\n",
    "deal_missing_deal_id = deals[\"deal_id\"].isna() | (deals[\"deal_id\"].astype(str).str.strip() == \"\")\n",
    "\n",
    "# --- Early-history cutoff for deals (drop whole deal_id group if any row < 1992-07-01) ---\n",
    "CUTOFF = pd.Timestamp(\"1992-07-01\")\n",
    "\n",
    "assert pd.api.types.is_datetime64_any_dtype(deals[\"cash_flow_date\"]), \"deals.cash_flow_date must be datetime\"\n",
    "pre_mask = deals[\"cash_flow_date\"] < CUTOFF\n",
    "early_deal_ids = deals.loc[pre_mask, \"deal_id\"].dropna().unique().tolist()\n",
    "\n",
    "if early_deal_ids:\n",
    "    before_rows   = len(deals)\n",
    "    before_groups = deals[\"deal_id\"].nunique(dropna=False)\n",
    "    deals = deals[~deals[\"deal_id\"].isin(early_deal_ids)].copy()\n",
    "    after_rows    = len(deals)\n",
    "    after_groups  = deals[\"deal_id\"].nunique(dropna=False)\n",
    "    print(f\"[CUTOFF] Removed {len(early_deal_ids)} deal_id groups with any date < {CUTOFF.date()} \"\n",
    "          f\"({before_groups} -> {after_groups}); rows {before_rows} -> {after_rows}.\")\n",
    "else:\n",
    "    print(f\"[CUTOFF] No deal_id groups with dates < {CUTOFF.date()} found.\")\n",
    "\n",
    "deals[\"_row_invalid\"] = (\n",
    "    deal_invalid_type\n",
    "    | deal_missing_required\n",
    "    | deal_invalid_date\n",
    "    | deal_missing_deal_id\n",
    ")\n",
    "\n",
    "# Funds: drop entire fund_id if ANY row violates\n",
    "fund_type_std = funds[\"cash_flow_type\"].astype(str).str.strip().str.lower()\n",
    "fund_invalid_type = ~fund_type_std.isin(ALLOWED_FUND_TYPES)\n",
    "fund_invalid_date = (~funds[\"cash_flow_date\"].between(MIN_DATE, TODAY)) | funds[\"cash_flow_date\"].isna()\n",
    "fund_missing_fund_id = funds[\"fund_id\"].isna() | (funds[\"fund_id\"].astype(str).str.strip() == \"\")\n",
    "\n",
    "# --- Early-history cutoff for funds (drop whole fund_id group if any row < 1992-07-01) ---\n",
    "CUTOFF = pd.Timestamp(\"1992-07-01\")  # reuse if already defined above\n",
    "\n",
    "assert pd.api.types.is_datetime64_any_dtype(funds[\"cash_flow_date\"]), \"funds.cash_flow_date must be datetime\"\n",
    "pre_mask_f = funds[\"cash_flow_date\"] < CUTOFF\n",
    "early_fund_ids = funds.loc[pre_mask_f, \"fund_id\"].dropna().unique().tolist()\n",
    "\n",
    "if early_fund_ids:\n",
    "    f_before_rows   = len(funds)\n",
    "    f_before_groups = funds[\"fund_id\"].nunique(dropna=False)\n",
    "    funds = funds[~funds[\"fund_id\"].isin(early_fund_ids)].copy()\n",
    "    f_after_rows    = len(funds)\n",
    "    f_after_groups  = funds[\"fund_id\"].nunique(dropna=False)\n",
    "    print(f\"[CUTOFF] Funds: removed {len(early_fund_ids)} fund_id groups with any date < {CUTOFF.date()} \"\n",
    "          f\"({f_before_groups} -> {f_after_groups}); rows {f_before_rows} -> {f_after_rows}.\")\n",
    "else:\n",
    "    print(f\"[CUTOFF] Funds: no fund_id groups with dates < {CUTOFF.date()} found.\")\n",
    "\n",
    "funds[\"_row_invalid\"] = fund_invalid_type | fund_invalid_date | fund_missing_fund_id\n",
    "\n",
    "# --- Escalate to group-level removal ---\n",
    "# Deals\n",
    "invalid_deal_ids = (\n",
    "    deals.groupby(\"deal_id\", dropna=False)[\"_row_invalid\"]\n",
    "    .any()\n",
    "    .pipe(lambda s: s[s].index.tolist())\n",
    ")\n",
    "deals_before_rows = len(deals)\n",
    "deals_before_groups = deals[\"deal_id\"].nunique(dropna=False)\n",
    "\n",
    "deals = deals[~deals[\"deal_id\"].isin(invalid_deal_ids)].copy()\n",
    "\n",
    "# Funds\n",
    "invalid_fund_ids = (\n",
    "    funds.groupby(\"fund_id\", dropna=False)[\"_row_invalid\"]\n",
    "    .any()\n",
    "    .pipe(lambda s: s[s].index.tolist())\n",
    ")\n",
    "funds_before_rows = len(funds)\n",
    "funds_before_groups = funds[\"fund_id\"].nunique(dropna=False)\n",
    "\n",
    "funds = funds[~funds[\"fund_id\"].isin(invalid_fund_ids)].copy()\n",
    "\n",
    "# Drop helper flags\n",
    "for df in (deals, funds):\n",
    "    if \"_row_invalid\" in df.columns:\n",
    "        df.drop(columns=[\"_row_invalid\"], inplace=True)\n",
    "\n",
    "# --- Write back cleaned CSVs ---\n",
    "deals.to_csv(DEALS_CSV, index=False)\n",
    "funds.to_csv(FUNDS_CSV, index=False)\n",
    "\n",
    "# --- QA checks and summaries ---\n",
    "# Assert remaining types are allowed\n",
    "assert set(deals[\"cash_flow_type\"].astype(str).str.strip().str.lower().unique()).issubset(ALLOWED_DEAL_TYPES), \\\n",
    "    \"Unexpected deal cash_flow_type after cleaning.\"\n",
    "assert set(funds[\"cash_flow_type\"].astype(str).str.strip().str.lower().unique()).issubset(ALLOWED_FUND_TYPES), \\\n",
    "    \"Unexpected fund cash_flow_type after cleaning.\"\n",
    "\n",
    "# Assert required fields present and in range\n",
    "assert deals[\"currency\"].notna().all(), \"Missing currency in deals after cleaning.\"\n",
    "assert deals[\"cash_flow\"].notna().all(), \"Missing cash_flow in deals after cleaning.\"\n",
    "assert deals[\"cash_flow_date\"].notna().all(), \"Missing cash_flow_date in deals after cleaning.\"\n",
    "assert deals[\"cash_flow_date\"].between(MIN_DATE, TODAY).all(), \"Deals dates out of range after cleaning.\"\n",
    "assert funds[\"cash_flow_date\"].notna().all(), \"Missing cash_flow_date in funds after cleaning.\"\n",
    "assert funds[\"cash_flow_date\"].between(MIN_DATE, TODAY).all(), \"Funds dates out of range after cleaning.\"\n",
    "\n",
    "# Print removals\n",
    "print(\"=== Removals ===\")\n",
    "print(f\"Deals: removed {len(invalid_deal_ids)} deal_id groups \"\n",
    "      f\"({deals_before_groups} -> {deals['deal_id'].nunique(dropna=False)}); \"\n",
    "      f\"rows {deals_before_rows} -> {len(deals)}.\")\n",
    "if invalid_deal_ids:\n",
    "    print(\"Sample removed deal_ids:\", invalid_deal_ids[:10])\n",
    "\n",
    "print(f\"Funds: removed {len(invalid_fund_ids)} fund_id groups \"\n",
    "      f\"({funds_before_groups} -> {funds['fund_id'].nunique(dropna=False)}); \"\n",
    "      f\"rows {funds_before_rows} -> {len(funds)}.\")\n",
    "if invalid_fund_ids:\n",
    "    print(\"Sample removed fund_ids:\", invalid_fund_ids[:10])\n",
    "\n",
    "# Counts by type\n",
    "deal_type_counts = deals[\"cash_flow_type\"].str.strip().str.lower().value_counts()\n",
    "fund_type_counts = funds[\"cash_flow_type\"].str.strip().str.lower().value_counts()\n",
    "\n",
    "print(\"\\n=== Type counts (deals) ===\")\n",
    "print(deal_type_counts.to_string())\n",
    "print(\"=== Type counts (funds) ===\")\n",
    "print(fund_type_counts.to_string())\n",
    "\n",
    "# Date ranges\n",
    "def safe_range(s: pd.Series):\n",
    "    if s.empty:\n",
    "        return (None, None)\n",
    "    return (s.min(), s.max())\n",
    "\n",
    "dmin, dmax = safe_range(deals[\"cash_flow_date\"])\n",
    "fmin, fmax = safe_range(funds[\"cash_flow_date\"])\n",
    "print(\"\\n=== Date ranges ===\")\n",
    "print(f\"Deals: {dmin} .. {dmax}\")\n",
    "print(f\"Funds: {fmin} .. {fmax}\")\n",
    "\n",
    "# Additional sanity\n",
    "print(\"\\n=== Sanity ===\")\n",
    "print({\"deal_rows\": len(deals), \"deal_groups\": deals['deal_id'].nunique(),\n",
    "       \"fund_rows\": len(funds), \"fund_groups\": funds['fund_id'].nunique()})\n",
    "\n",
    "#TODO: Remove fund_ids that have no contributions.\n",
    "#TODO: Remove fund_ids where sum(contributions) < 0.5*sum(distributions) and no NAV in the fund_id group.\n",
    "#TODO: NAV sanity: Check if there is more than one NAV/Fair Value row per fund_id/deal_id and remove all but the latest NAV snapshot. Also then look at code in 02 and remove the block!\n",
    "\n"
   ],
   "id": "45593b8c4cc85cf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUTOFF] Removed 6 deal_id groups with any date < 1992-07-01 (1261 -> 1255); rows 14289 -> 14209.\n",
      "[CUTOFF] Funds: removed 1 fund_id groups with any date < 1992-07-01 (145 -> 144); rows 7563 -> 7533.\n",
      "=== Removals ===\n",
      "Deals: removed 277 deal_id groups (1255 -> 978); rows 14209 -> 11992.\n",
      "Sample removed deal_ids: ['0178d460-b750-4b65-944a-35dd5647a51f', '0222e065-cd3d-4d0c-8bc9-266adf7a2054', '025a601e-f0a4-42a0-b579-8572833bf692', '036f81c2-d206-4134-9e90-ba2cbf5bdb45', '03e00158-66c9-4113-8ba8-ab827818aee5', '06e56493-ab4c-4746-b6fa-f0786a52af98', '0a086ab3-b0ce-4e60-9877-096eaf468422', '0a42cf0a-f224-4c25-ac9c-510e44466db4', '0ab3dea2-ceb2-4f3b-afdc-6f69b1eea789', '0b205007-0b03-4ab9-9549-3734b4b559fa']\n",
      "Funds: removed 3 fund_id groups (144 -> 141); rows 7533 -> 7300.\n",
      "Sample removed fund_ids: ['48b60287-d48d-4b8a-ae4c-7ebcedc99707', 'e2a37bbd-7670-4202-9cc8-81c0822ce08c', 'f6854930-2707-4825-958b-39a3eba5cf4f']\n",
      "\n",
      "=== Type counts (deals) ===\n",
      "cash_flow_type\n",
      "proceed       5977\n",
      "investment    5605\n",
      "fair value     410\n",
      "=== Type counts (funds) ===\n",
      "cash_flow_type\n",
      "contribution    4597\n",
      "distribution    2583\n",
      "nav              120\n",
      "\n",
      "=== Date ranges ===\n",
      "Deals: 1992-07-02 00:00:00 .. 2025-04-01 00:00:00\n",
      "Funds: 1994-04-06 00:00:00 .. 2025-03-31 00:00:00\n",
      "\n",
      "=== Sanity ===\n",
      "{'deal_rows': 11992, 'deal_groups': 978, 'fund_rows': 7300, 'fund_groups': 141}\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
