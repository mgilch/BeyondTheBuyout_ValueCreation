{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-19T17:18:29.849594Z",
     "start_time": "2025-10-19T17:18:18.826746Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Helpers ---------- #\n",
    "REL_PATH = Path(\"InputData/CoreData.xlsx\")\n",
    "\n",
    "def find_upwards(rel_path: Path, max_up: int = 8) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for parent in [here, *here.parents][: max_up + 1]:\n",
    "        candidate = (parent / rel_path)\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't locate '{rel_path.as_posix()}' from {here} by walking up {max_up} levels.\\n\"\n",
    "        f\"- Current working directory: {here}\\n\"\n",
    "        f\"- Checked: {[str((p / rel_path)) for p in [here, *here.parents][: max_up + 1]]}\"\n",
    "    )\n",
    "\n",
    "# ---------- Step 1: Read Excel / basic checks ---------- #\n",
    "INPUT_XLSX = find_upwards(REL_PATH)\n",
    "xfile = pd.ExcelFile(INPUT_XLSX)\n",
    "sheets = xfile.sheet_names\n",
    "print(\"Resolved path:\", INPUT_XLSX)\n",
    "print(\"Sheets:\", sheets)\n",
    "\n",
    "assert isinstance(sheets, list) and sheets, \"Sheet names missing.\"\n",
    "assert all(isinstance(s, str) and s.strip() for s in sheets), \"Invalid sheet name(s).\"\n",
    "assert len(sheets) == len(set(sheets)), \"Duplicate sheet names detected.\"\n",
    "\n",
    "SHEET = \"deal_cash_flow\"\n",
    "assert SHEET in sheets, f\"'{SHEET}' not found. Available sheets: {sheets}\"\n",
    "\n",
    "# Load only the header first to inspect columns safely\n",
    "probe = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, nrows=0)\n",
    "cols = list(probe.columns)\n",
    "print(\"Columns:\", cols)\n",
    "assert \"id\" in cols, f\"'id' column not found in '{SHEET}'.\"\n",
    "\n",
    "# ---------- Step 2: Initialize GrossToNet/Data/deal_cash_flow.csv with id ---------- #\n",
    "TARGET_DIR = (find_upwards(Path(\"GrossToNet\")) / \"Data\")\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TARGET_CSV = TARGET_DIR / \"deal_cash_flow.csv\"\n",
    "\n",
    "raw = pd.read_excel(INPUT_XLSX, sheet_name=SHEET, usecols=[\"id\"], dtype={\"id\": str})\n",
    "df = raw[[\"id\"]]\n",
    "df.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Wrote {len(df):,} rows to {TARGET_CSV}\")\n",
    "\n",
    "# ---------- Step 3: Integrity checks on the written CSV ---------- #\n",
    "assert TARGET_CSV.exists(), f\"Missing output: {TARGET_CSV}\"\n",
    "check_df = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "assert list(check_df.columns) == [\"id\"], f\"Unexpected columns: {list(check_df.columns)}\"\n",
    "assert len(check_df) == len(raw), f\"Row count changed: raw={len(raw)} vs written={len(check_df)}\"\n",
    "assert check_df[\"id\"].tolist() == raw[\"id\"].tolist(), \"Row order changed.\"\n",
    "assert check_df[\"id\"].notna().all(), \"Null id found.\"\n",
    "assert not check_df[\"id\"].duplicated().any(), \"Duplicate id values found.\"\n",
    "\n",
    "print(\"INIT (deal_cash_flow id seed) check passed. Shape:\", check_df.shape)\n",
    "\n",
    "# ---------- Step 3: Add columns from deal_cash_flow ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"GrossToNet\")) / \"Data\" / \"deal_cash_flow.csv\")\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "\n",
    "requested = [\"cash_flow_type\", \"currency\", \"cash_flow\", \"cash_flow_date\", \"deal_id\"]\n",
    "\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=\"deal_cash_flow\",\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Ensure one row per id to avoid row-multiplying merges.\n",
    "assert src[\"id\"].is_unique, \"deal_cash_flow: duplicate id values would explode rows on merge.\"\n",
    "\n",
    "# Normalize cash_flow_date if present (handle Excel serials and strings).\n",
    "if \"cash_flow_date\" in src.columns:\n",
    "    s = src[\"cash_flow_date\"]\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    ser = pd.to_numeric(s, errors=\"coerce\")\n",
    "    # Heuristic: many Excel serials are > 20000\n",
    "    is_serialish = (dt.isna() & ser.gt(20000)).mean() > 0.5\n",
    "    if is_serialish:\n",
    "        dt = pd.to_datetime(ser, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    src[\"cash_flow_date\"] = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]]\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"Added columns from 'deal_cash_flow': {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "# Post-merge checks\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed.\"\n",
    "missing = [c for c in to_add if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "_ = pd.to_datetime(after[\"cash_flow_date\"], errors=\"coerce\")  # smoke-check\n",
    "print(\"ADD_COLUMNS (deal_cash_flow) check passed. Shape:\", after.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved path: /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/InputData/CoreData.xlsx\n",
      "Sheets: ['general_partner', 'fund', 'fund_cash_flow', 'capital_account', 'deal', 'deal_time_series', 'deal_cash_flow', 'deal_partner', 'deal_acquirer', 'deal_vendor', 'organization', 'person']\n",
      "Columns: ['id', 'deal_revision_id', 'cash_flow_date', 'cash_flow', 'currency', 'adjustment', 'notes', 'cash_flow_type', 'cash_flow_subtype', 'deal_id', 'created_by_user_id', '_created_at_utc', '_revision_id', 'data_room_id']\n",
      "Wrote 15,770 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/deal_cash_flow.csv\n",
      "INIT (deal_cash_flow id seed) check passed. Shape: (15770, 1)\n",
      "Added columns from 'deal_cash_flow': ['cash_flow_type', 'currency', 'cash_flow', 'cash_flow_date', 'deal_id']. Wrote 15,770 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/deal_cash_flow.csv.\n",
      "ADD_COLUMNS (deal_cash_flow) check passed. Shape: (15770, 6)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T17:18:32.241745Z",
     "start_time": "2025-10-19T17:18:29.855122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- Build fund_cash_flow.csv (init + add columns) ---------- #\n",
    "TARGET_DIR = (find_upwards(Path(\"GrossToNet\")) / \"Data\")\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TARGET_CSV = TARGET_DIR / \"fund_cash_flow.csv\"\n",
    "\n",
    "SHEET_FUND_CF = \"fund_cash_flow\"\n",
    "assert SHEET_FUND_CF in sheets, f\"'{SHEET_FUND_CF}' not found. Available sheets: {sheets}\"\n",
    "\n",
    "# Init with id\n",
    "raw_id = pd.read_excel(INPUT_XLSX, sheet_name=SHEET_FUND_CF, usecols=[\"id\"], dtype={\"id\": str})\n",
    "df_init = raw_id[[\"id\"]]\n",
    "df_init.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"[fund_cf] Wrote {len(df_init):,} id rows to {TARGET_CSV}\")\n",
    "\n",
    "# Verify init\n",
    "check = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert list(check.columns) == [\"id\"], f\"Unexpected columns: {list(check.columns)}\"\n",
    "assert len(check) == len(df_init), \"Row count changed at init.\"\n",
    "assert check[\"id\"].tolist() == df_init[\"id\"].tolist(), \"Order changed at init.\"\n",
    "assert check[\"id\"].notna().all(), \"Null id at init.\"\n",
    "assert not check[\"id\"].duplicated().any(), \"Duplicate id at init.\"\n",
    "print(\"[fund_cf] INIT check passed.\", check.shape)\n",
    "\n",
    "# Add columns from fund_cash_flow\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "requested = [\"cash_flow_type\", \"fund_id\", \"cash_flow\", \"cash_flow_date\"]\n",
    "\n",
    "src = pd.read_excel(\n",
    "    INPUT_XLSX,\n",
    "    sheet_name=SHEET_FUND_CF,\n",
    "    usecols=[\"id\", *requested],\n",
    "    dtype={\"id\": str},\n",
    ")\n",
    "\n",
    "# Ensure one row per id\n",
    "assert src[\"id\"].is_unique, \"fund_cash_flow: duplicate id values would explode rows on merge.\"\n",
    "\n",
    "# Normalize cash_flow_date (handle strings and Excel serials)\n",
    "if \"cash_flow_date\" in src.columns:\n",
    "    s = src[\"cash_flow_date\"]\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    ser = pd.to_numeric(s, errors=\"coerce\")\n",
    "    is_serialish = (dt.isna() & ser.gt(20000)).mean() > 0.5\n",
    "    if is_serialish:\n",
    "        dt = pd.to_datetime(ser, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    src[\"cash_flow_date\"] = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "to_add = [c for c in requested if c not in working.columns]\n",
    "src = src[[\"id\", *to_add]]\n",
    "\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = working.merge(src, on=\"id\", how=\"left\")\n",
    "out = out.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"[fund_cf] Added columns: {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "# Post-merge checks\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str})\n",
    "assert len(after) == len(working), \"Row count changed after merge.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Order changed after merge.\"\n",
    "missing = [c for c in to_add if c not in after.columns]\n",
    "assert not missing, f\"Missing columns after merge: {missing}\"\n",
    "_ = pd.to_datetime(after[\"cash_flow_date\"], errors=\"coerce\")\n",
    "print(\"[fund_cf] ADD_COLUMNS check passed. Shape:\", after.shape)\n",
    "\n",
    "# ---------- Enrich fund_cash_flow.csv with fund_currency from fund sheet ---------- #\n",
    "TARGET_CSV = (find_upwards(Path(\"GrossToNet\")) / \"Data\" / \"fund_cash_flow.csv\")\n",
    "\n",
    "# Load current fund_cash_flow\n",
    "working = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"fund_id\": str})\n",
    "\n",
    "# Pull mapping: fund.id -> fund.reported_currency\n",
    "fund_map = (\n",
    "    pd.read_excel(\n",
    "        INPUT_XLSX,\n",
    "        sheet_name=\"fund\",\n",
    "        usecols=[\"id\", \"reported_currency\"],\n",
    "        dtype={\"id\": str},\n",
    "    )\n",
    "    .rename(columns={\"id\": \"fund_id\", \"reported_currency\": \"fund_currency\"})\n",
    ")\n",
    "\n",
    "# Robustness: ensure one row per fund_id in mapping\n",
    "assert fund_map[\"fund_id\"].notna().all(), \"Null fund_id in fund sheet.\"\n",
    "assert fund_map[\"fund_id\"].is_unique, \"Duplicate fund_id in fund sheet; cannot map currency uniquely.\"\n",
    "\n",
    "# Normalize currency text\n",
    "fund_map[\"fund_currency\"] = fund_map[\"fund_currency\"].astype(str).str.strip().str.upper()\n",
    "fund_map.loc[fund_map[\"fund_currency\"].isin([\"\", \"NAN\", \"NONE\"]), \"fund_currency\"] = pd.NA\n",
    "\n",
    "# Merge while preserving row order\n",
    "to_add = [\"fund_currency\"]\n",
    "working[\"_ord\"] = np.arange(len(working))\n",
    "out = (\n",
    "    working.merge(fund_map[[\"fund_id\", \"fund_currency\"]], on=\"fund_id\", how=\"left\")\n",
    "           .sort_values(\"_ord\")\n",
    "           .drop(columns=\"_ord\")\n",
    ")\n",
    "\n",
    "# Write back\n",
    "out.to_csv(TARGET_CSV, index=False)\n",
    "print(f\"[fund_cf] Added column: {to_add}. Wrote {len(out):,} rows to {TARGET_CSV}.\")\n",
    "\n",
    "# -------- QA -------- #\n",
    "after = pd.read_csv(TARGET_CSV, dtype={\"id\": str, \"fund_id\": str})\n",
    "\n",
    "# 1) Column exists\n",
    "assert \"fund_currency\" in after.columns, \"fund_currency missing after merge.\"\n",
    "\n",
    "# 2) Coverage: all non-null fund_id should have a currency\n",
    "missing_cur = after[after[\"fund_id\"].notna() & after[\"fund_id\"].astype(str).str.strip().ne(\"\") & after[\"fund_currency\"].isna()]\n",
    "assert missing_cur.empty, f\"Missing fund_currency for some fund_id values:\\n{missing_cur['fund_id'].value_counts().head()}\"\n",
    "\n",
    "# 3) Basic validity: currency looks like ISO code (3 letters)\n",
    "non_iso = after[\"fund_currency\"].dropna().astype(str).str.fullmatch(r\"[A-Z]{3}\") == False\n",
    "assert (~non_iso).all(), \"Some fund_currency values are not 3-letter codes.\"\n",
    "\n",
    "# 4) Order preserved and row count unchanged\n",
    "assert len(after) == len(working), \"Row count changed.\"\n",
    "assert after[\"id\"].tolist() == working.sort_values(\"_ord\")[\"id\"].tolist(), \"Row order changed.\"\n",
    "\n",
    "# Summary\n",
    "print(\"[fund_cf] Currency coverage:\",\n",
    "      int(after[\"fund_currency\"].notna().sum()), \"/\", len(after))\n",
    "print(\"[fund_cf] Unique currencies:\", sorted(after[\"fund_currency\"].dropna().unique().tolist()))\n"
   ],
   "id": "df8c842bbb3ac6d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fund_cf] Wrote 7,986 id rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/fund_cash_flow.csv\n",
      "[fund_cf] INIT check passed. (7986, 1)\n",
      "[fund_cf] Added columns: ['cash_flow_type', 'fund_id', 'cash_flow', 'cash_flow_date']. Wrote 7,986 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/fund_cash_flow.csv.\n",
      "[fund_cf] ADD_COLUMNS check passed. Shape: (7986, 5)\n",
      "[fund_cf] Added column: ['fund_currency']. Wrote 7,986 rows to /Users/michael/Library/Mobile Documents/com~apple~CloudDocs/Studium TUM/Master Management and Technology/06 Master Thesis/00 Thesis/05Code/GrossToNet/Data/fund_cash_flow.csv.\n",
      "[fund_cf] Currency coverage: 7986 / 7986\n",
      "[fund_cf] Unique currencies: ['DKK', 'EUR', 'GBP', 'NOK', 'SEK', 'USD']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T17:18:33.249066Z",
     "start_time": "2025-10-19T17:18:32.245717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- Clean cash-flow CSVs by group-level rules + QA ---------- #\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = (find_upwards(Path(\"GrossToNet\")) / \"Data\")\n",
    "DEALS_CSV = DATA_DIR / \"deal_cash_flow.csv\"\n",
    "FUNDS_CSV = DATA_DIR / \"fund_cash_flow.csv\"\n",
    "\n",
    "# Dates\n",
    "MIN_DATE = pd.Timestamp(\"1976-01-01\")\n",
    "TODAY    = pd.Timestamp.today().normalize()  # uses system local time\n",
    "\n",
    "# Allowed types\n",
    "ALLOWED_DEAL_TYPES = {\"investment\", \"proceed\", \"fair value\"}\n",
    "ALLOWED_FUND_TYPES = {\"contribution\", \"distribution\", \"nav\"}\n",
    "\n",
    "# --- Load ---\n",
    "deals = pd.read_csv(DEALS_CSV, dtype={\"id\": str, \"deal_id\": str, \"currency\": str})\n",
    "funds = pd.read_csv(FUNDS_CSV, dtype={\"id\": str, \"fund_id\": str})\n",
    "\n",
    "# Parse dates (tolerate bad values; weâ€™ll filter them)\n",
    "deals[\"cash_flow_date\"] = pd.to_datetime(deals[\"cash_flow_date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "funds[\"cash_flow_date\"] = pd.to_datetime(funds[\"cash_flow_date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# ---------- Universe restriction via mapping (deal_to_fund.csv) ----------\n",
    "MAP_CSV = DATA_DIR / \"deal_to_fund.csv\"\n",
    "assert MAP_CSV.exists(), f\"Mapping file not found: {MAP_CSV}\"\n",
    "\n",
    "map_df = pd.read_csv(MAP_CSV, dtype={\"deal_id\": str, \"fund_id\": str})\n",
    "# normalize ids\n",
    "for c in (\"deal_id\", \"fund_id\"):\n",
    "    if c in map_df.columns:\n",
    "        map_df[c] = map_df[c].astype(str).str.strip()\n",
    "    else:\n",
    "        raise KeyError(f\"Mapping file is missing column: {c}\")\n",
    "\n",
    "allowed_deals = set(map_df[\"deal_id\"].dropna().unique().tolist())\n",
    "allowed_funds = set(map_df[\"fund_id\"].dropna().unique().tolist())\n",
    "\n",
    "# Filter deals to mapped deal_ids\n",
    "before_rows, before_groups = len(deals), deals[\"deal_id\"].nunique(dropna=False)\n",
    "deals = deals[deals[\"deal_id\"].isin(allowed_deals)].copy()\n",
    "after_rows, after_groups = len(deals), deals[\"deal_id\"].nunique(dropna=False)\n",
    "print(f\"[MAP] Deals: kept only mapped deal_ids ({before_groups} -> {after_groups}); rows {before_rows} -> {after_rows}.\")\n",
    "\n",
    "# Filter funds to mapped fund_ids\n",
    "f_before_rows, f_before_groups = len(funds), funds[\"fund_id\"].nunique(dropna=False)\n",
    "funds = funds[funds[\"fund_id\"].isin(allowed_funds)].copy()\n",
    "f_after_rows, f_after_groups = len(funds), funds[\"fund_id\"].nunique(dropna=False)\n",
    "print(f\"[MAP] Funds: kept only mapped fund_ids ({f_before_groups} -> {f_after_groups}); rows {f_before_rows} -> {f_after_rows}.\")\n",
    "\n",
    "\n",
    "# --- Row-level invalidity flags (then escalate to group-level) ---\n",
    "# Deals: drop entire deal_id if ANY row violates\n",
    "deal_type_std = deals[\"cash_flow_type\"].astype(str).str.strip().str.lower()\n",
    "deal_invalid_type = ~deal_type_std.isin(ALLOWED_DEAL_TYPES)\n",
    "deal_missing_required = (\n",
    "    deals[\"currency\"].isna()\n",
    "    | deals[\"cash_flow\"].isna()\n",
    "    | deals[\"cash_flow_date\"].isna()\n",
    ")\n",
    "deal_invalid_date = (~deals[\"cash_flow_date\"].between(MIN_DATE, TODAY)) | deals[\"cash_flow_date\"].isna()\n",
    "deal_missing_deal_id = deals[\"deal_id\"].isna() | (deals[\"deal_id\"].astype(str).str.strip() == \"\")\n",
    "\n",
    "# --- Early-history cutoff for deals (drop whole deal_id group if any row < 1992-07-01) ---\n",
    "CUTOFF = pd.Timestamp(\"1992-07-01\")\n",
    "\n",
    "assert pd.api.types.is_datetime64_any_dtype(deals[\"cash_flow_date\"]), \"deals.cash_flow_date must be datetime\"\n",
    "pre_mask = deals[\"cash_flow_date\"] < CUTOFF\n",
    "early_deal_ids = deals.loc[pre_mask, \"deal_id\"].dropna().unique().tolist()\n",
    "\n",
    "if early_deal_ids:\n",
    "    before_rows   = len(deals)\n",
    "    before_groups = deals[\"deal_id\"].nunique(dropna=False)\n",
    "    deals = deals[~deals[\"deal_id\"].isin(early_deal_ids)].copy()\n",
    "    after_rows    = len(deals)\n",
    "    after_groups  = deals[\"deal_id\"].nunique(dropna=False)\n",
    "    print(f\"[CUTOFF] Removed {len(early_deal_ids)} deal_id groups with any date < {CUTOFF.date()} \"\n",
    "          f\"({before_groups} -> {after_groups}); rows {before_rows} -> {after_rows}.\")\n",
    "else:\n",
    "    print(f\"[CUTOFF] No deal_id groups with dates < {CUTOFF.date()} found.\")\n",
    "\n",
    "deals[\"_row_invalid\"] = (\n",
    "    deal_invalid_type\n",
    "    | deal_missing_required\n",
    "    | deal_invalid_date\n",
    "    | deal_missing_deal_id\n",
    ")\n",
    "\n",
    "# Funds: drop entire fund_id if ANY row violates\n",
    "fund_type_std = funds[\"cash_flow_type\"].astype(str).str.strip().str.lower()\n",
    "fund_invalid_type = ~fund_type_std.isin(ALLOWED_FUND_TYPES)\n",
    "fund_invalid_date = (~funds[\"cash_flow_date\"].between(MIN_DATE, TODAY)) | funds[\"cash_flow_date\"].isna()\n",
    "fund_missing_fund_id = funds[\"fund_id\"].isna() | (funds[\"fund_id\"].astype(str).str.strip() == \"\")\n",
    "\n",
    "# --- Early-history cutoff for funds (drop whole fund_id group if any row < 1992-07-01) ---\n",
    "CUTOFF = pd.Timestamp(\"1992-07-01\")  # reuse if already defined above\n",
    "\n",
    "assert pd.api.types.is_datetime64_any_dtype(funds[\"cash_flow_date\"]), \"funds.cash_flow_date must be datetime\"\n",
    "pre_mask_f = funds[\"cash_flow_date\"] < CUTOFF\n",
    "early_fund_ids = funds.loc[pre_mask_f, \"fund_id\"].dropna().unique().tolist()\n",
    "\n",
    "if early_fund_ids:\n",
    "    f_before_rows   = len(funds)\n",
    "    f_before_groups = funds[\"fund_id\"].nunique(dropna=False)\n",
    "    funds = funds[~funds[\"fund_id\"].isin(early_fund_ids)].copy()\n",
    "    f_after_rows    = len(funds)\n",
    "    f_after_groups  = funds[\"fund_id\"].nunique(dropna=False)\n",
    "    print(f\"[CUTOFF] Funds: removed {len(early_fund_ids)} fund_id groups with any date < {CUTOFF.date()} \"\n",
    "          f\"({f_before_groups} -> {f_after_groups}); rows {f_before_rows} -> {f_after_rows}.\")\n",
    "else:\n",
    "    print(f\"[CUTOFF] Funds: no fund_id groups with dates < {CUTOFF.date()} found.\")\n",
    "\n",
    "funds[\"_row_invalid\"] = fund_invalid_type | fund_invalid_date | fund_missing_fund_id\n",
    "\n",
    "# ---------- Additional structural consistency rules (group-level) ---------- #\n",
    "# Assumes: deals, funds loaded; cash_flow_date parsed; type domain checked earlier.\n",
    "\n",
    "# Normalize type labels once for robust matching\n",
    "deals[\"_t\"] = deals[\"cash_flow_type\"].astype(str).str.strip().str.lower()\n",
    "funds[\"_t\"] = funds[\"cash_flow_type\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# --- [1] Remove groups with payouts/NAV but no capital-in (deals: no investment; funds: no contribution) --- #\n",
    "# Deals: (proceed or fair value) present AND no investment  -> drop deal_id group\n",
    "d_has_inv   = deals.groupby(\"deal_id\")[\"_t\"].apply(lambda s: (s == \"investment\").any())\n",
    "d_has_p_out = deals.groupby(\"deal_id\")[\"_t\"].apply(lambda s: s.isin([\"proceed\",\"fair value\"]).any())\n",
    "drop_deal_ids_1 = d_has_p_out & (~d_has_inv)\n",
    "drop_deal_ids_1 = drop_deal_ids_1[drop_deal_ids_1].index.tolist()\n",
    "\n",
    "# Funds: (distribution or nav) present AND no contribution  -> drop fund_id group\n",
    "f_has_contrib = funds.groupby(\"fund_id\")[\"_t\"].apply(lambda s: (s == \"contribution\").any())\n",
    "f_has_out_nav = funds.groupby(\"fund_id\")[\"_t\"].apply(lambda s: s.isin([\"distribution\",\"nav\"]).any())\n",
    "drop_fund_ids_1 = f_has_out_nav & (~f_has_contrib)\n",
    "drop_fund_ids_1 = drop_fund_ids_1[drop_fund_ids_1].index.tolist()\n",
    "\n",
    "if drop_deal_ids_1:\n",
    "    n_before = len(deals)\n",
    "    deals = deals[~deals[\"deal_id\"].isin(drop_deal_ids_1)].copy()\n",
    "    print(f\"[RULE1] Deals: removed {len(drop_deal_ids_1)} groups with proceeds/FV but no investment; rows {n_before} -> {len(deals)}.\")\n",
    "else:\n",
    "    print(\"[RULE1] Deals: no groups with payouts/FV but no investment.\")\n",
    "\n",
    "if drop_fund_ids_1:\n",
    "    n_before = len(funds)\n",
    "    funds = funds[~funds[\"fund_id\"].isin(drop_fund_ids_1)].copy()\n",
    "    print(f\"[RULE1] Funds: removed {len(drop_fund_ids_1)} groups with dists/NAV but no contribution; rows {n_before} -> {len(funds)}.\")\n",
    "else:\n",
    "    print(\"[RULE1] Funds: no groups with dists/NAV but no contribution.\")\n",
    "\n",
    "# --- [2] Remove weak-realization groups with no NAV/FV:\n",
    "#       |sum(payouts)| < 0.5 * |sum(contributions)| and NO NAV/FV present --- #\n",
    "\n",
    "def _abs_sum(series: pd.Series) -> float:\n",
    "    # treat sign conventions defensively; use signed sums then absolute value\n",
    "    return float(abs(series.fillna(0).sum()))\n",
    "\n",
    "# Deals: compare proceeds vs investments; require NO fair value in group\n",
    "def _deal_weak_realization(g: pd.DataFrame) -> bool:\n",
    "    has_fv   = (g[\"_t\"] == \"fair value\").any()\n",
    "    inv_abs  = _abs_sum(g.loc[g[\"_t\"] == \"investment\", \"cash_flow\"])\n",
    "    proc_abs = _abs_sum(g.loc[g[\"_t\"] == \"proceed\",    \"cash_flow\"])\n",
    "    if has_fv or inv_abs <= 0:\n",
    "        return False\n",
    "    return proc_abs < 0.5 * inv_abs\n",
    "\n",
    "weak_deal_ids = (\n",
    "    deals.groupby(\"deal_id\", group_keys=False)\n",
    "         .apply(_deal_weak_realization)\n",
    ")\n",
    "weak_deal_ids = weak_deal_ids[weak_deal_ids].index.tolist()\n",
    "\n",
    "# Funds: compare distributions vs contributions; require NO NAV in group\n",
    "def _fund_weak_realization(g: pd.DataFrame) -> bool:\n",
    "    has_nav  = (g[\"_t\"] == \"nav\").any()\n",
    "    ctr_abs  = _abs_sum(g.loc[g[\"_t\"] == \"contribution\", \"cash_flow\"])\n",
    "    dist_abs = _abs_sum(g.loc[g[\"_t\"] == \"distribution\", \"cash_flow\"])\n",
    "    if has_nav or ctr_abs <= 0:\n",
    "        return False\n",
    "    return dist_abs < 0.5 * ctr_abs\n",
    "\n",
    "weak_fund_ids = (\n",
    "    funds.groupby(\"fund_id\", group_keys=False)\n",
    "         .apply(_fund_weak_realization)\n",
    ")\n",
    "weak_fund_ids = weak_fund_ids[weak_fund_ids].index.tolist()\n",
    "\n",
    "if weak_deal_ids:\n",
    "    n_before = len(deals)\n",
    "    deals = deals[~deals[\"deal_id\"].isin(weak_deal_ids)].copy()\n",
    "    print(f\"[RULE2] Deals: removed {len(weak_deal_ids)} weak-realization groups (no FV and proceeds < 50% of investments); rows {n_before} -> {len(deals)}.\")\n",
    "else:\n",
    "    print(\"[RULE2] Deals: no weak-realization groups to remove.\")\n",
    "\n",
    "if weak_fund_ids:\n",
    "    n_before = len(funds)\n",
    "    funds = funds[~funds[\"fund_id\"].isin(weak_fund_ids)].copy()\n",
    "    print(f\"[RULE2] Funds: removed {len(weak_fund_ids)} weak-realization groups (no NAV and dists < 50% of contribs); rows {n_before} -> {len(funds)}.\")\n",
    "else:\n",
    "    print(\"[RULE2] Funds: no weak-realization groups to remove.\")\n",
    "\n",
    "# --- [3] NAV/Fair Value snapshot sanity: keep only the latest snapshot per group --- #\n",
    "# Funds: keep only the last NAV row per fund_id (by cash_flow_date)\n",
    "def _keep_latest_snapshot(df: pd.DataFrame, id_col: str, label: str) -> tuple[pd.DataFrame, int, int]:\n",
    "    # returns (filtered_df, groups_adjusted, rows_dropped)\n",
    "    grouped = df.groupby(id_col, group_keys=False)\n",
    "    adjusted = 0\n",
    "    dropped = 0\n",
    "    out_parts = []\n",
    "    for gid, g in grouped:\n",
    "        mask = (g[\"_t\"] == label)\n",
    "        if mask.sum() <= 1:\n",
    "            out_parts.append(g)\n",
    "            continue\n",
    "        adjusted += 1\n",
    "        # keep row with the max date among the snapshots\n",
    "        keep_idx = g.loc[mask, \"cash_flow_date\"].idxmax()\n",
    "        drop_idx = g.index[mask & (g.index != keep_idx)]\n",
    "        dropped += len(drop_idx)\n",
    "        out_parts.append(g.drop(index=drop_idx))\n",
    "    return pd.concat(out_parts, ignore_index=False), adjusted, dropped\n",
    "\n",
    "# Apply to funds (NAV) and deals (fair value)\n",
    "funds, f_adj, f_drop = _keep_latest_snapshot(funds, \"fund_id\", \"nav\")\n",
    "deals, d_adj, d_drop = _keep_latest_snapshot(deals, \"deal_id\", \"fair value\")\n",
    "\n",
    "print(f\"[RULE3] Funds: {f_adj} groups had multiple NAV; dropped {f_drop} older NAV rows (kept latest).\")\n",
    "print(f\"[RULE3] Deals: {d_adj} groups had multiple Fair Value; dropped {d_drop} older FV rows (kept latest).\")\n",
    "\n",
    "# --- [4] Timing check: remove any rows AFTER the NAV/Fair Value date within each group --- #\n",
    "def _trim_after_snapshot(df: pd.DataFrame, id_col: str, label: str) -> tuple[pd.DataFrame, int, int]:\n",
    "    grouped = df.groupby(id_col, group_keys=False)\n",
    "    adj = 0\n",
    "    removed = 0\n",
    "    out_parts = []\n",
    "    for gid, g in grouped:\n",
    "        snap_dates = g.loc[g[\"_t\"] == label, \"cash_flow_date\"]\n",
    "        if snap_dates.empty:\n",
    "            out_parts.append(g)\n",
    "            continue\n",
    "        snap_date = snap_dates.max()\n",
    "        keep = g[\"cash_flow_date\"] <= snap_date\n",
    "        if (~keep).any():\n",
    "            adj += 1\n",
    "            removed += int((~keep).sum())\n",
    "        out_parts.append(g.loc[keep])\n",
    "    return pd.concat(out_parts, ignore_index=False), adj, removed\n",
    "\n",
    "funds, f_adj2, f_removed2 = _trim_after_snapshot(funds, \"fund_id\", \"nav\")\n",
    "deals, d_adj2, d_removed2 = _trim_after_snapshot(deals, \"deal_id\", \"fair value\")\n",
    "\n",
    "print(f\"[RULE4] Funds: trimmed rows after NAV in {f_adj2} groups; removed {f_removed2} rows.\")\n",
    "print(f\"[RULE4] Deals: trimmed rows after Fair Value in {d_adj2} groups; removed {d_removed2} rows.\")\n",
    "\n",
    "# Drop helper type column used above\n",
    "for df in (deals, funds):\n",
    "    if \"_t\" in df.columns:\n",
    "        df.drop(columns=[\"_t\"], inplace=True)\n",
    "\n",
    "# --- Escalate to group-level removal ---\n",
    "# Deals\n",
    "invalid_deal_ids = (\n",
    "    deals.groupby(\"deal_id\", dropna=False)[\"_row_invalid\"]\n",
    "    .any()\n",
    "    .pipe(lambda s: s[s].index.tolist())\n",
    ")\n",
    "deals_before_rows = len(deals)\n",
    "deals_before_groups = deals[\"deal_id\"].nunique(dropna=False)\n",
    "\n",
    "deals = deals[~deals[\"deal_id\"].isin(invalid_deal_ids)].copy()\n",
    "\n",
    "# Funds\n",
    "invalid_fund_ids = (\n",
    "    funds.groupby(\"fund_id\", dropna=False)[\"_row_invalid\"]\n",
    "    .any()\n",
    "    .pipe(lambda s: s[s].index.tolist())\n",
    ")\n",
    "funds_before_rows = len(funds)\n",
    "funds_before_groups = funds[\"fund_id\"].nunique(dropna=False)\n",
    "\n",
    "funds = funds[~funds[\"fund_id\"].isin(invalid_fund_ids)].copy()\n",
    "\n",
    "# Drop helper flags\n",
    "for df in (deals, funds):\n",
    "    if \"_row_invalid\" in df.columns:\n",
    "        df.drop(columns=[\"_row_invalid\"], inplace=True)\n",
    "\n",
    "# --- Write back cleaned CSVs ---\n",
    "deals.to_csv(DEALS_CSV, index=False)\n",
    "funds.to_csv(FUNDS_CSV, index=False)\n",
    "\n",
    "# --- QA checks and summaries ---\n",
    "# Assert remaining types are allowed\n",
    "assert set(deals[\"cash_flow_type\"].astype(str).str.strip().str.lower().unique()).issubset(ALLOWED_DEAL_TYPES), \\\n",
    "    \"Unexpected deal cash_flow_type after cleaning.\"\n",
    "assert set(funds[\"cash_flow_type\"].astype(str).str.strip().str.lower().unique()).issubset(ALLOWED_FUND_TYPES), \\\n",
    "    \"Unexpected fund cash_flow_type after cleaning.\"\n",
    "\n",
    "# Assert required fields present and in range\n",
    "assert deals[\"currency\"].notna().all(), \"Missing currency in deals after cleaning.\"\n",
    "assert deals[\"cash_flow\"].notna().all(), \"Missing cash_flow in deals after cleaning.\"\n",
    "assert deals[\"cash_flow_date\"].notna().all(), \"Missing cash_flow_date in deals after cleaning.\"\n",
    "assert deals[\"cash_flow_date\"].between(MIN_DATE, TODAY).all(), \"Deals dates out of range after cleaning.\"\n",
    "assert funds[\"cash_flow_date\"].notna().all(), \"Missing cash_flow_date in funds after cleaning.\"\n",
    "assert funds[\"cash_flow_date\"].between(MIN_DATE, TODAY).all(), \"Funds dates out of range after cleaning.\"\n",
    "\n",
    "# Print removals\n",
    "print(\"=== Removals ===\")\n",
    "print(f\"Deals: removed {len(invalid_deal_ids)} deal_id groups \"\n",
    "      f\"({deals_before_groups} -> {deals['deal_id'].nunique(dropna=False)}); \"\n",
    "      f\"rows {deals_before_rows} -> {len(deals)}.\")\n",
    "if invalid_deal_ids:\n",
    "    print(\"Sample removed deal_ids:\", invalid_deal_ids[:10])\n",
    "\n",
    "print(f\"Funds: removed {len(invalid_fund_ids)} fund_id groups \"\n",
    "      f\"({funds_before_groups} -> {funds['fund_id'].nunique(dropna=False)}); \"\n",
    "      f\"rows {funds_before_rows} -> {len(funds)}.\")\n",
    "if invalid_fund_ids:\n",
    "    print(\"Sample removed fund_ids:\", invalid_fund_ids[:10])\n",
    "\n",
    "# Counts by type\n",
    "deal_type_counts = deals[\"cash_flow_type\"].str.strip().str.lower().value_counts()\n",
    "fund_type_counts = funds[\"cash_flow_type\"].str.strip().str.lower().value_counts()\n",
    "\n",
    "print(\"\\n=== Type counts (deals) ===\")\n",
    "print(deal_type_counts.to_string())\n",
    "print(\"=== Type counts (funds) ===\")\n",
    "print(fund_type_counts.to_string())\n",
    "\n",
    "# Date ranges\n",
    "def safe_range(s: pd.Series):\n",
    "    if s.empty:\n",
    "        return (None, None)\n",
    "    return (s.min(), s.max())\n",
    "\n",
    "dmin, dmax = safe_range(deals[\"cash_flow_date\"])\n",
    "fmin, fmax = safe_range(funds[\"cash_flow_date\"])\n",
    "print(\"\\n=== Date ranges ===\")\n",
    "print(f\"Deals: {dmin} .. {dmax}\")\n",
    "print(f\"Funds: {fmin} .. {fmax}\")\n",
    "\n",
    "# Additional sanity\n",
    "print(\"\\n=== Sanity ===\")\n",
    "print({\"deal_rows\": len(deals), \"deal_groups\": deals['deal_id'].nunique(),\n",
    "       \"fund_rows\": len(funds), \"fund_groups\": funds['fund_id'].nunique()})\n"
   ],
   "id": "45593b8c4cc85cf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAP] Deals: kept only mapped deal_ids (1599 -> 1559); rows 15770 -> 15491.\n",
      "[MAP] Funds: kept only mapped fund_ids (162 -> 138); rows 7986 -> 6030.\n",
      "[CUTOFF] Removed 6 deal_id groups with any date < 1992-07-01 (1559 -> 1553); rows 15491 -> 15411.\n",
      "[CUTOFF] Funds: removed 1 fund_id groups with any date < 1992-07-01 (138 -> 137); rows 6030 -> 6000.\n",
      "[RULE1] Deals: removed 2 groups with proceeds/FV but no investment; rows 15411 -> 15409.\n",
      "[RULE1] Funds: removed 3 groups with dists/NAV but no contribution; rows 6000 -> 5939.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/xj9tr7rx0hg2pt0j07wqkvcc0000gn/T/ipykernel_13979/1570991196.py:169: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_deal_weak_realization)\n",
      "/var/folders/qd/xj9tr7rx0hg2pt0j07wqkvcc0000gn/T/ipykernel_13979/1570991196.py:184: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_fund_weak_realization)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RULE2] Deals: removed 269 weak-realization groups (no FV and proceeds < 50% of investments); rows 15409 -> 13644.\n",
      "[RULE2] Funds: removed 8 weak-realization groups (no NAV and dists < 50% of contribs); rows 5939 -> 5643.\n",
      "[RULE3] Funds: 8 groups had multiple NAV; dropped 38 older NAV rows (kept latest).\n",
      "[RULE3] Deals: 51 groups had multiple Fair Value; dropped 161 older FV rows (kept latest).\n",
      "[RULE4] Funds: trimmed rows after NAV in 2 groups; removed 2 rows.\n",
      "[RULE4] Deals: trimmed rows after Fair Value in 5 groups; removed 6 rows.\n",
      "=== Removals ===\n",
      "Deals: removed 171 deal_id groups (1282 -> 1111); rows 13477 -> 11861.\n",
      "Sample removed deal_ids: ['0178d460-b750-4b65-944a-35dd5647a51f', '0222e065-cd3d-4d0c-8bc9-266adf7a2054', '025a601e-f0a4-42a0-b579-8572833bf692', '036f81c2-d206-4134-9e90-ba2cbf5bdb45', '0a42cf0a-f224-4c25-ac9c-510e44466db4', '0ab3dea2-ceb2-4f3b-afdc-6f69b1eea789', '0b205007-0b03-4ab9-9549-3734b4b559fa', '0d431c68-8712-43b6-a6ff-66be14647dd5', '0d4f39a4-304e-45c0-afae-e3851611ca7c', '0d82c37d-48dd-46c2-9834-61150dfbf995']\n",
      "Funds: removed 3 fund_id groups (126 -> 123); rows 5603 -> 5370.\n",
      "Sample removed fund_ids: ['48b60287-d48d-4b8a-ae4c-7ebcedc99707', 'e2a37bbd-7670-4202-9cc8-81c0822ce08c', 'f6854930-2707-4825-958b-39a3eba5cf4f']\n",
      "\n",
      "=== Type counts (deals) ===\n",
      "cash_flow_type\n",
      "proceed       5780\n",
      "investment    5521\n",
      "fair value     560\n",
      "=== Type counts (funds) ===\n",
      "cash_flow_type\n",
      "contribution    3455\n",
      "distribution    1832\n",
      "nav               83\n",
      "\n",
      "=== Date ranges ===\n",
      "Deals: 1992-07-02 00:00:00 .. 2025-04-01 00:00:00\n",
      "Funds: 1994-04-06 00:00:00 .. 2025-06-30 00:00:00\n",
      "\n",
      "=== Sanity ===\n",
      "{'deal_rows': 11861, 'deal_groups': 1111, 'fund_rows': 5370, 'fund_groups': 123}\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
